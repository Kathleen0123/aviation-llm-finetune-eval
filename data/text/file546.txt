
	
	
		
Introductionivil aviation plays a vital role in the U. S. economy.According to the Federal Aviation Administration (FAA), U. S. air carriers transported 793 million passengers over 1,039 billion revenue passenger-miles in 2009. 1 During this time period, 53 billion revenue ton-miles of freight went through U. S. airports.Civil aviation activity in the U. S. sustained over 10 million jobs, contributed $1.3 trillion in economic activity and resulted in value-added economic activity totaling 5.2 percent of the $14,119.4 billion U. S. Gross Domestic Product in 2009.While the demand for aviation fluctuates with economic and geopolitical conditions, most forecasts call for an increase of available seat-miles, revenue passenger-miles, enplanements, number of operations and fleet size.Reference 2 projects an average annual increase of available seat-miles, revenue passenger-miles and domestic enplanements of 2.8 percent, 2.2 percent and 2.0 percent during the 2013-2033 timeframe, respectively.In the same time frame, the annual growth rate in the number of operations is planned to be 1.7 percent at large hub airports, 1.5 percent at medium hub airports and 0.6 percent at small and non-hub airports.The number of commercial aircraft is expected to grow from 7,024 aircraft in 2012 to 8,554 in 2033.U. S. mainline air carrier passenger fleet, regional carrier passenger fleet and cargo large jet aircraft fleet are forecast to grow at an annual rate of 1.4 percent, 0.2 percent and 1.9 percent, respectively.In order to accommodate the growth in air traffic activity and emerging unmanned operations, the National Aeronautics and Space Administration (NASA), FAA and other stakeholders are developing concepts and technologies to create the Next Generation (NextGen) Air Transportation System.As the nation embarks on implementing the FAA's mid-term plan described in Ref. 3 and beyond, caution is predicated with respect to a history of cost growth, schedule delays and unmet expectations in implementing complex systems. 4Based on a review of 18 programs, the report in Ref. 4 concluded that the estimated cost increased by about 43% compared to the original estimate and the schedule slipped by one to 12 years.Problems were traceable to complex software development, overambitious plans, changing requirements and poor cost estimates.While it might be impossible to totally eliminate these problems, there is a need to analyze the interaction between legacy hardware and software systems and new concepts, technologies and systems implemented in hardware and software to determine benefits.These benefits are typically related to delay, throughput and controller workload.There is also a need to make investment and requirement change decisions based on benefits assessment, to identify gaps between the current system and the NextGen architecture, to improve and optimize air traffic system performance, and to predict system behavior under different forecast conditions.Table 1 lists the gaps in capabilities of the existing ATM simulation facilities such as the FAA NextGen Integration and Evaluation Capability (NIEC), NASA Airspace Operations Lab (AOL), NASA North Texas Research Station (NTX), Embry Riddle Florida Test Bed (FTB), Metron Aviation and MITRE Integration Demonstration and Experimentation for Aeronautics (IDEA) relative to the test bed required capabilities.The gaps in the capabilities were determined based on a survey of existing NextGen research lab capabilities. 5The original 0-5 ranking in Ref. 5 has been simplified to "gap, partial, and full" in Table 1.In order to provide a platform to fill technological deficiencies listed in Table 1 and in existing NASA tools to test, simulate, and conduct rapid near-real time "what-if" operations at regional, NAS-wide and global scales, NASA's Airspace Operations and Safety Program is developing a Shadow Mode Assessment using Realistic Technologies for NAS (SMART NAS) test bed.The test bed should be able to 1) test specific air traffic technologies and facilitate interactions with other systems in a test environment, 2) run in the background using realtime data to evaluate specific concepts with current systems, and 3) conduct rapid near real-time evaluations using actual air traffic data for decision support.The test bed will enable testing with real air traffic data in the same Table 1.Gaps with respect to required test bed capabilities.
ATM Lab CapabilityNIEC AOL NTX FTB Metron IDEA Lab manner as it would be run in operation, but the control decisions reached using the system/decision support tool are not automatically applied to the operational system; they are used only for evaluation.It is envisioned that eventually feedback of the control will influence the operational system."Shadow Mode Assessment" implies that the test bed driven in part by real air traffic data can be used for "what-if" type of analysis needed for operational decision support.The "using Realistic Technologies" part of SMART NAS test bed implies that the test bed will employ actual air traffic data, systems and procedures used in ATM operations and integrate high-fidelity human-inthe-loop aircraft, tower and air traffic control simulators.It will provide the necessary glue in terms of models, data and infrastructure for binding with the realistic technologies and high-fidelity simulators.Future systems could be modeled within the test bed using simulated data or be represented by external simulators.The rest of the paper is organized as follows.SMART NAS test bed vision and requirements are outlined in Section II.The test bed architecture development effort is discussed in Section III.A five-year plan for the development of the test bed is outlined in Section IV.Risks and mitigation strategies are discussed in Section V.The paper is concluded in Section VI.
II. SMART NAS Test Bed Vision and RequirementsThe test bed addresses the major difficulties in analyzing interactions of new concepts and technologies with legacy ATM systems.The ATM system is an operational system; it is not possible to turn it off, introduce new concept and technologies, validate and test the new system, and then turn it back on.It is also not practical to do a clean-sheet re-design of the ATM system because the existing systems have been developed over decades and at substantial cost.While the evolution of the current system might not have been satisfactory, it has been continuously enhanced to address the safety issues exposed by aviation incidents and accidents, and the needs of the users of the ATM system.It is not obvious that an equally or more safe alternative system can be developed by a complete redesign.Due to the safety critical nature of the ATM system, any technology or procedural changes have to be tested incrementally in steps and deployed in phases.As errors and issues are exposed during the testing steps and deployment phases, parts of the system have to be re-designed, re-developed and have to be put back again through the testing and re-deployment steps.This leads to delays and cost increases.Often, introduction of newer technologies also requires changes in the legacy systems that they interact with.Development and testing of new technologies, resulting from concepts, are accomplished by simulations and human-in-the-loop testing supported by limited amount of real air traffic data.Simulations, while necessary, are not completely trusted especially by the operators of the ATM system because models in the simulation are approximate representations and not the real system they use.If a technology being introduced is planned to interact with an operational system like the Time-Based Flow Management (TBFM), simulation testing that does not include the real TBFM might not expose the integration issues.Similarly, human-in-the-loop testing with small number of controllers and pseudo-pilots is also limited to small regions of airspace and few airports.Due to their focus on detailed evaluation of a small region, human-in-the-loop simulations are not suitable for determining the ripple effect of control actions in the focus region propagating into upstream regions.For example, control actions in the northeastern region can cause flow constraints to propagate westward, which a simulation solely focused on the northeastern region would not anticipate.The limitations discussed in this paragraph suggest that a NAS-wide simulation test bed that uses real operational data and models with actual ATM decision support systems and human operators is needed for benefits and feasibility analysis, cost assessment, testing and validation of NextGen and future concepts and technologies.Holistically integrated assessments, testing and validation using the test bed will provide the necessary data and confidence that the proposed concepts, algorithms, technologies, human-centered automation and air-ground architectures are beneficial, reliable, inter-operable, mature and ready for deployment.The test bed development is motivated by the need to accelerate acceptance of NextGen and far-term concepts and technologies based on testing and validation in a realistic environment and to provide capability for ATM and airline decision support.Impediments to acceleration of acceptance are often due to unavailability of archived and live data, lack of scenario generation capability and extensive human-in-the-loop testing with stakeholders for their buy-in.The other motivation is to support the needs of different types of users such as researchers, operators and decision-makers as a community resource.The test bed will provide data, models and services to enable NAS-wide simulation.This includes geographical data service, flight data service, weather data service, trajectory service, environmental service, Traffic Flow Management (TFM) service, Communication, Navigation and Surveillance (CNS) service, and Conflict Detection and Resolution (CD&R) service.Once developed, the test bed will provide a suitable platform for government, industry and academia for simulating safety, security, contingency planning and training scenarios, and developing and testing concepts/technologies.The next significant requirement for the test bed is enabling simulation with realistic technologies such as En Route Automation Modernization (ERAM), Standard Terminal Automation Replacement System (STARS), Flight Schedule Monitor (FSM), Terminal Flight Data Manager (TFDM), TBFM and Traffic Flow Management System (TFMS).FSM is used for setting up ground delay programs and airspace flow programs.TFDM is a new decision support system for improving the efficiency of airport surface operations and air traffic control tower functions.TBFM is the enhanced version of the Traffic Management Advisor (TMA) with Enroute Departure Capability (EDC) and coupled-scheduling capability for sequencing and spacing of airborne flights, merging departures into overhead stream and maximizing the use of available capacity.TFMS is for monitoring NAS and predicting demand-capacity imbalances.Research versions, which have the same functionality as the operational version, of these decision support systems will be used as black boxes in that the information needed by them will be provided by the test bed.Decision support data and control data generated by them will be received by the test bed and distributed to appropriate modules for processing.For example, the test bed will provide the flight-plans, track data and weather data needed by TBFM.TBFM will process the flight-plans, track data and wind forecast data using its trajectory synthesizer and dynamic planner to output Scheduled Time of Arrivals (STA) for the arrivals at metering reference locations such that the airport arrival rate constraints and metering constraints are satisfied.The STAs received from TBFM would be used by the TFM service of the test bed to determine the departure delay, speedchange and path-stretch needed by flights in the arrival flow to arrive at the metering reference locations such as the outer meter arc, meter fix and runway at the STA determined by TBFM.These decision support systems are run on the test bed infrastructure and are not parts of the ATM system used for control and flow management of actual air traffic.The test bed will only receive real air traffic state and control data from the actual ATM system via System Wide Information Management (SWIM) type of interfaces; it will not inject decision support and control data directly into the actual ATM system.However, the human operator can choose to do so based on test bed recommendation.Being able to interact with realistic technologies discussed in the previous paragraph places the requirement of real-time simulation.Since ATM systems operate at different update rates, for example surface systems at onesecond, Terminal Radar Approach Control (TRACON) systems at four-second, enroute systems at 12-second and Airline Situation Display to Industry (ASDI) 6 data at one-minute to three-minute update rate, particular attention is required for timing and synchronization of updates.The test bed is planned to have modules that update periodically and update based on events.Update rates also affect the performance of filters, estimators and trajectory synthesizers employed in the ATM decision support systems and models in the simulation.The test bed is planned to run in fast-time starting from initial state and control information derived from the operational ATM system to provide decision support data for ATM and airline decision-making.The test bed in this mode will only use realistic technologies that can run in fast time.The test bed is planned to support simulations with real people, real systems and simulated components.For example, data from a real-time feed from an aircraft in flight can be received and processed by the test bed during an experiment.The test bed will enable simulations where real people operate simulated systems.For example, air traffic controllers can provide separation services using displays and decision support systems driven by simulated air traffic data.The test bed will have the capability to drive such simulations and receive and process data received from human-in-the-loop systems.Finally, the test bed will enable simulations without human interaction in which all aspects are simulated.Real people can provide input to such a simulation, but are not involved in determining the outcomes.An example of this type of simulation is setting up TBFM for a particular airport configuration such as south-flow configuration at Dallas-Fort Worth airport along with arrival capacity, spacing and metering parameters, and then using TBFM created metering lists with scheduling data without human interaction in the simulation run.The test bed is planned to be configurable with combinations of these simulation types.Figure 1 portrays this vision of the test bed.This figure shows the test bed physical cloud, core services and application layers and several parallel universes.Parallel Test Universe 1 shows the test bed interacting with FAA, NASA, aircraft manufacturers and airline systems in a shadow mode test to evaluate new arrival flows at a major U. S. airport.In the Parallel Universe 2, the test bed interacts with Unmanned Aerial Systems Traffic Management (UTM) and ATM to evaluate autonomous/Remotely Piloted Vehicle (RPV) integration into the NAS.Parallel universes allow multiple users to independently use the test bed at the same time.The test bed is envisioned to have an open-architecture design, be open-source and be based on open-standards for integrating data, models, operational systems and high-fidelity simulators.This architecture is planned to provide access for partners and stakeholders to contribute concepts, data, models and their systems.The architecture is also required to support a plug-and-play capability for researchers and organizations inside and outside NASA to examine the feasibility and benefits of the proposed concepts and technologies at NAS-wide and global scales in an integrated fashion using real-life data sources that are not possible with current capabilities.The test bed is eventually expected to be an ATM community resource.It is also planned to be designed with memory management and fault recovery functions to enable it to run continuously 24-hours a day 7-days a week.The test bed needs to have recording and playback capabilities.It needs to have pre-processing tools for conditioning and preparing input data, post-processing tools for computing NAS performance metrics such as delays and throughput from simulation output data and generating reports, and visualization tools for visualizing geospatial, air traffic, weather and environmental data.Finally, the emerging trend of employing modern massively parallel computing and storage infrastructure and databases for efficient data storage and analytics needs to be explored for the test bed design.Database options include Structured Query Language (SQL) databases, no-SQL databases, databases for storing time-series data and Graph databases.The next section describes the tasks for architecture design, which is the first step of the development of the test bed for meeting the goals and requirements discussed in this section.
III. Architecture Design TasksAs a step towards defining the test bed, in November 2013, NASA awarded a two-year test bed architecture development contract to four teams from 13 organizations.** These teams have the tasks of developing architecture alternatives for the test bed, conducting benefits assessment of the test bed capability to justify the cost of building it ** The design contracts were awarded through NASA Research Announcement (NRA) contract process.NRAs are awarded via a competitive bid.and estimating the cost of implementing each architecture alternative.Information from the architecture study will frame the test bed development.The four teams are required to review current ATM gate-to-gate NAS-wide simulation and modeling capabilities, simulation with interaction between real people, real systems and simulators, their applicable technologies for integrating legacy and new systems/models, and report their assessment of technology gaps and lessons learned.The teams will develop open-source and open-architecture concepts to support the required functionalities discussed in Section II.They will identify and describe open-source and open-architecture alternatives for the test bed based on standard architectural views of a system, especially the functional, structural, concurrency and data views.As a part of the alternative architectures design effort to meet the NextGen and beyond technology acceleration, decision support and community resource objectives of the test bed, the teams will provide detailed descriptions of functional, data, hardware, integration and software architectural alternatives for the test bed.It is expected that plug-and-play capabilities, data collection and recording capabilities, visualization capability, and capability for planning, conducting and monitoring NAS-wide distributed simulations will be adequately described.A clear mapping of the test bed functionality to support air traffic control, flight deck and airline dispatch operations, and associated capabilities will be discussed and documented.The performance metrics that will be generated and computed from the test bed outputs will also be described.The teams will pay particular attention to the specification of modularity and flexibility of the design and construction of interfaces among the components of the system.They will provide sufficient details to enable evaluation of realism.The test bed also requires the use and integration of specific, heterogeneous data sources such as Official Airline Guide schedules, radar tracks, flight-plans and weather data for handling both fast-time and real-time modes.The NRA teams will identify all data sources that the test bed will use and how its architecture will consider them as inputs, and address possible data integration techniques such as Data Warehousing versus Data Federation approaches or a hybrid approach of the two that could support the test bed.As a part of developing studies and analyses of alternative system design architecture concepts and cost estimates for the test bed, the NRA teams are expected to include details of components such as models, simulators, existing capabilities, and proposed systems that could be included in the test bed, and answer the following questions.If they are Commercial Off The Shelf (COTS)-based, how well can they be adapted to the test bed requirements?What are the risks involved?What are some prior applications of these components that could be related to the test bed objectives?The teams will provide analysis of anticipated future alternative air-ground NAS architectures and how well their test bed architectures will handle them.Links, application protocol interfaces and access to all data streams will be discussed to assist development of data mining tools to enable detection of precursors to aviation safety events.The issue of the test bed "usability" is important.Possible test bed user groups will be identified, the importance of system usability will be addressed, and detailed key features that will promote usability will be described.The key features will be used to guide the process of setting up, configuring, and running the test bed.Validation of the test bed during design and at each stage of development will be critical in establishing its utility and effectiveness.The NRA teams are required to also provide prototypes of models and supporting evidence developed during this effort, documentation of repeatable test and experimental validation capabilities in the final report to NASA.Proposed methodologies for validating the test bed architecture and detailed specification of performance metrics will also be discussed in the reports.The teams are expected to clearly describe and explain how the goals, characteristics and features of the test bed will be satisfied by the proposed architectures.The task products and deliverables are planned to be in such detail that the test bed can be developed using that architecture description.Finally, the NRA teams will conduct benefits assessment and provide initial cost assessment for developing the test bed based on identified architectures.The benefits assessment will describe how the test bed is beneficial when compared to other existing modeling and simulation approaches.NASA will begin to review the architecture designs to identify areas of agreement among the designs and areas requiring further investigation subsequent to the December 2015 deliverables.Based on this review, the best combination of architecture designs created by the NRA teams will be chosen for implementation decision.Application Programming Interfaces (API), baseline model libraries and data/model sharing policies will be implemented.Services listed in Section II will be implemented and the test bed will be built incrementally and tested extensively at each step of development.Figure 2 shows a notional test bed environment in which the test bed in the cloud (upper portion of the figure) and the operational system (lower portion of the figure) are connected via the standardized interface.State data such as flight-plans and aircraft position data, and control data such as airport arrival rate, departure settings and flight cancellations provided by Air Navigation Service Provider (ANSP), airport service provider and aircraft operator are received as live-data via the interface.A copy of the decision support systems used by the operators of the system is hosted on the test bed computation, storage and communications platform in the cloud.They interact with each other and the core models via APIs and standard interfaces.These decision support systems are included in the "Model" portion of the test bed environment shown in Fig. 2. Note that these decision support systems in the model portion of the test bed and live-data from the operational ATM system represent realistic technologies.The figure also shows that the ANSP can use the interface to run simulations with realistic technologies to generate data needed for decision-making.Other users such as airlines and concept/technology developers can also run simulations in a similar manner.
IV. First Five Years Development PlanThe project will begin implementation of the test bed leveraging the architecture designs and use cases created by the NRA teams.After evaluation of the design alternatives provided by them, NASA will select a design or a combination of designs.Various requirements/features/capabilities will then be prioritized.Initial tasks are to develop interfaces and capabilities to provide the core distributed simulation and test layer, live-data feed and distributed interactive display, develop trajectory, weather and scheduling services, and connect them together in the test bed environment.This includes defining and implementing models, APIs, standard communication/data/model interfaces, and compute, storage and communications platform to enable experimentation and testing with real people, real systems and simulated components.The initial project timeline for developing the test bed is shown in Fig. 3.The preliminary first five-year plan that follows the test bed project timeline in Fig. 3 is discussed below.A major objective of this paper and the five-year plan is to initiate early community involvement for initial feedback from the community.The out-year plans will continue to be refined based on community input, reviews and outcomes of previous years.
Fiscal Year 2015 (FY15)In the first fiscal year, focus continues to be on review and evaluation of the test bed open-architecture designs and test bed development cost estimates provided by the four NRA teams.This process will end in the second quarter of FY16.Based on this review and evaluation, a design or a combination of designs will be selected.If the cost and benefit assessment provided by the NRA teams are deemed reasonable and acceptable to NASA, full implementation of the test bed will begin.Complete set of capabilities will be accomplished over the next several fiscal years.To prepare for the evaluation process, test bed prototype development will begin with an in-house development team from NASA using private and third-party cloud platforms.This will include a hybrid hardware and software architecture configuring labs at NASA working in and through the cloud space with associated distributed file systems, support libraries, distributed configuration service, data backup and redundancy management, resource management, and workflow scheduling.Open source big-data tools such as Hbase 7 for storage and Spark 8 for inmemory computations will be installed and configured.Benchmark tests will be designed and run to determine the feasibility of transferring large amounts of data into and out of the cloud.Test bed prototype implementations will be constructed using existing models and algorithms for trajectory generation, scheduling and weather, for example.The first phase will be using an existing Airspace Concept Evaluation System (ACES) 9 trajectory generator functionality in the test bed.This will be accomplished by starting with a standalone version of the Gate-to-Gate Trajectory Generator (GTG), a Base of Aircraft Data (BADA) 10 performance model based kinetic trajectory generator, from ACES code base by removing ACES dependencies.GTG will be tested extensively and enhanced to correctly model very short flights, flights from and to high-altitude airports, international flights, and departure and arrival meter fix altitude and speed constraints.Profilers will be used to determine computation choke points in GTG, which will guide actions for improving GTG execution speed.An API will be initially defined and developed for GTG input and output.The in-house Hadoop 11 cluster and big-data technologies will be used to implement a trajectory generator service using GTG, and a baseline simulation will be created.The combination of big-data tools for in-memory computation, and organizing, storing and searching simulation output data in databases will be determined.Weather use cases will be developed and API for weather products will be defined.Existing Rapid Update Cycle (RUC) reader will be enhanced to read Rapid Refresh (RUC RR) 12 data and display 1-hour, 2-hour, 3-hour and 6hour wind forecast data.The best way to store these data on big-data databases and to serve it to clients using an API will be identified.The eventual goal is to extend the API to include wind, turbulence, icing and severe weather data and create a weather service.Weather impact or translation models for each weather constraint will also be available.The trajectory generator service will then be tested with wind data obtained from the wind data service.ASDI service will be created.Real-time ASDI data messages will be ingested and parsed to update flight-plans and aircraft position data.A Representational State Transfer (REST) 13 server for a client to receive ASDI data in XML format via http protocol will be developed.Servers and load balancers will be set up on governmentcontracted cloud to ingest parsed ASDI streaming data from the REST server and distribute to web clients for display at NASA.Tests will be done with multiple web clients to determine user experience as a function of number of servers and load balancers allocated in the cloud.The existing First-Come First-Serve (FCFS) algorithm will be used to create a scheduler service to generate departure and enroute delays for complying with airport arrival and departure capacity constraints and sector capacity constraints.Algorithms will be implemented for determining GTG maneuvers (controls) in response to scheduled time of arrival constraints (command) determined using the FCFS scheduler.The trajectory generator service will be enabled to execute departure time, speed-change, path-stretch and holding maneuvers to meet the constraints.Data exchange formats including Flight Information Exchange Model (FIXM), 14 Aeronautical Information Exchange (AIXM) 15 and Weather Information Exchange Model (WXXM) 16 standards will be reviewed.Input data required for simulation will be converted into standards-based formats.A simple web-based visualization service for displaying aircraft position data and flight-plans will be implemented.A report generation service will be developed for reading simulation output data from big-data database such as Hbase and generating a Portable Document Format (PDF) report summarizing the simulation results.Figure 4 presents a block-diagram description of the core set of the test bed capabilities.Any air traffic simulator such as the Future ATM Concepts Evaluation Tool (FACET) 17 and ACES or an ATM decision support system like TBFM requires a trajectory generator.A trajectory generator is needed in the test bed to simulate aircraft flight, provide surveillance and flight-plan data needed by operational systems and to modify aircraft flight in response to commands from operational systems.Since aircraft trajectory is affected by wind, a wind service is needed.The FCFS scheduler is required for computing the controls for meeting airport arrival and departure capacity constraints since delays in the NAS are primarily driven by airport capacity constraints.The database is needed for storing aircraft states as a function of time.Visualization and report generation are needed for analyzing simulation results.Finally, the results obtained using the prototype test bed baseline simulation capability shown in Fig. 4 will be compared against those obtained with an existing simulation system like ACES for validating some of the test bed required capabilities.Post-processing and real-time tools will be utilized to compute metrics such as arrival/departure time histories, delays and sector-counts for comparison.
Fiscal Year 2016 (FY16)In Fiscal Year 2016, the test bed development will focus on replacement of prototype components with operational equivalents, extension of system inputs, and identification of additional features.These augmentations are intended to support realization of the envisioned test bed, while retaining its phased development approach.
Component IntegrationThe test bed prototype developed in FY15 will be further developed to allow replacement of components.GTG API defined earlier in FY15 will be refined based on the final architecture design to enable replacement of GTG with an alternative trajectory generator.APIs for the other simulation services of the test bed will be similarly refined to enable replacement of the components such as the FCFS with user provided components.Uncertainties such as 1) aircraft performance parameters such as drag, thrust and fuel-flow coefficients, 2) intent related to path, cruise speed and cruise altitude, 3) departure time and taxi speed, and 4) wind and weather will be provided as inputs for the "Uncertainty management, prognostics and data mining toolbox" indicated in Fig. 3. Uncertainty parameters will also be used to model actions of thousands of human controllers in the simulation.Trajectory generator service and the APIs will be enhanced to model uncertainties and receive uncertainty data to support Monte Carlo fast-time simulations.Communication, Navigation and Surveillance (CNS) models will be defined and implemented to create the Build 0.1 CNS toolbox.Controller workload, controller and pilot decision making delays, surface, airport and enroute surveillance systems noise, bias and data drop characteristics, GPS and onboard position sensor characteristics, and ADS-B 18 Out/In characteristics for example will be modeled.APIs will be defined for replacing these CNS models with user provided models.
System Input ExtensionPublish and subscribe service will be implemented in the test bed to ingest surface, TRACON, Center and Traffic Flow Management (TFM) data from SWIM feed.Compliance with standards such as FIXM for data exchange between the test bed and aircraft and air traffic simulators like Aircraft Simulation for Traffic Operations Research (ASTOR), Airspace and Traffic Operations Simulation (ATOS) and Multi-Aircraft Control System (MACS), FAA NIEC facility, and user systems will be explored.The SWIM publish-subscribe and security framework could prove to be a convenient standards-based method for the test bed and systems interacting with it to run independently and exchange data.A simulation manager will be designed and implemented following the test bed architecture for registering software modules implementing models and algorithms, sequencing and timing their execution, and exchanging data with them.External simulation and operational systems will also be registered with the simulation manager with information such as data format and data exchange frequency, which could be part of the exchange standard.A Graphical User Interface (GUI) based simulation setup and management tool will also be developed.Publish and subscribe service will be extended to connect with terminal scheduling data, initially as a prototype to the Research Traffic Management Advisor (RTMA) as preparations are made for TBFM scheduling data via the SWIM interface.Standards-based API will be used for data exchange.Command and control API will be designed and implemented to provide scheduled time of arrival at metered locations generated by RTMA/TBFM to the trajectory generator service.Actual times of arrival generated by the trajectory generator service will then be validated against the scheduled times of arrival.To implement command and control capability, TFM decision data such as sector/flow evaluation area counts and airport arrival rates will be forecast.Air traffic data with known constraints such as TFM and TBFM constraints provided by SWIM feed will also be predicted.The forecasted traffic will then be used to determine if capacities of NAS elements such as sectors and airports would be exceeded in the future.Weather service will be developed to support decision-making and control functions, and to answer questions related to weather along the route, proximity of the route/trial-planned route to weather, severe weather over airport and arrival/departure routes affected by weather.Wind-optimal routing based flight-planning service for domestic and oceanic traffic will be developed.
Feature IdentificationNASA and FAA NextGen concepts will be reviewed to identify several concepts for evaluation using the test bed.Requirements including models, algorithms and data for concept evaluation in a realistic environment or for decision support in shadow mode will be defined.A post processing toolbox will be developed with tools for visualizing raw and processed data, data analysis, machine learning and document generation.The basic visualization capability developed in the first year will be enhanced to display weather, traffic, airspace geometry, surface traffic, TFM decision support data and SWIM data for example.Finally, the process for identifying a consortium to develop a proposal for hosting, maintaining, supporting and sustaining the test bed for the community will be initiated.
Fiscal Year 2017 (FY17)The test bed will extend to include outside users.The test bed is expected to be evolved enough to enable modeling of airline fleet operations and plug in of airline Flight Operations Center (FOC) tools.This will be accomplished in part by modeling current and future airline fleet operations using flight plan and Official Airline Guide data.An understanding of airline FOC tools will be accomplished by working with several airlines and APIs will be developed for data exchange with these tools.Discussions will be held with the collaborative decision making community to prototype ATM-airline negotiation tools.The SMART NAS Beta test bed will be connected to live-data feed, several aircraft and air traffic simulators like ATOS and MACS, FAA NIEC-type facility and user systems to validate the APIs and the simulation environment.A large-scale simulation with the connected systems and human-in-the-loop will be conducted to validate the selected test bed architecture.Vehicle models, and terrain, population and other databases needed for supporting operations of Unmanned Aerial Systems (UAS) and other advanced vehicles in the NAS will be implemented.Data needed for safety margin assessments will be determined and APIs needed for data exchange with the toolbox for determining safety margins will be defined and implemented.The test bed will be enhanced to provide the needed input data.Environmental toolbox with noise and emissions models will be integrated into the test bed.Visualization service developed in the previous years will be expanded to include tools for display of ATMairline negotiation data, terrain and population data, safety margin assessment data and environmental data for example.
Fiscal Year 2018 (FY18)Separation assurance will be accomplished using Center and TRACON data obtained via the SWIM feed.A separation assurance service will be created using Automated Airspace Concept (AAC) algorithms driven with ERAM data and Terminal Automated Airspace Concept (TAAC) algorithms driven with TRACON data obtained via the SWIM feed.API will be developed and implemented to exchange data with the conflict detection and resolution toolbox included in the separation assurance service.Airborne self-separation and interval management simulators/systems will be connected to the test bed via the developed APIs.Data exchange between the test bed and Integrated Arrival Departure and Surface (IADS) schedulers such as the Dantzig-Wolfe 19 optimal IADS scheduler, developed under several NASA contracts, and those developed by the IADS project team will be integrated as a TFM model alternatives.API for data exchange with IADS schedulers will be defined and implemented.IADS schedulers in the NY and Bay Area metroplexes for example will be tested in the test bed environment."Uncertainty management, prognostics and data mining toolbox" developed by NASA will be integrated into the test bed.Uncertainty modeling capabilities developed in FY16 will be used for generating the data needed by the prognostics toolbox.Enhanced CNS models developed by NASA will also be integrated.Visualization tools will be enhanced to display separation assurance and scheduling data.The test bed capabilities will be verified and validated against requirements and accuracy of the simulation results.Tests will be done to ensure it is robust and runs continuously without breaking down.The test bed production Build 1 will be open to the university/other partners for further testing.
Fiscal Year 2019 (FY19)Algorithms (including machine-learning algorithms) will be designed and implemented for computing TFM decision support/control data and translating them into parameters like airport arrival rate setting for TBFM and Flight Schedule Monitor (FSM).Algorithms will also suggest use of particular Severe Weather Avoidance Plan (SWAP) Playbook 20 routes for avoiding regions of severe weather along with associated TFM controls such as miles-in-trail for preventing congestion along the chosen routes and sectors.Use cases for decision support data will be identified working with the FAA, airports, airlines, and other stakeholders.APIs will be developed for providing the test bed decision support data into the SWIM "research" feed.Selected NASA and FAA NextGen concepts will be implemented in the test bed and evaluated.A joint shadow mode experiment will conducted with the FAA and airlines using the test bed in which the generated decision data will be provided via the SWIM "research" feed for display and evaluation at FAA and airline facilities.Build 2 CNS models developed by NASA will be integrated into the test bed.Visualization tools will be enhanced to display TFM decision support data, and NASA and FAA concepts evaluation data for example.The test bed production Build 2 along with its model libraries, test suites and documentation will be released to the consortium for hosting, maintaining, supporting and sustaining the test bed for the community.NASA will likely continue to be a part of the community both as a user and contributor.
Beyond FY19The test bed will continue to integrate algorithms supporting evaluation of ATM concepts and technologies developed through NASA, FAA and other programs.The test bed will continue be used with the FAA and other partners for ATM technology readiness demonstrations.NASA will assist in the transfer of the test bed to the Department of Defense and other government agencies if desired and to provide the needed technical information.
V. Risk and Mitigation StrategiesIn the current day networked simulation landscape, there are already organizations that host and maintain distributed simulation frameworks.These systems were created with specific design choices in mind to meet the primary needs of their creators.This meant that some desired but not absolutely necessary attributes were superseded by requirements because the available technology at the time could not accommodate both.For example, data security criteria might have trumped hardware cost or ease of access for user connection.Through the commercially driven market, technology products emerge every year to enable better, faster, and cheaper connectivity.The test bed vision is to use these newly available technologies (often developed for financial transactions and social sharing) and apply them to the aerospace simulation paradigm to enable a larger set of desired functionalities to be possible while meeting the same system requirements.Therefore, the risk for the test bed is not that any particular functionality is impossible, since these individual capabilities are already demonstrated in disparate existing systems.The risk is that the full suite (or at least the critical subset) is not possible as a cohesive, elegant system within the time and budgetary cost available to NASA, and within a complexity that users are willing to work with.Another possible risk is that the fidelity is not acceptable to end users and integrators like the FAA.In addition, test bed use outside NASA might be limited unless it is very user friendly, provides a host of models and easily connects to external systems via standard interfaces.The complexity risk derives from the potential need to marry many technology solutions and data sources together to achieve all the necessary functionality.The unknown at this stage in the design is how well these individual technologies, some of which will be created in the future, can be adapted to future use cases, and how easily they can be managed together within a single system.Part of management has to consider user tolerance.Though much of the complexity of interfacing to these systems can be abstracted away through the software architecture, some users and system developers will have to manage these pieces.The list of potential technologies is extensive and includes software languages (Java, C++, Python, Jython), formatting and encoding schemes (ASCII, HTTP, XML, SQL), formatting protocols (ARINC, AIXM, FIXM, WXXM), web transfer services (SOAP, WSDL, REST), data storage and management (Big-Data, Hadoop, YARN, MapReduce, Hive, Ambari), and distributed communication (HLA, DDS, DIS, SIMNET) to name a few.Are these technologies flexible enough to work together?Are all the needs for NASA's use cases met by the functionalities developed for their original niche markets?Will the burden of adopting and managing new technology products be greater than the user and developer community is willing to accept?All these are potential risks to the success of the test bed.An additional source of risk is the availability of global data for ATM modeling and simulation.The mitigation to this risk is the diversity of the contracted test bed design teams.Each team brings a different set of strengths to the design table and prioritizes aspects of the system consistent with their expertise and corporate philosophies.The role of visionary can be balanced by a "devil's advocate" counterpart.The pessimist can be balanced by the optimist with each company deriving comfort in the aspects of the system that are most familiar to them.This does not mean that all of the risk for this complexity can be resolved.The final disposition may be that although we have made significant progress in technology in the past decade, the final technology solution for the test bed is not ready yet.However, the diversity of the teams help insure that this answer does not emerge simply as a result of non-familiarity, giving NASA better confidence in the final answer.The in-house prototype development of the test bed in FY15 and 16 is part of the technical risk reduction strategy.By allowing for constant and sustained interaction between the designers, developers and users of the test bed, in-house development can better support changes to evolving research and hence test bed requirements.The prototype will enable evaluation of various big-data technologies and help identify the combination of these technologies for efficient computation, storage and communication needed for meeting the goals and objectives of the SMART-NAS test bed discussed in Section II.Furthermore, the lessons learned on earlier NASA projects like Virtual Airspace Modeling and Simulation (VAMS) and NASA Exploratory For The NAS (NEXTNAS), and experience developing NAS-wide ATM systems like ACES and FACET will also be used to contain the technical risk.The cost risk includes both monetary cost and time cost, and must include the effort to build the system, the effort to maintain the system, and the effort of individual users to adapt their labs to interface to the system.Clearly these will be substantial, but potentially they will be compensated by the benefit provided once the test bed is operational and low cost of use.Estimating these costs and benefits is not trivial.Without an existing system of similar complexity to reference, novel cost and benefit methods must be performed to arrive at an answer.This creates additional risk in the reliability of the methodology.The risk mitigation to address all of these cost risks is the redundancy in the contracts requiring each team to develop a method for cost/benefit analysis and then to provide the results for comparison.The intention is not to pick the result that looks most desirable, but rather to compare and blend techniques and results to gain confidence in an answer that leverages methods and expertise of all contributors.Ultimately, NASA must be convinced that the final numbers are both believable and are reasonable.It is possible that the projected cost of the test bed will be too high.If this is the case, the best scenario is to know this now before committing to full-scale development of the system.With the benefit numbers in hand, creation of the test bed can be delayed until the supporting technology is mature enough and affordable enough to make it worthwhile.Even if the decision is made to go ahead with the implementation, the ultimate successful development will depend on the government budget priorities.Desired capabilities of the test bed will be reviewed and prioritized each year to reflect budgetary realities as a mitigation step.Finally, the desire to make the test bed open source, community resource and global might not come to fruition because of national security concerns about data and models in the test bed and the likelihood of the test bed being used for unintended purposes.
VI. ConclusionsVision and requirements for the Shadow Mode Assessment using Realistic Technologies for the National Airspace System (SMART NAS) test bed for accelerating the deployment of National Aeronautics and Space Administration (NASA) and Federal Aviation Administration (FAA) NextGen concepts and technologies based on testing and validation in a realistic environment and to provide "what-if" capability for air traffic management and airline decision support were described.The charter of the four test bed architecture development teams will provide a framework for future test bed development.This included review of current air traffic management simulation and modeling capabilities, human-in-the-loop simulation and technologies for integrating legacy and new systems for identifying technology gaps and lessons learned, identification of open-source and open-architecture alternatives for the required test bed capabilities, and initial cost assessment for developing the test bed based on alternative architectures.Finally, a five-year plan for developing the test bed was outlined.The highlights of the first year are evaluation of design alternatives provided by the four architecture development teams and selection of a design or a combinations of designs, implementation of a prototype test bed with trajectory generator service, wind service and scheduling service using big-data technologies to assist evaluation of the alternative architectures, and development of a visualization capability.Highlights of the second year include capability to replace test bed components with user-provided components, modeling of uncertainties, inclusion of communication, navigation and surveillance (CNS) models, implementation of publish-subscribe service, implementation of simulation manager, creation of weather service and creation of post processing toolbox with visualization capability enhancements.The third year effort will focus on modeling of airline operations, inclusion of airline tools and airline-service provider collaborative decision-making tools, inclusion of safety margin assessment and environmental impact toolboxes, large simulation with the test bed exchanging information with other simulators, and enhancement of the visualization capability to display new data types.In the fourth year, the test bed will be enhanced to enable separation assurance, integrated arrival departure and surface traffic management, assessment via the prognostics and data mining toolbox, and support for enhanced CNS models.During the fifth year, traffic flow management tools for generating decision support data will be implemented.A shadow mode evaluation will be conducted to evaluate selected NextGen concepts.Finally, the developed test bed will be released to the ATM community such as a consortium for hosting, maintenance, support and sustenance.Figure 1 .1Figure 1.SMART NAS test bed vision.
Figure 2 .2Figure 2. SMART NAS test bed notional architecture and operational systems.
Figure 3 .3Figure 3. SMART NAS test bed project timeline.
Figure 4 .4Figure 4. First year SMART NAS test bed prototype.
			Downloaded by NASA AMES RESEARCH CENTER on July 2, 2015 | http://arc.aiaa.org| DOI: 10.2514/6.2015-2794
		
		
			

				


	
		The Economic Impact of Civil Aviation on the U. S. Economy
	
	
		Federal Aviation Administration
		
			August, 2011. 2. August, 2014
		
	
	NextGen Implementation Plan
	Federal Aviation Administration, "The Economic Impact of Civil Aviation on the U. S. Economy," August, 2011. 2 Federal Aviation Administration, "FAA Aerospace Forecast Fiscal Years 2013-2033." 3 Federal Aviation Administration, "NextGen Implementation Plan," August, 2014.



	
		Air Traffic Control Modernization: FAA Faces Challenges in Managing Ongoing Projects, Sustaining Existing Facilities, and Introducing New Capabilities
		AV-2008-049
		
			April 14, 2008
			Office of the Inspector General, Office of the Secretary of Transportation, U. S. Department of Transportation
		
	
	Report
	Office of the Inspector General, Office of the Secretary of Transportation, U. S. Department of Transportation, "Air Traffic Control Modernization: FAA Faces Challenges in Managing Ongoing Projects, Sustaining Existing Facilities, and Introducing New Capabilities," Report No. AV-2008-049, April 14, 2008.



	
		Work Plan for the Shadow Mode Assessments using Realistic Technologies for the National Airspace System (SMART NAS) Architecture Development and Assessment
		94035-0001
	
	
		Metron Aviation
		Moffett Field, CA
		
			February 28, 2014
		
	
	Contract Number NNA14AA03C, CDRL Number: 02
	Metron Aviation, "Work Plan for the Shadow Mode Assessments using Realistic Technologies for the National Airspace System (SMART NAS) Architecture Development and Assessment," Contract Number NNA14AA03C, CDRL Number: 02, NASA Ames Research Center, Moffett Field, CA 94035-0001, February 28, 2014.



	
		Aircraft Situation Display To Industry: Functional Description and Interface Control Document for the XML Version
		
	
	
		NJ
		
			08234
			April 15, 2011
			Egg Harbor Township
		
		
			Computer Sciences Corporation
		
	
	Federal Sector -Civil Group, 100 Decadon Drive. cited 29 April 2015
	Computer Sciences Corporation, "Aircraft Situation Display To Industry: Functional Description and Interface Control Document for the XML Version," Federal Sector -Civil Group, 100 Decadon Drive, Egg Harbor Township, NJ 08234, April 15, 2011, URL: http://www.fly.faa.gov/ASDI/asdidocs/ASDI_XML_ICD-v1.8-rev1.pdf [cited 29 April 2015].



	
		Apache HBase Reference Guide
		
			ApacheHbase
		
		
			Team
		
		
		
	
	cited 29 April 2015
	Apache HBase Team, "Apache HBase Reference Guide," URL: http://hbase.apache.org/apache_hbase_reference_guide.pdf [cited 29 April 2015].



	
		Learning Spark
		
			HKarau
		
		
			AKonwinski
		
		
			PWendell
		
		
			MZaharia
		
		
			February 2015
			O'Reilly Media
		
	
	Karau, H., Konwinski, A., Wendell, P., and Zaharia, M., "Learning Spark," O'Reilly Media, February 2015.



	
		User Manual for the Base of Aircraft Data (BADA) Revision 3.12
		
			LMeyn
		
		Number 14/04/24-44
		
	
	
		Proceedings of the AIAA Modeling and Simulation Technologies Conference and Exhibit
		the AIAA Modeling and Simulation Technologies Conference and ExhibitKeystone, Colorado
		
			August 21-24, 2006
		
	
	EEC Technical/Scientific Report
	Build 4 of the Airspace Concept Evaluation System. cited 29 April 2015
	Meyn, L., et. al., "Build 4 of the Airspace Concept Evaluation System," Proceedings of the AIAA Modeling and Simulation Technologies Conference and Exhibit, Keystone, Colorado, August 21-24, 2006. 10 Eurocontrol Experimental Centre, "User Manual for the Base of Aircraft Data (BADA) Revision 3.12," EEC Technical/Scientific Report Number 14/04/24-44, URL: https://www.eurocontrol.int/sites/default/files/field_tabs/content/documents/sesar/user-manual-bada-3-12.pdf [cited 29 April 2015].



	
		11 The Apache Software Foundation
		
		
	
	Apache Hadoop. cited 29 April 2015
	11 The Apache Software Foundation, "Apache Hadoop," URL: https://hadoop.apache.org/ [cited 29 April 2015].



	
		12 Earth System Research Laboratory
		
	
	
		National Oceanic and Atmospheric Administration
		
	
	Rapid Refresh. cited 29 April 2015
	12 Earth System Research Laboratory, "Rapid Refresh," National Oceanic and Atmospheric Administration, URL: http://rapidrefresh.noaa.gov/ [cited 29 April 2015].



	
		
		14 "Flight Information Exchange Model
		
	
	Representational State Transfer. cited 29 April 2015
	13 Wikipedia, "Representational State Transfer," URL: http://en.wikipedia.org/wiki/Representational_state_transfer [cited 29 April 2015]. 14 "Flight Information Exchange Model," URL: http://www.fixm.aero/ [cited 29 April 2015].



	
		Aeronautical Information Exchange Model
		
		
	
	cited 29 April 2015
	15 "Aeronautical Information Exchange Model," URL: http://www.aixm.aero/public/subsite_homepage/homepage.html [cited 29 April 2015].



	
		
		Weather Information Exchange Model
		
			29 April 2015
		
	
	cited
	16 "Weather Information Exchange Model," URL: http://www.wxxm.aero/public/subsite_homepage/homepage.html [cited 29 April 2015].



	
		FACET: Future ATM Concepts Evaluation Tool
		
			KDBilimoria
		
		
			BSridhar
		
		
			GBChatterji
		
		
			KSSheth
		
		
			SRGrabbe
		
	
	
		Air Traffic Control Quarterly
		
			9
			1
			
			2001
		
	
	17 Bilimoria, K. D., Sridhar, B., Chatterji, G. B., Sheth, K. S., and Grabbe, S. R., "FACET: Future ATM Concepts Evaluation Tool," Air Traffic Control Quarterly, Vo. 9, No. 1, 2001, pp. 1-20.



	
		Automatic Dependent Surveillance -Broadcast
		
			Wikipedia
		
		
		
	
	cited 29 April 2015
	Wikipedia, "Automatic Dependent Surveillance -Broadcast," URL: http://en.wikipedia.org/wiki/Automatic_dependent_surveillance_%E2%80%93_broadcast [cited 29 April 2015].



	
		Massively Parallel Dantzig-Wolfe Decomposition Applied to Traffic Flow Scheduling
		
			JRios
		
		
			KRoss
		
	
	
		Journal of Aerospace Computing, Information, and Communication
		
			7
			1
			
			19. 2010
		
	
	19 Rios, J., and Ross, K., "Massively Parallel Dantzig-Wolfe Decomposition Applied to Traffic Flow Scheduling," Journal of Aerospace Computing, Information, and Communication, Vol. 7, No. 1 (2010), pp. 32-45.



	
		Air Traffic Control System Command Center
		
	
	
		Federal Aviation Administration
		
			30 April 2015
		
	
	National Severe Weather Playbook
	Air Traffic Control System Command Center, "National Severe Weather Playbook," Federal Aviation Administration, URL: http://www.fly.faa.gov/Operations/playbook/current/current.pdf [cited 30 April 2015].


				
			
		
	
