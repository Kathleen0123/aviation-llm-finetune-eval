
	
	
		
vi
List of Figures
IntroductionThe desire and ability to fly Unmanned Aircraft Systems (UAS) in the National Airspace System (NAS) is of increasing urgency.The application of unmanned aircraft to perform national security, defense, scientific, and emergency management are driving the critical need for less restrictive access by UAS to the NAS.UAS represent a new capability that will provide a variety of services in the government (public) and commercial (civil) aviation sectors.The growth of this potential industry has not yet been realized due to the lack of a common understanding of what is required to safely operate UAS in the NAS.NASA's UAS Integration in the NAS Project is conducting research in the areas of Separation Assurance/Sense and Avoid Interoperability (SSI), Human Systems Integration (HSI), and Communications (Comm), and Certification to support reducing the barriers of UAS access to the NAS.This research is broken into two research themes namely, UAS Integration and Test Infrastructure.UAS Integration focuses on airspace integration procedures and performance standards to enable UAS integration in the air transportation system, covering Detect and Avoid * (DAA) performance standards, command and control performance standards, and human systems integration.The focus of Test Infrastructure is to enable development and validation of airspace integration procedures and performance standards, including integrated test and evaluation.In support of the integrated test and evaluation efforts, the Project will develop an adaptable, scalable, and schedulable relevant test environment capable of evaluating concepts and technologies for unmanned aircraft systems to safely operate in the NAS.To accomplish this task, the Project is conducting a series of human-in-the-loop (HITL) and flight test activities that integrate key concepts, technologies and/or procedures in a relevant air traffic environment.Each of the integrated events will build on the technical achievements, fidelity, and complexity of the previous tests and technical simulations, resulting in research findings that support the development of regulations governing the access of UAS into the NAS.The integrated events started with two initial flight test used to develop and test early integrations and components of the test environment.Test subjects and a relevant test environment were brought in for the integrated HITL (or IHITL) conducted in 2014.The IHITL collected data to evaluate the effectiveness of DAA Well Clear † (DWC) algorithms and the acceptability of UAS concepts integrated into the NAS.The first integrated flight test (and the subject of this report) followed the IHITL by replacing the simulation components with live aircraft.The project finishes the integrated events with a final flight test to be conducted in 2016 that provides the researchers with an opportunity to collect DWC and Collision Avoidance ‡ (CA) interoperability data during flight encounters.* Detect and Avoid is defined as the capability of an unmanned aircraft to remain a safe distance from other airborne aircraft to avoid collisions.The two primary concepts that make up Detect and Avoid are appropriately called well clear and collision avoidance.† DAA Well Clear is defined as the boundary around the UA defined by time and/or distance intended to be an electronic means of compliance for UAS to provide safety and conform to the rules of the air.‡ Collision Avoidance (upper case): Last-resort method of preventing mid-air collisions, between aircraft, as directed by a Collision Avoidance System, like the Traffic Alert and Collision Avoidance System (TCAS).
ScopeThe integrated Flight Test Series 3 (FT3) test period was divided into two distinct configurations.Configuration 1 test execution began on 15 June 2015, completed 27 July 2015, and was comprised of 12 flights and more than 200 test points.Configuration 2 began concurrently on 16 July 2015 and was concluded early on 12 August 2015.The Configuration 2 phase conducted three test sorties and numerous systems integration sorties, but was unable to achieve the desired performance needed by the researchers and was therefore concluded early.Further details about each flight test configuration are described in the report titled Flight Test Series 3, Flight Test Report 1 .This report focuses on the test infrastructure and test environment used to support the development of the flight test activities.Included in this report are descriptions of the ground control station (GCS), and the Live, Virtual, Constructive distributed environment (LVC-DE).Also discussed are analysis and findings of the system latency encountered throughout the LVC-DE and a section describing the test setup configuration (methodology, participants, communication and system architecture).A summary of the data collected during the flight test is included, as well as feedback from the test subjects and subject matter experts.
PurposeFlight Test Series 3 collected and provided data to the UAS researchers for their evaluation and development of a communication system, DAA algorithms, and pilot displays for candidate UAS in a relevant environment.The technical goals of FT3 were to: 1) perform end-to-end traffic encounter tests of pilot maneuver guidance generated by the DAA algorithms, 2) conduct flight test of a prototype Communication system as part of an integrated UAS system, and 3) collect data to inform the Minimum Operational Performance Standards (MOPS) for DAA and Command and Control (C2).The completion of FT3 reduced the risks associated with building a relevant flight test environment moving towards the final flight tests (FT4).Flight Test Series 3 was divided into two distinct test configurations, each focusing on different aspects of the primary technical goals.The first was a four-week study (described as Scripted or Pairwise Encounters) looking at the DAA algorithm alerting times to support the definition of the well-clear boundary.The second was a study (described as Full Mission flights) focusing on UAS pilot response times to, and acceptability of, the same DWC alerts, pilot maneuver guidance, and GCS displays under real world uncertainties, including real voice communication delays.Figure 1 shows the general usage of live, virtual and constructive assets contributing to the flight test environment.While the live, virtual, and constructive components of a test environment only encompass a portion of a full simulation or flight test, the test environment is widely known as an LVC. 2,3,4A more detailed discussion of the UAS-NAS Project's LVC-DE is described by Murphy,et. al. 5 Enabling the collection of data, Flight Test Series 3 utilized the LVC-DE framework developed by the Project to provide the core infrastructure and supporting simulation LVC software components in order to integrate a UAS flying under nominal (non-contingency) operations with ATC and live and virtually manned aircraft.An instance of the LVC-DE was configured to meet the requirements for each of the FT3 configurations, providing the appropriate level of functionality, fidelity, and security.The LVC-DE test components included a UAS aircraft equipped with multiple sensors, live intruder aircraft, a surrogate UAS aircraft equipped with a prototype Command and Control (C2) radio system, a research prototype GCS, multiple DAA algorithms, constructive aircraft target generators, and virtual ATC workstations.Figure 1.LVC Environment Concept of Operations.An LVC environment promotes the integration of multiple live, virtual, and constructive data sources.
Stakeholders, Participants, and ResponsibilitiesThe NASA Integrated Aviation Systems Program provides oversight for the UAS-NAS project.The project office had the overall responsibility for FT3 flight test.NASA Ames, NASA Armstrong, NASA Glenn, NASA Langley, General Atomics, and Honeywell supported the project and were participants in the FT3 activity.The following is a brief description of responsibilities:NASA Ames Research Center (ARC): NASA Ames provided the HSI research requirements for subject pilot evaluation based on performance during scenario events.Subject pilots performed scenario tests from the Research Ground Control Station (RGCS) located at NASA Armstrong.ARC also provided Java Architecture for DAA Extensibility and Modeling (JADEM), one of the DWC algorithms used during Configuration 1 and the sole algorithm used during Configuration 2. The underlying LVC-DE infrastructure components used during the flight test were developed and tested at ARC.The air traffic controllers and confederate pilots, providing control and traffic fidelity during the test flights, were operated from facilities at ARC.
NASA Armstrong Flight Research Center (AFRC):NASA Armstrong was the responsible test organization for all test missions flown from AFRC and coordination of the airspace.AFRC provided the RGCS for subject pilot evaluation.Further, AFRC hosted the LVC-DE for data distribution between NASA Ames, Glenn, and Langley.AFRC also provided live aircraft used as intruders for both configurations, as well as the Ikhana (NASA 870) unmanned aircraft ownship platform for Configuration 1 encounters within the R-2515 airspace.
NASA Glenn Research Center (GRC):NASA Glenn was the participating test organization for all test missions flown with the T-34C (NASA 608).GRC provided the communication and control system interface and the UAS Surrogate ownship aircraft for use during Full Mission flights.Although initially planned, the NASA Glenn S-3B Viking aircraft was not available for use as a high-speed ownship during Configuration 1 testing.
NASA Langley Research Center (LaRC):NASA Langley provided a DWC algorithm (Stratway+) subsequently called Detect and Avoid Alerting Logic for Unmanned Systems (DAIDALUS) that was evaluated by subject pilots during Configuration 1 flight encounters.
General Atomics Aeronautical Systems Inc. (GA-ASI):GA-ASI provided hardware, software and integration support on the Ikhana UAS and specifically the Due Regard Radar and connectivity of the Ikhana to the LVC infrastructure.GA-ASI also provided scripted encounter requirements for autonomous aircraft response maneuvers to Traffic Alert and Collision Avoidance System (TCAS) alerting, as well as a DWC algorithm, CPDS (Conflict Prediction and Display System), for evaluation.Honeywell: Honeywell provided the software for the Surveillance Tracking Module prototype that contained the Honeywell Fusion Tracker.Honeywell also provided a TCAS II equipped intruder aircraft (N3GC) in support of both configurations.TCAS data recorded onboard the N3GC was made available to the rest of the FT3 test team to support data analyses.
FT3 OperationsThe two planned baseline test configurations were conducted in two phases.Configuration 1 (Scripted Encounters) was conducted out of NASA Armstrong over a four-week period beginning in June 2015.Configuration 2 (Full Mission) flights were scheduled to start data collection in July 2015 and continue over a four-week period, also run out of NASA Armstrong.NASA Glenn and NASA Armstrong provided the live aircraft, and NASA Glenn also provided the C2 system under test.One aircraft (a T-34C with call sign NASA 608) from NASA Glenn supported the test as a UAS surrogate and operated out of Bakersfield, CA.Over the course of FT3, data was scheduled to be collected from a total of 10 pilot subjects and evaluated over fifty aircraft encounters.Configuration 1 (Scripted Encounters) included a UAS aircraft equipped with representative sensors to detect cooperative (Mode 3/A/C transponder or ADS-B § ) and non-cooperative (no transponder or ADS-B) live intruder aircraft performing a series of scripted encounters to test timing and interoperability issues related to DAA algorithms.All flights were conducted in restricted airspace in order to minimize risk and maximize test efficiency.Configuration 2 (Full Mission) utilized a surrogate UAS aircraft flown from a research ground control station equipped with pilot displays that provided advisories of potential conflicts and loss of separation.The surrogate UAS was equipped with ADS-B (and simulated non-cooperative sensors) and operated in special use airspace.The locations of the surrogate UAS, along with the populated mix of live and virtual background traffic as well as the intruder aircraft, were translated into a virtual air traffic control airspace to provide and represent a realistic NAS environment.The pilot of the surrogate UAS was given a routine "mission" to perform while using the DWC advisories to maintain well clear of both the live and virtual aircraft.The UAS-NAS Project has ongoing research efforts focusing on the investigation of DAA, defined by the CA and DWC concepts, and its interoperability with air traffic control separation assurance.As shown in Figure 2, these concepts have distinct thresholds (which are a focus of the research) that may overlap in both temporal and spatial domains.Three different DWC algorithms were tested during Flight Test Series 3 (each during different test points).The first was the Conflict Prediction and Display System (CPDS), developed by General Atomics and Delft University of Technology, which provided its own display to the pilot. 6CPDS shows the ownship with proximal traffic and represents advisories as vertical and horizontal warning zones.The second was the Java Architecture for DAA Extensibility and Modeling (JADEM) technologies from NASA Ames originally to support resolution advisories for manned aircraft. 7The third was the Detect and Avoid Alerting Logic for Unmanned Systems (DAIDALUS) from NASA Langley Research Center. 8AIDALUS was derived from software that was originally developed to support tactical resolution advisories for manned aircraft and provided DWC informational guidance to the pilot in the form of "preventive" advisory bands displayed on the ground control station indicating headings and speeds that could potentially cause a loss of "well clear" separation with other aircraft.The DAA algorithm research was coupled with the investigation of the UAS GCS pilot environment, display of the DAA advisories to the pilot, and interaction between pilots using these advisories and air traffic controllers.As such, the Project is conducting a series of integrated human-in-the-loop simulations and flight tests to evaluate pilot and controller acceptance of the usability, display, and timeliness of different types of DAA alerting and maneuver guidance.
Overall Test Goals and ObjectivesThe Flight Test Series 3 high-level goals and objectives are provided here for convenience.For a detailed description please refer to the Flight Test 3 Objectives and Requirement Document (ORD). 9e Flight Test Series 3 testing served as the mechanism to test two primary technical goals:1) Integrate and evaluate the state of UAS concepts and supporting technologies defined within the scope of the UAS Integration in the NAS Project.Identify areas of future research and development emphasis and reduce risk for the flight tests 2) Evaluate and measure the effectiveness and acceptability of the DAA algorithms and displays to inform and advise UAS pilots, as well as the acceptability of resulting maneuvers and workload to the air traffic controller.These high-level test goals were further broken down into six general test objectives:1) Integrate and evaluate the state of UAS concepts and supporting technologies defined within the scope of the UAS Integration in the NAS Project as a function of traffic scenarios 2) Evaluate the pilot and controller acceptability of UAS maneuvers in response to DAA advisories 3) Assess the impact of variable winds on the execution of DWC advisories as well as interoperability with TCAS equipped aircraft 4) Assess the impact of communication delay and wind uncertainty on the air traffic controller workload and acceptability of DWC maneuvers 5) Evaluate the acceptability and performance of the LVC (i.e.software components of the distributed test environment) to provide a relevant environment 6) Collect data to improve batch simulation models.
Test Environment Infrastructure and ResourcesFlight Test Series 3 was conducted using two testing configurations defined as Configuration 1 (Scripted Encounters) and Configuration 2 (Full Mission).Figures 3 and4 describe a high-level architecture used during the two testing configurations; the specific hardware and details of the major LVC components are presented later in this section.Each configuration relied on the LVC-DE framework but utilized specific mix of hardware and software components to support each flight test objectives.The Scripted Encounters of Configuration 1 (Figure 3) was performed solely at the NASA Armstrong facility.The Ikhana and associated ground control station were integrated into the LVC-DE via an interface gateway.This gateway allowed for the ownship and sensed intruder position data from the UAS to be sent to the DWC algorithms with advisories then displayed to the UAS pilot.Figure 3 shows the high-level architecture system for the Scripted Encounters, showing the flight assets and LVC system components.It should be noted that only one DWC algorithm was presented to the pilot during each data collection test point.Figure 9 shows a more detailed description of the Scripted Encounters architecture.The flight test portion of Configuration 2 (Figure 4) was also flown out of NASA Armstrong, but the virtual air traffic control (ATC) and constructive aircraft processes were run at NASA Ames all translated into a common virtual airspace.The framework for the LVC-DE was supplied by components connected via the High Level Architecture (HLA) messaging infrastructure.A prototype research GCS along with a traffic display used to present DAA (DWC and CA) advisories to the pilot were integrated into the LVC-DE and configured to control the T-34C surrogate UAS aircraft from the ground.The LVC components sent and received data through a gateway connected to the HLA network.The constructive manned aircraft and ATC workstations communicated directly to the other LVC components via a local gateway and communicated with the other components via that gateway and the HLA.The constructive manned aircraft generators provided the required background traffic supporting a more realistic environment.The detailed description of this configuration is presented in Figure 10.
LVC Test EnvironmentThe DSRL at NASA Ames and the LVC Lab at NASA Armstrong ran the underlying infrastructure to connect the components of the flight test together.Figure 5 depicts the notional connection of the systems that supplied data to the DAA algorithms.Description of the specific components for each of the configurations is described in the sections below.During Configuration 1, all test control and data collection activities were conducted at Armstrong, with a connection to Ames for data monitoring only.The DSRL facility maintained the ability to receive, monitor, and record test data, audio and video in realtime as the test was being conducted at Armstrong.During Configuration 2, the DSRL facility played a more prominent role, as it supported the test with air traffic controller and pseudo-pilot staffing.Figure 5 shows the controllers situated in the DSRL lab and the pseudo pilots in the ATC lab (CVSRF) at Ames.Other components of the lab are described in detail in subsequent topics in this section.
Test Environment Voice CommunicationsThe FT3 test configurations used a combination of Quintron/DICES (digital integrated communications electronic system), Simphonics and Plexsys voice communication systems that utilized the Distributed Interactive Simulation (DIS) IEEE 1278.1A-1998v6 Standard Protocol.Figure 6 shows that the system was comprised of hardware and software that distributed multicast voice packets over a local area network.Packets were routed to specific receivers based on a simulated "frequency" that each station monitored.At NASA Ames, confederate pilots, and test engineers located in the CVSRF utilized Simphonics systems.The air traffic controller, the ghost controller, and test engineers located within the DSRL utilized PlexComm systems.Voice recording for Configuration 2 was accomplished using the Simphonics Recorder in the CVSRF ATC Lab.The firewall configuration that establishes a Virtual Private Network (VPN) connection between Ames and Armstrong did not permit User Datagram Protocol (UDP) (multicast) packets to pass through the VPN.Therefore, tunneling software that converts the UDP packets to Transmission Control Protocol/Internet Protocol (TCP/IP) and back to UDP on each end was installed on the routers at both ARC and AFRC.
Video DistributionVideo distribution was a critical feature of the test environment.During Configuration 1, several of the UAS pilot displays were run in the Ikhana GCS.In order to evaluate the system in real-time, the researchers had a need to monitor those displays.However, since the GCS is considered an active cockpit during flight, non-essential personnel were not permitted to observe. Figure 5 depicts the mechanism for streaming the live data to mirror the display as seen by the UAS pilot, as well as provide the Test Conductor with additional information to support management of the encounters.
Ikhana (Scripted Encounters)The Ikhana aircraft was configured with a due regard radar, ADS-B, GPS, and TCAS II equipment.In addition, the Ikhana also included the data fusion logic as well as the Sense and Avoid Processor (SAAP) algorithms.The Ikhana was controlled from the Ikhana GCS. Figure 9 depicts the communication interaction between the UAS, intruder, and ground antenna.The figure also shows the routing of the data and data flow between the Ikhana GCS and the AFRC LVC laboratory.It should be noted that the Ikhana transmitted the ownship and intruder position data to the GCS via Ikhana's existing SatCom-Ku band data link.
T-34C (Full Mission Encounters)During the Full Mission flights, the T-34C played the role of a surrogate UAS.The T-34C was not equipped with a radar, but had ADS-B, GPS, TCAS 1, and the CNPC equipment to aid with the testing of the Command and Control protocol being developed by the NASA Glenn researchers.Figure 10 shows the architecture required to support the Full Mission testing, including the LVC infrastructure, the aircraft, and the air traffic controller and pilot lab needed to interject the constructive aircraft (virtual intruders) into the testing.The CNPC equipment included a communications tower located at the Aeronautical Tracking Facility #2.This tower transmitted and received the line-of-sight communication between the T-34C and the GCS during the flight test.
Test Resources
High Level Architecture (HLA) and LVC GatewayThe framework for the simulation environment was supplied via the High Level Architecture (HLA) messaging infrastructure.The connections used a version of the IEEE 1516 standard Pitch portable Real Time Infrastructure HLA and Federation Object Model middleware to exchange information about the air traffic environment (aircraft state, flight plans, etc.) among the participants operating from distributed facilities. 10The HLA utilized Toolboxes to convert data from simulation components (e.g.air traffic control display) into its expected format.Messages from and to a distributed facility were routed to the HLA through the Toolboxes via the LVC Gateway.The LVC Gateway is a messaging server that routes data among local LVC clients, providing a single interface connection to the HLA from a distributed site.This is described in detail by Murphy,et. al. 5
Multi-Aircraft Control System (MACS) The underlying MACS software was used extensively in FT3 to provide aircraft control emulation, aircraft target generation, and a standalone UAS ground control station with integrated DAA algorithms.The following sections provide more detail on each of the functionalities.It should be noted that the MACS program interfaced with the test environment through an associated gateway called the Aeronautical Data link and Radar Simulator (ADRS).The use and connectivity of the MACS programs to the test environment via the ADRS is shown in Figure 5.
Air Traffic Control DisplayThe MACS program provided the ATC display functionality for the simulation used in support of FT3.An instance of MACS was used for each ATC sector position.Depending on the start-up configuration, it provided an emulation of the En Route Automation Modernization (ERAM) environment or Standard Terminal Automation Replacement System (STARS) environment. 13For the flight-testing MACS was configured as an ERAM sector display, with the ERAM style menus, electronic quick-access buttons, and flight plan read-outs.The emulation also had the capability to display Terminal area video maps.
Aircraft Target GenerationThe MACS SimMgr and MACS pseudo pilot programs provided simulated aircraft targets during testing (see Figure 11
UAS Ground Control Station (GCS)In order to support display development for the project, MACS was expanded to function as a standalone Pilot Station with built-in UAS characteristics providing a virtual GCS, called the MACS GCS (see Figure 12).This version of MACS had the NASA Langley Stratway+ DWC system integrated into its software.
Conflict Prediction and Display System (CPDS)Figure 13 shows a screen shot of the Conflict Prediction and Display System (CPDS) developed by General Atomics, which provides the GCS traffic display functionality.It shows the ownship aircraft with proximal surrounding traffic.During FT3, the CPDS provided the UAS pilot with situation awareness and DAA advisories.A key feature of the CPDS is to keep the pilot involved in conflict resolution before collision avoidance is necessary.There are still many research and safety questions to be addressed in order to standardize displays for UAS pilot use, but the CPDS, developed by GA-ASI and TU Delft, the Netherlands, builds on current display standards. 6The CPDS is a display that helps the pilot obtain situation awareness to anticipate and resolve potential conflicts before they become time-critical through the implementation of conflict Probes. 6igure 13.GA-ASI Conflict Prediction and Display System (CPDS)The CPDS includes a Cockpit Display of Traffic Information (CDTI), shown in Figure 13, which utilizes many features discussed in previous work, includes an ownship centric display with ownship depicted as a simple triangle at the center of the upper window.The UAS is at the apex of the triangle and the speed line helps the pilot understand where the UAS is projected to be in 10 seconds.A heading circle shows both current ownship heading and an abbreviation every 30 degrees.Traffic is displayed using directional symbols in accordance with the RTCA MOPS for Aircraft Surveillance Applications System (DO-317). 14Additional information displayed with the traffic symbol includes the Traffic ID (received through ADS-B), altitude relative to ownship (received through TCAS or calculated using ADS-B) and vertical rate sense indicator (received through TCAS or calculated using ADS-B).Range rings support the operator in determining the proximity of the traffic relative to the separation requirements.The pilot can select the maximum forward display range and half of that value is displayed to the sides of ownship.This also illustrates that ownship is positioned so that 2/3 of the display is in front and 1/3 is behind.
Vigilant Spirit Control System (VSCS)The Vigilant Spirit Control Station (VSCS) is the software component of a ground control station developed by the Air Force Research Lab (AFRL) that provides the command and control of live aircraft, simulator capabilities for virtual UAS, and an integrated traffic display of proximal traffic (See Figure 14). 15During the Scripted Encounters, the VSCS traffic display was used to provide the UAS pilot with a standalone (non-integrated) display of the DAA algorithm alerting and guidance.The traffic display showed DWC and CA conflict advisories in addition to intruder information such as call sign (if available), relative altitude, vertical velocity, and ground speed.During the Full Mission Encounters, the VSCS traffic display provided integrated DAA advisories and traffic, as well as the pilot interface to the command and control of the surrogate aircraft.
Research Ground Control Station (RGCS)The UAS Ground Control Station (GCS) capability was provided by the Research GCS (RGCS) at NASA Armstrong for Configuration 2 testing.The RGCS comprised of a hardware test-bed for UAS GCS information display and human factors concepts.It contained the monitors and computer systems that run the display systems under test.The RGCS used the VSCS as the UAS aircraft simulator, as well as the VSCS traffic display for its GCS traffic display instantiations.The RGCS is shown in Figure 15.20
Vehicle/Ground Interface ModulesEach of the ownship and ground control systems used in Flight Test Series 3 utilized software programs designed to convert the format of the data received from the aircraft into the format required by the LVC-DE infrastructure.For the Ikhana aircraft/GCS, this was the Input/Output Server (IOServer) developed by General Atomics.For the T-34 UAS surrogate used in Configuration 2, it was the Vehicle Specific Module (VSM), developed by NASA GRC.In addition, the VSM also had a software program that relayed the command and control messages from the VSCS to the autopilot on board the aircraft.
Remote User Monitoring System (RUMS)The Remote User Monitoring System (RUMS) software was used to facilitate the monitoring of the data collected during FT3.The RUMS server connected to the Gateway process and recorded all data messages to a remotely accessible database.Test participants were able to connect to the database via queries from an Internet web browser and display data of interest.For a more detailed description of RUMS, see the paper by Soler, et.al. 16
Flight Test Configurations and Data CollectionFT3 had distinct test configurations tracing back to the two system configurations (Figure 3 and Figure 4), corresponding to the specific objectives, hardware, and software required to conduct the experiment.The Scripted Encounter test was composed of three configurations with JADEM, Stratway+ and CPDS each separately providing advisories to the pilot of the Ikhana ownship.Configuration 2 introduced air traffic control and virtual/live intruders (Full Mission) in the mix with a live ownship flying a mid speed profile.The Full Mission test configuration utilized the RGCS UAS pilot station with the UAS pilots as subjects.Distributing the simulation among the NASA Centers allowed the project to utilize the existing simulation and test infrastructure already in place at those facilities.
Flight Test System Configurations
Configuration 1 -Scripted EncountersAll aircraft that participated in this flight test were equipped with Global Positioning System (GPS) navigation systems.The Honeywell C90 King Air (N3GC) was equipped with TCAS II version 7.1.A high level summary of the equipage installed on each aircraft is found in Table 2.The full description of the flight test activities are presented in the UAS-NAS ITE Flight Test 3, Flight Test Report. 1 Some of the materials are replicated here for convenience.This test configuration evaluated the advisories generated by the DWC and CA algorithms fed by data from live aircraft during flight.** In these tests the Ikhana ownship aircraft was flown against either one or two manned intruder aircraft.Both DWC and CA algorithms were evaluated.Figure 9 (above) shows the system design of the Scripted Encounters in more detail.The DAA algorithms were evaluated using both mitigated and unmitigated encounters.Unmitigated, also known as fly-through and non-maneuvering encounters, were designed for each aircraft to fly the route as planned all the way to the predicted closest point of approach (CPA) regardless of alerting displayed.These encounters evaluated each System Under Test's (SUT) ability to maintain the correct alerting thresholds.The mitigated encounters were designed for the test aircrew to maneuver the ownship aircraft away from the Collision Avoidance Threshold or Near Mid-Air Collision threshold and maintain a well clear distance between intruder and ownship.Table 3 categorizes each SUT.The JADEM DWC algorithm utilized the VSCS traffic display to show resolution maneuvers to the pilot of the Ikhana.JADEM connected via a Gateway, receiving data from the GA-ASI IOServer and sending advisories to a standalone VSCS traffic display.The MACS GCS was used to present the Stratway+ advisories to the Ikhana pilot during the Scripted Encounters Test Configuration.CPDS is a standalone display in the Ikhana GCS.
Scenario DevelopmentConfiguration 1 encounters used a low-speed ownship (< 210 kts).A subset of the scripted encounter geometries were designed to test the AutoResolver/JADEM algorithm.The algorithm was further divided into two portions, AutoResolver 1 and 2, with different alerting thresholds.These encounters were divided ** Flight Test Configuration 1 was further defined into two distinct groups, Configuration 1a and 1b.Configuration 1a involved flight test encounters using NASA's Ikhana aircraft as the low-speed (< 210 knot IAS) ownship.Configuration 1b was planned to incorporate a high-speed (250 knot IAS) ownship aircraft, but the test points associated with high-speed ownship encounters were not completed during flight-testing.in three types: fly-through/maneuvering, climbs/descents/level (Figure 16), and fly-through TCAS alerting (Figure 17).Aircraft in the fly-through scenarios fly towards a target CPA of 0 NM horizontal offset.Actual CPA is not critical since any CPA can be compared to the CPA predictions.It was desirable to fly close enough to trigger a DWC alert.The maneuver encounter scenarios were ones in which the pilot maneuvers the aircraft as suggested by the specific DWC advisory shown on the traffic display.These fly-through/maneuvering encounters tested various angles of: 0°, 45°, 90°, 110°, and 180°, with additional 45° and 90° blunder angles.The blunder maneuver happens when the intruder aircraft encroaches upon the conflict alert volume of the ownship, as defined in the DWC alerting algorithm.All encounters had a minimum 1,000 ft vertical separation that was offset artificially within the algorithm, so as to make the ownship and intruder appear co-altitude.Other scripted encounters were designed to evaluate interoperability between TCAS and DWC systems.DWC systems are expected to help the pilot keep the ownship well clear of an intruder.Ideally, the DWC alert would trigger long before the TCAS alert.These encounters were designed simply as a fly-through to gather data.Because the vertical profile was planned to go as close as 300 ft, a "build-up" approach was used; the encounters were flown first at 1,000 ft, then 500 ft, and finally, 300 ft.To stay consistent, all of these encounters employed the 3,000 ft lateral separation (although only the 300 ft encounter required the lateral offset).These test points had encounter angles of 0°, 45°, and 90°.More details are available in UAS-NAS ITE Flight Test Series 3.
Configuration 2 -Full MissionFull Mission flight encounters, also identified as Configuration 2, followed a pre-planned route of a flight plan that represented a 40-minute fictitious fire line support mission flown in an emulation of Oakland Center (ZOA) Class E airspace (sectors 40/41) previously used during prior tests (including IHITL, Part Tasks 4, PT4B, and PT5).The objective of the Full Mission was to gather real flight data to help improve simulation modeling.These missions involved a single ownship aircraft (UAS surrogate, T-34C) flown from the RGCS, with a safety pilot on board the aircraft.The surrogate ownship navigated a flight plan, with two live intruder aircraft performing scripted flight encounters.In order to provide a relevant test environment, multiple virtual aircraft were also displayed to the UAS pilot in the RGCS (though not to the safety pilot onboard the surrogate) to increase the number of encounters per test run as well as add the expected background air traffic control chatter.(See Figure 10 above for details of the Full Mission.)Air traffic controllers were fully engaged during Configuration 2 testing, issuing commands to both virtual aircraft (flown by confederate pilots) as well as the UAS ownship pilot.The air traffic controllers were confederates for these scenarios, working closely with the Test Conductor to maneuver the live and virtual intruder aircraft to ensure the researcher specified encounter requirements are met.This task was complicated by the fact that during the 40-minute test run, the ownship aircraft would have maneuvered out of its prescribed path due to previous encounters.A retired Oakland Center controller with over 25 years of operational experience controlling aircraft in this airspace developed the traffic scenarios.The basic traffic scenario was created based on live traffic recordings.Two scenarios were developed to be high workload (high traffic density, arrival sequencing for local Terminal airspace), while the remaining two were developed to be nominal workload (average traffic density, non-arrival push to local Terminal).Since high and nominal workloads are subjective, these scenarios were also reviewed and were rated as high and nominal by a second retired Oakland Center controller.The pilots in the UAS RGCS of the UAS surrogate aircraft were the subjects of the Full Mission configuration.They controlled the aircraft via the VSCS software run from the RGCS at NASA Armstrong.The VSCS traffic display was used to show resolution maneuvers.Maneuver resolutions facilitated by the DWC system, which was derived from the JADEM technologies connected via a Gateway, receiving data from VSCS, the UAS surrogate ownship, and MACS SimMgr.
Scenario DevelopmentFigure 18 shows the ownship fire-line route and the expected paths and intercepts of the two live intruders (Intruder 1 -red, Intruder 2 -blue).Figure 19 shows the ownship, Intruder 1 (live), and Intruder 2 (live) routes overlaid, as well as expected virtual encounters.NASA 608 acted as a surrogate UAS for this configuration, piloted on the ground from the RGCS, with a safety pilot on board the aircraft.The VSCS translated the displayed route of flight of the aircraft to make it appear to the pilot to be in the ZOA airspace.The RGCS pilot had direct control over the lateral maneuvers of the UAS aircraft.However, due to limitations in the interface to the autopilot, other maneuvers such as airspeed or altitude changes were sent from the RGCS to the research computer on board NASA 608 and electronically sent to the pilot on the aircraft to maneuver manually.
LVC Test Environment AnalysisData collected in the flight test are disseminated to various research teams in the UAS-NAS project.The SSI team's focus is on the performance of the DWC system, while the HSI team performs analysis on the pilots' experience with the representation of the alerting methodology.This report focuses on the LVC system performance, relating to system latency time between flight test components, timing of the live flight data, and environment feedback from the subject matter experts.For the purposes of these analyses, the time associated with the location of an aircraft (whether measured, calculated, or inferred) will be referred to as the time of applicability (TOA).This is assumed to be the most accurate time associated with a given position.
System Timing (Latency and Update Rate)To collect the required data for analyzing the latency between specific components, software on board the aircraft, in the MACS target generators, and throughout the LVC infrastructure were instrumented to output message timing information.Since some of the timing output adds a significant amount of data to the files, some of the analyses were performed during the system checkout.This also allowed for validation of the system by the researchers prior to data collection.
ARC to AFRC Facility LatenciesFor the Full Mission configuration, the background/intruder virtual/constructive aircraft position updates were generated at NASA Ames and transmitted to the RGCS and DWC algorithms running at NASA Armstrong via the LVC network.The network was an encrypted Virtual Private Network (VPN) running over NASA's existing network administered by NASA Integrated Services Network (NISN).The latencies between the ARC and AFRC facilities could impact the outcomes of the flight test if they exceeded the latency requirement for transmission of ADS-B data (uncompensated 0.6 seconds). 17 order to calculate these latencies, the time on each computer in the LVC system was synchronized to a GPS device.Network Time Protocol (NTP) was used for clock synchronization between computer systems.NTP provides Coordinated Universal Time (UTC) including scheduled leap second adjustments.The NTP uses a hierarchical, semi-layered system of levels of clock sources.Each level of this hierarchy is termed a stratum and is assigned a layer number starting with 0 (zero) at the top.The stratum level defines its distance from the reference clock and exists to prevent cyclical dependencies in the hierarchy.It is important to note that the stratum is not an indication of quality or reliability.To account for any (even minimal) drift between two computers, a pair of simple programs were created.The programs "tquery" and "timesvr" were used to baseline the time offset between computers residing in two remote facilities of the LVC environment.The tquery program measures the time it takes to send a message to timesvr and receive the reply.The offset is calculated by comparing the average of the tsquery send and receive time on the local server to the time the remote timesvr program recorded as the time the original message was received.The tquery program has the ability to run large sample pools to determine an average and standard deviation on the final difference output by the program.Table 4 contains the average latency values calculated between the DSRL Laboratory at NASA Ames Research Center and the other participating laboratories by the tquery/tserver programs.The unimpeded tquery/tserver offsets were recorded over a 10-minute period prior to each test day with values recorded every 5 seconds.The system offsets were very small, with an observed average of 5.5 milliseconds (ms) (0.0055 sec) between Ames and Armstrong, well within tolerance of the required latencies.Table 4. tquery offset times between NASA Ames DSRL and NASA Armstrong LVC Labs.
Test NameAverage Network Latency (ms)
Standard Deviation (ms) Max Latency (ms)Ames to Armstrong 5.5 0.4 9
MACS Virtual/Constructive Data Timing
Scripted EncountersNo simulated aircraft were used during the Scripted Encounters flight test configuration.
Full MissionMACS target generation was used for the Full Mission configuration to provide constructive background traffic and virtual intruders.Refer to Figure 10 for a depiction of how the MACS target data were integrated into the LVC system.
Virtual/Constructive LatencyThe MACS software has internal processing and message passing cycles that add latency to the time the LVC system receives the aircraft position updates.Coupled with the distributed facility latency, the total uncompensated latency needs to be under the 0.6 second ADS-B latency requirement.Table 5 contains the total average latencies values calculated between the MACS target generation programs running at NASA Ames and the LVC Gateway at NASA Armstrong.MACS and LVC Gateway log files were compared to determine the overall latency.Due to the labor-intensive method to match the data from the MACS output files to the LVC log, the analysis was performed for a single aircraft (with spot-checks for other aircraft).An average of 62 ms with the maximum latency of 132 ms was observed.This is well below the 600 ms required (400 ms recommended) uncompensated latency for ADS-B data.
Virtual/Constructive Update RateIt was intended for virtual/constructive aircraft to emulate a 1-second ADS-B data stream to the DWC algorithms.Figure 20 and Table 6 show histograms and supporting data regarding the position updates received by the LVC from the MACS target generators.The data were collected on 12 August 2015 in the LVC Gateway during a successful flight with a pilot controlling the surrogate T-34 aircraft from the RGCS.The vast majority of the data (97.2%)falls within 1 std dev of the average update rate of 1.01 seconds.The histogram uses a logarithmic scale to show a small number of targets grouping around the 2 and 3 second update rate.It also shows corresponding targets well below the 1-second rate.The reason for these occasional update issues is under investigation, though most have been attributed to the underlying asynchronous generation and message passing cycles inherent in the MACS and LVC software.
Live Aircraft Data TimingThe Scripted Encounters and the Full Mission configurations were conducted with live aircraft as the ownship and intruders.In all tests, the ownship provided the ownship and intruder position data used by the DWC algorithms via a communications link to the ground.This section describes the timing of the data from each of the data sources.
Scripted Encounters (Ikhana)The Ikhana transmitted all DWC position data through the SatCom Ku-band communication link to the Ikhana GCS.On board the Ikhana, all input data were processed by the SAAP, which recorded the time received.Every second the SAAP transmitted the position of the ownship and intruders to the Ikhana GCS in a message called the Digital Transmission Interface Frame (DTIF) record.On the ground, the DTIF records from the Ikhana were processed internally by the CPDS and sent to the LVC Gateway process via the IOServer.The SAAP and LVC Gateway log files are the primary sources of data for the latency analyses.The LVC Gateway is recorded in plain comma separated value text and is easily parsed.The SAAP data is binary and requires support from GA-ASI to parse the data into a usable format.It should be noted that the SAAP data files contains a tremendous amount of data, much of which is proprietary, so the parsed data files only contain the limited data needed to complete the analyses.
Live Aircraft LatencyThe TOA of the ownship and intruder aircraft position reports is recorded in SAAP data messages sent from the Ikhana aircraft.The time from these messages is compared to the time the LVC Gateway received the message to determine the overall latency of the data.It should be noted that inherent latency due to the use of a SatCom link is built-in to the latency values obtained in these analyses.The data provided by GA-ASI for these analyses does not include information that supports explicit measuring of the air to ground timing, though it has been estimated to be between 0.3-0.5 seconds during these flight tests.
Ikhana OwnshipThe Ikhana Ownship data are provided by internal processing of the on-board GPS and the internal Inertial Navigation System (INS), sent to the SAAP at 50Hz, 10Hz, and 1 Hz rates.The data transmitted to the GCS and DWC algorithms come from the 1 Hz cycle and include the TOA of the ownship position.The time from these messages is compared to the time the LVC Gateway received the message to determine the overall latency of the data.Table 7 shows sample latency values for the Ownship data from each of the flight test days.What is immediately striking is that the observed average latency for the Ikhana self-reported data ranges between 2.14 and 4.13 seconds on the sample taken over each of the test days.These values were expected to be much lower based on the observed SatCom latency of tracker processed intruder data (described in the next section).Though an independent source of position data for the Ikhana was not available for analysis, the 10 and 50 Hz data recorded in the SAAP did indicate a potential problem with the timing of the 1 Hz data sent to the GCS for processing.Figure 21 depicts the time plotted against the reported latitude of the position taken from the SAAP DTIF records (sent to the ground) and from internal 10 Hz self-reports from the 17 June 2015 flight test.Comparing these two data sources shows a clear shift of the Ikhana position by approximately 3 seconds.In addition, if the times of the post-processed (10 Hz) data source are compared to time received on the ground by the LVC, they differ by an average of 0.39 seconds (which corresponds with the estimated internal processing and air to ground latency).Further analysis of the internal SAAP data recordings indicate an issue with the time recorded in the DTIF message, not the underlying time/position accuracy of the Ikhana sensors.It has been determined by the GA-ASI team that a reference to a low fidelity clock on board the aircraft when populating the ownship position caused the error.A fix for this time issue has been determined by the GA-ASI team and will be incorporated for future flights.
Ikhana IntrudersThe live intruder position data is captured in the SAAP output files (along with the ownship data) and includes ADS-B, TCAS, and the Due Regard Radar data.For this analysis, we are interested in the latency of the primary Honeywell King Air intruder aircraft and only looked at data that had the appropriate International Civil Aviation Organization (ICAO) identifier (N3GC) in archive of SAAP messages sent to the GCS.The latencies were calculated by comparing the TOA recorded in the SAAP DTIF records to the time the message was received by the LVC Gateway.Table 8 contains a record of the observed N3GC latencies.On days that ADS-B was not part of the flight-testing, no intruder data was analyzed, since the call sign was not part of the recorded data.Notice that there is a notable difference in the observed latencies on different days.When the data fusion and tracking algorithm was under test (7 and 10 July), the tracker algorithm compensated for the existing latency at the time the message was being sent to the GCS and extrapolated the intruder position synchronized to the message time.This time can be associated with the observed air to ground data link time (with a minimal tracker processing time included).When the data fusion/tracking algorithm was not used, the available ADS-B time and location was recorded without additional compensation, adding to the observed intruder latency.The use of fusion/tracking is noted in the right hand column of Table 8.The position reports for the Ikhana provided by the internal GPS and INS are internally recorded at a higher than 1 Hz update rate, so the 1 Hz cycle used to send data from the aircraft to the ground is consistently populated with valid data.Analysis of these data sources was outside the scope of this report.
Ikhana IntrudersAs noted above, the King Air aircraft provided by Honeywell was installed with an ADS-B system that provided position reports to the system installed on the Ikhana aircraft.These data were recorded in the SAAP and sent to the LVC for recording.Data from 7 July 2015 and 17 June 2015 were analyzed to determine example update rates observed with the King Air aircraft recorded with and without the data tracker function, respectively.With the tracker function, the most recent ADS-B data available is extrapolated to current time and sent to the GCS; without the tracker, only the most recent data is sent.Since the data are subject to data link message dropouts, the observed update rate may be different then the source update rate.To mitigate (and demonstrate) this issue, samples with and without dropouts were also analyzed.Table 9 provides a summary of these analyses.The best case is with the tracker on a sample of data with no dropouts, with a perfect 1-second update rate.When data drops are included, a slightly greater update rate was observed (1.026 seconds).Without the tracker function, variability in the underlying ADS-B data can be observed with a slightly higher update rate (1.003 second, no drop-out, 1.078 seconds including dropouts).Note: the SAAP also provided TCAS, Due Regard Radar, and outputs from the sensor fusion, however analyses of these data are outside the scope of this report.
Full Mission (T-34C)As described above, the T-34C transmitted all position data through the prototype CNPC radio system provided by the NASA Glenn Communication sub-project.The TOA of the ownship comes from an internal GPS and INS installed on the aircraft.The intruder TOA comes from the ADS-B data recorded on the T-34C.Both the ownship and intruder aircraft position reports are transmitted to the VSM running at NASA Armstrong.The TOA from these messages is contained in the "Time Created" field in the LVC Gateway.
Live Aircraft LatencyThe latency of the live aircraft is calculated by comparing the "Time Created" and the "Time Received" fields for each aircraft position recorded in the LVC Gateway output files.It should be noted that time sent for the aircraft to the VSM only sends the fraction of seconds since the last 1-second epoch.It is assumed that the clock on the VSM is in sync with a GPS level accurate clock, but time differentials were not checked using tquery.If this is not the case, some time messages could be off by one second (depending on the drift of the VSM clock and the time the message is received).This fact may explain some of the interesting timing described below.
T-34 OwnshipThe ownship position for the T-34C was transmitted to the VSM twice per second via the prototype CNPC data link.The VSM constructed the TOA of the position, populated the "Time Created" field of the LVC message, and sent the message to the LVC Gateway where the data were recorded.Table 10 shows sample latency values for the Ownship data from each of the flight test days.Though an average latency between 100 and 200 ms is within the predicted estimate for a line of sight link used by the CNPC communication system, the relatively large standard deviation and negative latencies imply issues with the time-stamping or system clocks reporting wither the Time Created or the Time Received.This needs to be investigated further to resolve.
T-34C IntrudersFull Mission intruders were equipped with ADS-B.The T-34C ownship ADS-B unit received position updates and sent them to the VSM over the CNPC data link.The ADS-B time was constructed from the time sent in the position message and used to populate the "Time Created" field in the LVC message.Table 11 contains the basic information for the data collected during the 12 August 2015 flight test.While the average latency at first glance appears reasonable (0.535 seconds) the standard deviation (0.270 seconds) and minimum (-0.017 seconds) betray an underlying issue that needs to be investigated.A negative latency means that the timestamp on the position of the target is later than the time the message was received.This can only happen if there is a discrepancy in the clock that records the target time or with the clock recording the time received.In addition, during system shakedown an average of 0.92 seconds was observed, indicating a potential clock error during the time of data collection.The update rate of the live aircraft is calculated by comparing the "Time Created" for successive fields for each aircraft position recorded in the LVC Gateway output files.During the three days of data collection, a significant number of missing track updates from the Intruder aircraft were observed.The dropout rate for one of the live intruders during a 10 August 2015 test run was investigated.Over the 47 minutes the intruder aircraft was in flight, a total of nearly 15 minutes of tracks were missing.There were 12 occurrences of 10 more concurrent missing tracks, with the greatest dropout period recorded at over 28 seconds.During these dropout periods, ownship data was still being received (though periods of ownship missing tracks at other times during the same flight were also observed).It is unknown whether the missing data were caused by reception issues with the ADS-B units on the intruder or the T-34C, the prioritization of message passing on the communication link between the T-34C and the ground, or handling of the data on board the T-34C research computer.Figure 21 depicts a 5-minute sample of the intruder track hits that depicts the missing track issue.Due to the missing track issue, only spot checks of the update rate for the ownship and intruders were performed.As expected, during times of no data dropouts, ownship data was sent and received at a 2 Hz rate and the intruder position updates (supplied by ADS-B) were at an average 1-second rate.Table 12 shows the basic statistics for a sample of the ownship and intruder update rate during a no dropout time period.
Position Time Usage by DWC AlgorithmsThe DWC algorithms received all of the required data inputs through the LVC Gateway.With each position report, the LVC Gateway provided the DWC algorithms with two time stamps, namely the "Time Created," which was intended to indicate the TOA of the position, and the "Time Received," which was the time the LVC Gateway received the position report from the source.For Flight Test Series 3, the JADEM DWC system used the "Time Received" as the time coordination for their position data for both the intruders and the ownship.While the Stratway+ algorithm used the "Time Created" value.Since "Time Created" was intended to represent the aircraft position TOA, it is the preferred time for the DWC algorithm to use.The SSI team at NASA Ames is making changes to their software to fix this issue in preparation for FT4.The impact to the use of the "Time Received" by the JADEM software is described below.
Scripted EncountersUnfortunately the aircraft position TOA sent from the Ikhana was overwritten on the ground when received by the IOServer, hence the reason for processing the internal SAAP data to obtain the TOA for the analyses above.As a result, the "Time Created" value reflected the time the position message was received by the IOServer, losing the original TOA.This effectively synchronized the ownship and intruder aircraft to the same time.It also mitigated any issue with using the "Time Received" vs the "Time Created," since both were essentially the same.However, due to the ownship timing issue described above (Section 4.1.3.1.1.1),when the data fusion/tracker was used, this actually provided mitigation to that problem.The data fusion/tracker algorithm was designed to synchronize the intruder to the ownship time, based on the periodic time.Overwriting the TOA time effectively shifted the ownship time to coincide with the intruder time, which was the intention of the track algorithm.Referring back to Figure 21, the time of the ownship aircraft reported to the GCS would be shifted to coincide with the internal SAAP value.The time of both aircraft were delayed by the equivalent of the air to ground transmission latency, but they were synchronized.When data fusion/tracker was not in use, the overwriting of the TOA has a real impact that should be taken into consideration.Assume that the ownship TOA issue for the Ikhana ownship could be attributed to an internal timing error and the adjusted time was closely in line with the time the message was constructed to be sent from the Ikhana to the ground (which is a plausible assumption, since the position data of the ownship is updated at a 10Hz rate).If true, overwriting the TOA will give an error equivalent to the intruder latency (minus the air to ground transmission latency) for each position report.Based on Table 8 this falls between 1 and 2 seconds (average) during the analyzed non-data fusion days.If these assumptions hold true, the time for the intruder aircraft is 1-2 seconds out of sync with the Ikhana.Depending on closure rates of the encounter, that could be as much as a 0.25 mile position error.Though it does not impact these analyses, it should be noted that the "Time Created" value that was provided by the IOServer (and overwrote the original TOA) also truncated the time to the nearest second.This is being addressed for FT4, "Time Created" will provide resolution to the nearest millisecond.
Full MissionFor the Full Mission configuration, the T-34C and VSM software module did not overwrite the "Time Created" TOA value on the ground.However, JADEM was used for Full Mission and thus used the "Time Received" value.As with the Scripted Encounters, this synchronized the ownship and intruder positions to the same time.Since the researchers did not analyze the FT3 Full Mission data, analysis of this impact was not performed.However, the DWC developers are modifying JADEM to use "Time Created" time value for FT4.
Subject Matter Expert FeedbackDuring the FT3 testing period, questionnaires were developed to gather feedback from the air traffic controllers (located at Ames' DSRL lab) and the UAV pilot (flying the UAV from Armstrong).The nature of these questionnaires focused on the LVC environment (controller) and the ground pilot station test environment (UAV pilot).The set of questionnaires are included in Appendix A. Of note, the HSI team distributed a separate set of questionnaires to the pilot, with an emphasis on pilot interaction with the implementation of the DWC system, but these results are not included in this report.Five questionnaires were distributed, three for air traffic controllers, one for ghost controller, and one for the UAS pilot.Two Oakland Center controllers participated during the Full Mission testing period.Each controller participated during a one-week period.They were not the subjects of the test, but provided feedback on the ATC environment.In addition, a research staff member (a retired controller) acted as a ghost controller.The role of a ghost controller is to ensure that the intended traffic (of the scenario) is set up correctly with the virtual aircraft before making handoff to the two Oakland Center controllers.A total of three pilot participants filled out questionnaires concerning the physical environment that hosted their ground pilot station.
Controller Participant EvaluationControllers filled out the Form 1 questionnaire during their training period and again at the exit of their participation period.They filled out Form 2 at the end of each test run and Form 3 also at the exit period.The ghost controller gave feedback at the end of each run using Form 4. Due to the extremely small sample size, this report provides only anecdotal comments from these questionnaires.Form 1 asked controllers to characterize the scenario files and their impact on controller workload, as compared to real world traffic.The controllers commented that the low altitude traffic was typical of current day traffic density, but that the number of encounters was somewhat more frequent then current traffic, which was intentional due to the flight test schedule and to capture as much encounters as possible during each test run.However, they stated that the encounters were neither easier nor difficult to detect and resolve and the workload was either typical or lower than normal.Post run questionnaire (Form 2) tried to capture simulation constraints that may impact 'real world' operational procedures.An overall theme that emerged was that pilots were not following check-in and maneuvering protocols.A controller commented about waiting for pilots to check in, "In the real world, we wouldn't wait for the pilot to ask for a turn, we would turn them for traffic.Also, the VFR aircraft pilots calling for advisories would check on with position and altitude" and "UAV pilot was turning, climbing, and descending off course without a clearance.IFR aircraft that turns in this easy is a pilot deviation."Controllers were asked for their feedback about the ATC display they used to conduct their work.They stated the quality of the simulated ATC display was adequate for the controlling traffic and performing routine tasks.However, they wished for the installation of their Display System Replacement (DSR) keyboard and trackball setup, for more efficient operations, instead of using the standard keyboard and number pad.The ghost controller was asked to fill out Form 4, to record the workload required to support each run.On average, the ghost controller stated that effort to set up the encounters were 'greater' or 'much greater than normal' to perform additional coordination effort, with a corresponding increase in workload to realized the encounter-of-interest per test configuration.
Pilot Subject EvaluationThe pilots were asked to evaluate two aspects of their work area: quality of the voice communication system, and the physical environment that they were working in.Concerning the clarity of the voice system, they reported that it was typical, somewhat clearer, and much clearer than normal, but not noisier.And reported that any observed delay in the reply time of the system was not noticeable.In addition, two out of three pilots reported that frequency congestion was typical, while one reported much more congestion than normal.The physical simulation environment questions asked about the lighting and noise level surrounding the ground pilot station work area.Two pilots reported that the lighting condition was at a typical level, while one said it was somewhat darker than normal and two of three said they would like to have the ability to make lighting adjustment.On the topic of noise level, one pilot stated that it was at a typical noise level, while the other two said it was somewhat quieter than normal, with missing computer fan and cooling system noises.They also stated that there was no ambient noise that was distracting them.They were also asked to rate the level of interaction with other people or systems in their due course of conducting their tasks.Pilots noted that the level of interaction was typical to somewhat less than normal.All said that the pilot station room temperature was set 'just right' though they would like to have the ability to make adjustments.Unfortunately, the pilot interaction with the flight environment was a primary cause of the early conclusion of the Full Mission configuration after only 3 test subjects.During test conduct, the pilots in the RGCS noticed significant delay between the time a maneuver was input into the VSCS system and when the aircraft could be observed to start its maneuver (instances of approximately 5 seconds for lateral maneuvers and 12 seconds for horizontal maneuvers were noted).This caused some confusion with the pilots and occurrences of repeated maneuvers being sent to the aircraft (due to lack of feedback that the original maneuver was received).Coupled with the significant observed data dropout described above, cessation of the flight test was the correct decision.Figure 1 . 3 Figure 2 . 5 Figure 3 . 7 Figure 4 . 8 Figure 5 .Figure 6 .Figure 7 .Figure 8 . 12 Figure 9 . 14 Figure 10 . 16 Figure 12 .13253748567812914101612Figure 1.LVC Environment Concept of Operations.An LVC environment promotes the integration of multiple live, virtual, and constructive data sources................................................................................ 3 Figure 2. Sense and Avoid and Separation Assurance Interoperability.The Sense and Avoid concept encompasses the collision avoidance, detect and avoid, and boundary with separation assurance......... 5 Figure 3. High-Level Architecture of Configuration 1 (Scripted Encounters) ................................................ 7 Figure 4. High-Level Architecture of Configuration 2 (Full Mission) ............................................................ 8 Figure 5. High-level Ames and Armstrong LVC lab usage and system connectivity diagram ..................... 10 Figure 6.LVC Voice Communication configuration for FT3 at ARC .......................................................... 11 Figure 7. AFRC Voice Communication configuration for FT3 ..................................................................... 11 Figure 8. Video distribution and streaming connectivity used for monitoring and test data display............ 12 Figure 9. Configuration 1 (Scripted Encounters) communication and data flow architecture...................... 14 Figure 10.Configuration 2 (Full Mission) communication and data flow architecture.Please see Appendix B for an enlarge version of this diagram................................................................................................ 15 Figure 11.Multi-Aircraft Control System (MACS) pseudo pilot displays.................................................... 16 Figure 12.MACS Ground Control Station displays...................................................................................... 17
Figure 13 .Figure 14 .Figure 16 .Figure 18 .Figure 19 .Figure 20 .Figure 21 .13141618192021Figure 13.GA-ASI Conflict Prediction and Display System (CPDS) .......................................................... 18 Figure 14.Vigilant Spirit Control System (VSCS) Integrated Traffic and Tactical Situation Display......... 19 Figure 15.Research Ground Control Station layout...................................................................................... 20 Figure 16.JADEM Scripted Encounter Angles 1: fly-through, maneuvering climbs, descents, and level... 22 Figure 17.ARC Scripted Encounter Angles 2: fly-through TCAS alerting.................................................. 23 Figure 18.Ownship fire line route (magenta) with live intruder intercepts (at arrow heads)........................ 24 Figure 19.Fire line routing (green), ownship and live/virtual intruders........................................................ 25 Figure 20.Target update rate from MACS virtual target generator.............................................................. 28 Figure 21.Comparison of the Ikhana self-reported location vs. the post processing of the internal SAAP data from the Ikhana.The Ikhana output is shifted by approximately 3 seconds early (to the left)...... 30 Figure 22.Periods of missing tracks from the intruder data feed over a 5-minute time period.................... 34
FT3objectives and test infrastructure builds from previous UAS project simulations and flight tests.The basic test infrastructure has been used during the Integrated Human in the Loop (IHITL) simulation, Part Task 4 (PT4) HITL simulation, Part Task 5 (PT5) HITL simulation, UAS Controller Acceptability Study HITL simulation, and GRC Comm prototype Control and Non-Payload Communications (CNPC) system ground and flight tests.NASA Ames (ARC), NASA Armstrong Flight (AFRC), NASA Glenn (GRC), and NASA Langley (LaRC) Research Centers shared responsibility for conducting the tests, each providing a test lab and critical functionality.In 2014 the UAS-NAS project also participated in the DAA Initial Flight Test (also known as the ACAS-Xu Flight Test), supporting data collection for Collision Avoidance (CA) and DWC technologies.These tests significantly contributed to building up infrastructure and procedures for FT3 as well.
Figure 2 .2Figure 2. Sense and Avoid and Separation Assurance Interoperability.The Sense and Avoid concept encompasses the collision avoidance, detect and avoid, and boundary with separation assurance.
Figure 3 .3Figure 3. High-Level Architecture of Configuration 1 (Scripted Encounters)
Figure 4 .4Figure 4. High-Level Architecture of Configuration 2 (Full Mission) 2.1 Test Facilities
Figure 5 .5Figure 5. High-level Ames and Armstrong LVC lab usage and system connectivity diagram
Figure 6 .6Figure 6.LVC Voice Communication configuration for FT3 at ARC Figure 7 depicts the NASA Armstrong installation of two ASTi ACE-RIU HLA to Audio converter devices, which converted the voice-over-IP DICES format running through its channel banks to DIS IEEE 1278.1A-1998v6 Standard Protocol utilized by the (also newly installed) ASTi Voisus Server, which communicated with the DIS voice systems at Ames.The resulting configuration enabled Armstrong to leverage its existing DICES installations for all positions within the RAIF, SAF, and Ikhana GCS required for use by the RGCS pilot, researchers, test engineers, and control room.
Figure 7 .7Figure 7. AFRC Voice Communication configuration for FT3
Figure 8 .8Figure 8. Video distribution and streaming connectivity used for monitoring and test data display.2.2 Test Aircraft
Figure 9 .9Figure 9. Configuration 1 (Scripted Encounters) communication and data flow architecture.
Figure 10 .10Figure 10.Configuration 2 (Full Mission) communication and data flow architecture.Please see Appendix B for an enlarge version of this diagram.
).At the beginning of each FT3 simulation, MACS SimMgr read the initial conditions and flight paths from an input scenario file.Aircraft were then assigned to the MACS pseudo pilot stations where the aircraft position updates were generated based on the flight paths and aircraft model data.This aircraft data was distributed to each test participant as required.MACS used a four degree-of-freedom trajectory engine to update the location of the aircraft at one-second intervals (emulating ADS-B).The simulated targets flew a predetermined flight path emulating either an Instrument Flight Rules (IFR) or Visual Flight Rules (VFR) aircraft providing background and intruder traffic.
Figure 11 .11Figure 11.Multi-Aircraft Control System (MACS) pseudo pilot displays.
Figure 12 .12Figure 12.MACS Ground Control Station displays.
Figure 14 .14Figure 14.Vigilant Spirit Control System (VSCS) Integrated Traffic and Tactical Situation Display.
Figure 15 .15Figure 15.Research Ground Control Station layout.
Figure 16 .16Figure 16.JADEM Scripted Encounter Angles 1: fly-through, maneuvering climbs, descents, and level.
11
Figure 17 .17Figure 17.ARC Scripted Encounter Angles 2: fly-through TCAS alerting.
Figure 18 .18Figure 18.Ownship fire line route (magenta) with live intruder intercepts (at arrow heads).
Figure 19 .19Figure 19.Fire line routing (green), ownship and live/virtual intruders.
Figure 20 .20Figure 20.Target update rate from MACS virtual target generator.
Figure 21 .21Figure 21.Comparison of the Ikhana self-reported location vs. the post processing of the internal SAAP data from the Ikhana.The Ikhana output is shifted by approximately 3 seconds early (to the left).
Figure 22 .22Figure 22.Periods of missing tracks from the intruder data feed over a 5-minute time period.


Table 1 .1List of facilities and laboratories........................................................................................................ 8
Table 2 .2Aircraft Equipage............................................................................................................................. 13
Table 3 .3Configuration 1 SUT Summary....................................................................................................... 21
Table 4 .4tquery offset times between NASA Ames DSRL and NASA Armstrong LVC Labs..................... 26
Table 5 .5Data characteristics of the MACS virtual target update rate........................................................... 27
Table 6 .6Data characteristics of the MACS virtual target update rate........................................................... 28
Table 7 .7Data characteristics of the Ikhana ownship position report latencies.............................................. 29
Table 8 .8Data characteristics of the live intruder position report latencies.................................................... 31
Table 9 .9Data characteristics of the live intruder position report update rate...34............................................. 32 Table 10.Data characteristics of the T-34C ownship position report latencies............................................. 33 Table 11.Data characteristics of the live intruder position report update rate.............................................. 33 Table 12.Data characteristics of the live position report update rate............................................................
Table 1 .1List of facilities and laboratories.FacilityLocationDescriptionDistributed Simulation ResearchPrimary LVC laboratory used for both theLaboratory (DSRL)ARCdevelopment and conduct of flight test
Table 2 .2Aircraft Equipage.
Table 3 .3Configuration 1 SUT Summary.SUTResearcher/DeveloperDWCCAMitigatedUnmitigatedJADEMAmes Research CenterPrimarySecondaryYesYesStratway+ Langley Research Center PrimarySecondaryYesYesCPDSGA -ASI & TU DelftPrimarySecondaryYesNoRadarGA -ASIPrimarySecondaryNoYesTCASGA -ASISecondaryPrimaryYesNo
Table 5 .5Data characteristics of the MACS virtual target update rate.Number of data points900Sample time (minutes)15Average (sec)0.062 (62 ms)Std Dev (sec)0.019 (19 ms)Minimum Latency (sec)0.019 (19 ms)Maximum Latency (sec)0.132 (132 ms)
Table 6 .6Data characteristics of the MACS virtual target update rate.Number
of data points (34 minutes) 72432 Sample time (minutes) 30Average (sec)1.01Std Dev (sec)0.074Minimum Time between targets (sec)0.088Maximum Time between targets (sec)3.035% within 1 Std Dev97.18 %% outside 3 Std Dev0.51 %
Table 7 .7Data characteristics of the Ikhana ownship position report latencies.DateAverage (sec)Std Dev (sec)17-Jun-153.230.42618-Jun-152.560.09822-Jun-154.130.01026-Jun-152.670.5127-Jul-152.250.0379-Jul-152.210.01010-Jul-152.320.03121-Jul-152.140.06322-Jul-153.090.25524-Jul-153.170.062
Table 8 .8Data characteristics of the live intruder position report latencies.DateAverage (sec)Std Dev (sec)Data Fusion/Tracking17-Jun-151.950.426No18-Jun-151.550.373No22-Jun-152.170.323No26-Jun-151.690.338No7-Jul-150.380.022Yes9-Jul-151.670.495No10-Jul-150.380.023Yes4.1.3.1.2 Live Aircraft Update Rate4.1.3.1.2.1 Ikhana Ownship
Table 9 .9Data characteristics of the live intruder position report update rate.ADS-B withADS-BADS-B withoutADS-BTracking NowithTracking NowithoutDropsTrackingDropsTrackingNumber of Tracks120499116565Average Update (sec)1.0001.0261.0031.078Std Dev (sec)0.0000.2130.2260.311Min (sec)1.0001.0000.4000.624Max (sec)1.0004.0001.5993.376
Table 10 .10Data characteristics of the T-34C ownship position report latencies.DateAverage (sec)Std Dev (sec)Minimum Latency (sec)Maximum Latency (sec)10-Aug-150.177 sec0.222 sec-0.894 sec1.156 sec11-Aug-150.185 sec0.230 sec-0.740 sec0.721 sec12-Aug-150.147 sec0.263 sec-0.793 sec0.693 sec
Table 11 .11Data characteristics of the live intruder position report update rate.Average (sec)0.535Std Dev (sec)0.270Minimum (sec)-0.017Maximum (sec)1.2054.1.3.2.2 Live Aircraft Update Rate
Table 12 .12Data characteristics of the live position report update rate.Ownship (T-34C)IntruderAverage (sec)0.5231.032Std Dev (sec)0.2190.182Minimum (sec)0.0710.901Maximum (sec)1.1262.074
			Doc. No. FAA--2007--29305, 75 FR 30194, May 28, 2010; Amdt.91--314--A, 75 FR 37712, June 30, 2010; Amdt.91--316, 75 FR 37712, June 30, 2010
		
		
			
Appendix A -Controller and Pilot QuestionnairesFive questionnaire forms were formulated to capture user inputs during the FT3 testing.The subjects included ATC controllers, ghost controller, and UAV pilots, as it related to their physical working environment.The HSI team developed their own and more extensive questionnaire to capture pilot feedback on the interaction with the ground control station, and are not included in this report.
Date			
			

				


	
		UAS
		
			MMarston
		
		
			DSternberg
		
		
			SValkov
		
	
	
		NAS ITE Flight Test Series
		
			3
			Oct. 2015
		
	
	Flight Test Report
	Rev B
	Marston, M., Sternberg, D., Valkov, S., "UAS--NAS ITE Flight Test Series 3, Flight Test Report, Rev B", NASA Armstrong Flight Research Center, Oct. 2015



	
		Department of Defense Handbook. DoD-Produced CD-ROM Products.
		
			Department Of Defense  Washington Dc
		
		10.21236/ada308209
	
	
		Modeling and Simulation Master Plan
		
			Defense Technical Information Center
			Oct 1995
		
	
	DoD 5000.59P
	2 Department of Defense: "Modeling and Simulation Master Plan," DoD 5000.59P, Oct 1995



	
		
			AmyEHenninger
		
		
			DannieCutts
		
		
			MargaretLoper
		
		Live Virtual Constructive Architecture Roadmap (LVCAR) Final Report
		
			Sept, 2008
		
	
	Institute for Defense Analysis
	Henninger, Amy E., Cutts, Dannie, Loper, Margaret, et al, "Live Virtual Constructive Architecture Roadmap (LVCAR) Final Report", Institute for Defense Analysis, Sept, 2008



	
		Live, Virtual & Constructive Simulation for Real Time Rapid Prototyping, Experimentation and Testing using Network Centric Operations
		
			WilliamBezdek
		
		
			JoelMaleport
		
		
			RobertOlshan
		
		10.2514/6.2008-7090
	
	
		AIAA Modeling and Simulation Technologies Conference and Exhibit
		
			American Institute of Aeronautics and Astronautics
			2008. August 2008
			7090
		
	
	Bezdek, W. J., Maleport, J., Olshon, R., "Live, Virtual & Constructive Simulation for Real Time Rapid Prototyping, Experimentation and Testing using Network Centric Operations," AIAA 2008--7090, AIAA Modeling and Simulation Technologies Conference and Exhibit, August 2008



	
		Message Latency Characterization of a Distributed Live, Virtual, Constructive Simulation Environment
		
			JamesRMurphy
		
		
			SrbaJovic
		
		
			NeilOtto
		
		10.2514/6.2015-1647
	
	
		AIAA Infotech @ Aerospace
		
			American Institute of Aeronautics and Astronautics
			January 2015
		
	
	Murphy, J. Jovic, S., Otto, N., "Message Latency Characterization of a Distributed Live, Virtual, Constructive Simulation Environment," AIAA Infotech@Aerospace Conference, January 2015



	
		Development, Integration and Testing of a Stand-alone CDTI with Conflict Probing Support
		
			BrandonSuarez
		
		
			KevinKirk
		
		
			ErikTheunissen
		
		10.2514/6.2012-2487
		AIAA 2012--2487
	
	
		Infotech@Aerospace 2012
		
			American Institute of Aeronautics and Astronautics
			June 2012
		
	
	Suarez, B., Kirk, K., Theunissen, E., "Development, Integration and Testing od a Stand--Alone CDTI with Conflict Probing Support," AIAA 2012--2487, AIAA Infotech@Aerospace, June 2012



	
		Pilot Evaluation of a UAS Detect--and--Avoid System's Effectiveness in Remaining Well Clear
		
			CSantiago
		
		
			EMueller
		
	
	
		Eleventh UAS/Europe Air Traffic Management Research and Development Seminar (ATM2015)
		
			June 2015
		
	
	Santiago, C., Mueller, E., "Pilot Evaluation of a UAS Detect--and--Avoid System's Effectiveness in Remaining Well Clear," Eleventh UAS/Europe Air Traffic Management Research and Development Seminar (ATM2015), June 2015



	
		DAIDALUS: Detect and Avoid Alerting Logic for Unmanned Systems
		
			CesarMunoz
		
		
			AnthonyNarkawicz
		
		
			GeorgeHagen
		
		
			JasonUpchurch
		
		
			AaronDutle
		
		
			MariaConsiglio
		
		
			JamesChamberlain
		
		10.1109/dasc.2015.7311421
	
	
		2015 IEEE/AIAA 34th Digital Avionics Systems Conference (DASC)
		
			IEEE
			September 2015
		
	
	Munoz, C., Narkawicz, A., Hagen, G., Upchurch, J., Dutle, A., Consiglio, M., Chamberlain, J., "DAIDALUS: Detect and Avoid Alerting Logic for Unmanned Systems," 2015 IEEE/AIAA 34 th Digital Avionics Systems Conference (DASC), September 2015



	
		Integrated Test and Evaluation, Objectives & Requirements Document (ORD)
		
			Armstrong Flight ResearchNasa
		
		
			Center
		
		NASA IT&E ORD--01
	
	
		Flight Test
		
			4
		
	
	NASA Armstrong Flight Research Center, "Integrated Test and Evaluation, Objectives & Requirements Document (ORD), Flight Test 4 (FT--3), NASA IT&E ORD--01



	
		Walters, Very Rev. (Rhys) Derrick (Chamberlain), (10 March 1932–5 April 2000), Dean of Liverpool, 1983–99
		
			ARev
		
		10.1093/ww/9780199540884.013.u182588
		
	
	
		10 Software website
		
			Oxford University Press
			March 2015. April 2016
		
	
	Rev A," March 2015 10 Software website: http://www.pitch.se, April 2016



	
		MACS: A Simulation Platform for Today's and Tomorrow's Air Traffic Operations
		
			ThomasPrevot
		
		
			JoeyMercer
		
		10.2514/6.2007-6556
	
	
		AIAA Modeling and Simulation Technologies Conference and Exhibit
		Hilton Head, SC
		
			American Institute of Aeronautics and Astronautics
			August 20--23. 2007
			
		
	
	11 Prevot, T., Mercer, J., "MACS --A Simulation Platform for Today's and Tomorrow's Air Traffic Operations", AIAA MST Conference and Exhibit, August 20--23, 2007 Hilton Head, SC, AIAA--2007--6556



	
		A Multi-Fidelity Simulation Environment for Human-In-The-Loop Studies of Distributed Air Ground Traffic Management
		
			ThomasPrevot
		
		
			EverettPalmer
		
		
			NancySmith
		
		
			ToddCallantine
		
		10.2514/6.2002-4679
	
	
		AIAA Modeling and Simulation Technologies Conference and Exhibit
		Cambridge, MA
		
			American Institute of Aeronautics and Astronautics
			25 October 2002
			202
		
		
			Massachusetts Institute of Technology
		
	
	Prevot, T., "Exploring the Many Perspectives of Distributed Air Traffic Management: The Multi Aircraft Control System (MACS)," International Conference on Human--Computer Interaction Aeronautics, HCI-- Aero 202, 23--25 October 2002, Massachusetts Institute of Technology, Cambridge, MA



	
		An Overview of Current Capabilities and Research Activities in the Airspace Operations Laboratory at NASA Ames Research Center
		
			ThomasPrevot
		
		
			NancyMSmith
		
		
			EverettPalmer
		
		
			ToddJCallantine
		
		
			PaulULee
		
		
			JoeyMercer
		
		
			LynneMartin
		
		
			ConnieBrasil
		
		
			ChristopherCabrall
		
		10.2514/6.2014-2860
	
	
		14th AIAA Aviation Technology, Integration, and Operations Conference
		
			American Institute of Aeronautics and Astronautics
			2014--2860
		
	
	Prevot, T., Smith, N. M., Palmer, E., Callantine, T. J., Lee, P. U., Mercer, J., Martin, L., Brasil, C., and Cabrall, C. "An Overview of Current Capabilities and Research Activities in the Airspace Operations Laboratory at NASA Ames Research Center", 14th AIAA Aviation Technology, Integration, and Operations Conference, AIAA Aviation, (AIAA 2014--2860).



	
		Minimum Operational Performance Standards (MOPS) for Aircraft Surveillance Applications System (ASAS)
		
			Rtca
		
		
			Apr 14, 2009
		
	
	RTCA DO--317
	RTCA: "Minimum Operational Performance Standards (MOPS) for Aircraft Surveillance Applications System (ASAS)," RTCA DO--317, Apr 14, 2009



	
		Vigilant Spirit Control Station (VSCS): The Face of COUNTER
		
			GregoryFeitshans
		
		
			AllenRowe
		
		
			JasonDavis
		
		
			MichaelHolland
		
		
			LeeBerger
		
		10.2514/6.2008-6309
		AIAA 2008--6309
	
	
		AIAA Guidance, Navigation and Control Conference and Exhibit
		Honolulu, Hawaii
		
			American Institute of Aeronautics and Astronautics
			August 2008
		
	
	15 Feitshans, G. L., Rowe, A. J., Davis, J. E., Holland, M., and Berger, L., "Vigilant Spirit Control Station (VSCS)--"The Face of COUNTER"," Proceedings of the AIAA Guidance, Navigation and Control Conference and Exhibition, Honolulu, Hawaii, AIAA 2008--6309, August 2008.



	
		RUMS - Realtime Visualization and Evaluation of Live, Virtual, Constructive Simulation Data
		
			GeorgeSoler
		
		
			SrbaJovic
		
		
			JamesRMurphy
		
		10.2514/6.2015-1648
	
	
		AIAA Infotech @ Aerospace
		
			American Institute of Aeronautics and Astronautics
			Jan. 5--9, 2015
			
		
	
	Soler, G., Jovic, S., Murphy, J.R., "RUMS --Realtime Visualization and Evaluation of Live, Virtual, Constructive Simulation Data", AIAA SciTech, Jan. 5--9, 2015, AIAA--2015--1648


				
			
		
	
