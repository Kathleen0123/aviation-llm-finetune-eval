
	
	
		
IntroductionThe Center-TRACON Automation System (CTAS) is software being developed at NASA Ames Research Center, Moffett Field, CA, USA, in conjunction with the U.S. Federal Aviation Administration (FAA).CTAS provides decisionmaking assistance to air traffic controllers in the current terminal and en-route air traffic environments, through optimizing arrival traffic flow and generating advisories. 1 Several elements of the CTAS tool suite, the Traffic Management Advisor (TMA), the Descent Advisor (DA) and the Passive Final Approach Spacing Tool (pFAST), have all undergone thousands of hours of simulation testing and, in the past several years, have been the focus of extensive field evaluations in Dallas/Ft.Worth, Texas and Denver, Colorado.The focus of this paper is the human factors results from the field testing of the terminal area tool, pFAST.Information regarding the development and testing of TMA and DA can be found in Refs. 2 through 9.Passive FAST integrates runway assignment and sequence number advisories into the full datablock (FDB) of the TRACON arrival controllers' radar displays.The "passive" designation indicates the split of the full functionality of FAST; 10 pFAST provides runway and sequence number advisories.The "active" phase of FAST will additionally display heading, speed, and turn advisories. 11ctive FAST is currently under development at NASA Ames Research Center.The engineering specifications, methodology, and results of the pFAST field evaluation are reported in Refs.12, 13, and 14.Overall, an increase in throughput and runway balancing efficiency was shown, coupled with improvements in surface operations. 12owever, in addition to such operational benefits, a successful air traffic control decision-support tool must offer direct benefits t o the air traffic controllers and air traffic facility who will be using it. 15Therefore, an important part of the evaluation and assessment of CTAS is to understand the impact upon the controller.This emphasis contributes to the distinctively human-centered design of CTAS, 1 and is examined under the framework of human factors.The CTAS tool development process has successfully coupled engineering and human factors efforts.The emphasis in this report is on the coordination and communication data from the 1996 operational field assessment of pFAST.Refs.12, 16, and 17 describe the overall human factors assessment in greater detail; preliminary results have appeared in Refs.12 and 17.
MethodsPrior to the operational testing of pFAST in the DFW TRACON, several years of development and simulation were performed with a team of controllers who were well-trained on the use of pFAST. 10he pFAST Assessment Team, consisting of eight controllers and one area supervisor, provided input into software functionality and the refinement of the human factors questionnaires and data collection methods.The Assessment Team helped to ensure that the questionnaires that would be used were understandable and meaningful and that the proposed methods would not be intrusive to live operations.All of the human factors data in the operational assessment were collected from the Assessment Team, with the exception of two substitute controllers who participated when there was a staffing shortage.The substitute controllers were chosen by the Assessment Team and were briefed on the operation of pFAST prior to their participation in the test.The specific framework of the human factors operational assessment of pFAST was built upon previous human factors evaluations of TMA and DA, 17,18 as well as the assessment of the Computer Oriented Metering Planning and Advisory System (COMPAS), a decision-support tool developed by Deutsche Forschungsanstalt fŸr Luft und Raumfahrt (DLR) for German air traffic control.The general approach included developing an understanding of: (1) the existing operational environment, (2) the tasks for which the controllers, area supervisors, and traffic management coordinators (TMCs) are responsible, and (3) the constraints of conducting a test in an operational environment.This approach required significant interaction between the researchers and controllers, and allowed both groups to define the operational tasks and the testing objectives, while respecting the boundaries and needs of both parties during testing activities.In addition, these interactions contributed to refinement of data collection procedures and interpretation of results.The human factors evaluation examined the usability, suitability, and acceptance 19 of the pFAST advisories.Usability issues reflect the physical characteristics of the equipment and displays; in the case of pFAST, usability questions were posed about keyboard and slewball use and ability to detect the advisories themselves.Suitability issues reflect how pFAST is incorporated into the controller's tasks, and involved questions of overall workload as well as coordination and communication.Acceptance is influenced by both usability and suitability, as well as larger issues such as job satisfaction.The human factors operational evaluation data, described here, addresses acceptance, usability, and suitability issues based on controller coordination observations during the operational test periods and questionnaire responses.During the six months of operational testing of pFAST, controllers used pFAST advisories during 25 arrival rush periods across 7 different rush times.Baseline observations (when pFAST was not being used) were collected during 12 rush periods.Engineering data, such as airport throughput, in-trail separation on final approach, and adherence to the pFAST sequence and runway advisories were collected; these findings are described in Refs.12, 13, and 14.The engineering team was stationed in a room adjacent to the operational TRACON.In this separate area, the engineering data were collected and stored, and the overall system was monitored during operational use of pFAST.The human factors engineers conducted their data collection activities on the operational floor.The human factors team recorded observations and limited their interaction with the controllers, except to answer questions about pFAST.The Dallas/Ft.Worth (DFW) airport operates primarily in either North flow (with traffic arriving and departing towards the North) or South flow (traffic arriving and departing towards the South), with South flow the predominant airport configuration.Six of the 25 test rushes were in North flow.All the data presented in this paper reflect a 3-arrival runway configuration; the operational assessment took place prior to the addition of a fourth arrival runway at DFW airport, in October, 1996.The pFAST advisories, which consist of runway assignments and sequence numbers, are incorporated into the FDBs of the arrival aircraft on the existing Full Digital Automated Radar Terminal System (ARTS) Displays (FDADs), utilized by the TRACON arrival controllers.A few additional keyboard entries were required t o input runway changes and accept runway advisories when they differed from default runway assignments.No other physical manipulation of the equipment was required when using pFAST.The display of the pFAST advisories is described in Figure 1.The second line of the pFAST FDB shows timeshared information: in one mode, the default runway assignment and the aircraft type are shown and in a second mode, the aircraft's altitude and speed are shown.On the third line, the aircraft's sequence number to the pFASTallocated runway is shown, together with the pFAST runway advisory.A runway advisory is displayed on the third line of the FDB only if it differs from the default runway assignment (shown on the second line).For the example shown in figure 1, the pFAST runway advisory is to 17L, but the default runway assignment is 17C.Until the controller  acknowledges the pFAST runway advisory (either accepting it or rejecting it through a keyboard entry), the 17L advisory continues to be displayed in the 3rd line of the FDB.If pFAST's runway advisory did not differ from the default runway assignment, there would be no runway information in the 3rd line of the FDB.The sequence number displayed in the 3rd line is for the pFAST-advised runway.If the controller chooses not to direct the aircraft to the pFASTsuggested runway, another entry can be made t o indicate the controller's runway assignment, and the sequence number would update accordingly.
Questionnaire DataAfter each test rush, controllers were asked t o answer questionnaires regarding: overall workload (using a scale based on the NASA-TLX 20 ), the contributors to their workload, and acceptability (using the Controller Acceptance Rating Scale [CARS 10 ]).Approximately once per every three rushes, controllers were given an in-depth survey including questions regarding controlling strategy, perceived coordination, and perceptions about the Center feed.
Controller Observation DataDuring both baseline and pFAST test conditions, observations were recorded by two human factors engineers at two positions along the arrival wall: one between the two parallel finals and one on the busy side of the rush (typically this was the East side).Figure 2 illustrates the locations of the controller and observer positions.West side operations were located on the left of the arrival wall, and East side operations were located on the right.The two feeder positions (Feeder West or FW, and Feeder East or FE) were assisted by handoff positions (designated by ÒhÓ preceding the feeder name in Fig. 2).Feeder controllers are responsible for merging the multiple streams of traffic that arrive from the Center and vectoring these streams (which may be separated by altitude as well as arrival fix) into single streams towards the runways.In the DFW airspace configuration during the operational assessment, the FW controller was responsible for merging traffic arriving over both West arrival fixes, and the FE controller was responsible for merging traffic arriving over both East arrival fixes.Feeder controllers hand off traffic to the final controllers, who are responsible for controlling the traffic to their final approach courses.AR2 and AR1 were the final controllers who were each responsible for working one of the two parallel runways.Either the Meacham North (MN) or the Dallas South (DS) position was responsible for the diagonal runways, 13R and 31R, respectively.The MN and DS positions were not co-located along the arrival wall, and observations were not collected from these positions.Basic characteristics of each observed rush were noted by the human factors engineers, including: airport configuration, weather conditions, changes to staffing, and coordination between the area supervisor and the TMCs, and between the area supervisor/TMCs and the Tower and the Center.Specific attention was paid to coordination between the arrival controllers, and, where possible, coordination between arrival controllers and the Center.Coordination was defined as an instance of any verbal or non-verbal contact that was related to controlling traffic.The observations from the human factors engineers were merged into a single transcript per observed rush.Any observations that were incomplete, or unrelated to the traffic situation, were not included in the analysis.Codes were assigned to each observed coordination event.The codes fell into 8 general categories: Runway, Sequence, TRACON situation, Aircraft Status, Point-outs/Handoffs, Weather, Traffic Management Issues, Communication Issues, and Equipment Problems.Each of the 8 major categories were further subdivided into 2 to 6 subcategories.The 33 sub-categories are described in Ref. 16.
ResultsThe results are described according to the general framework of acceptance, usability, and suitability.More extensive discussion of the results can be found in Ref. 16.
AcceptanceThe user acceptance of the system was determined through direct ratings using the CARS.The CARS (see Refs. 10 and 17) was developed with the direct participation of the pFAST Assessment Team, who helped to define the various descriptors tied to the different rating levels, as well as the definitions behind the ratings of "adequate" versus "desired" performance.The overall acceptance rating across all the test rushes was 7.82 (SD=1.10), on a scale of 1 to 10, where 1 is the least desirable rating and 10 is the most desirable rating.The overall rating, rounded to 8, is associated with the following description of the system: "Mildly unpleasant deficiencies.System is acceptable and minimal compensation is needed to meet desired performance."Extensive comments were collected with the CARS ratings and reflected the controllers' concern about the accuracy of the sequence advisories.These comments related to overtakes and general disagreement with some sequences.However, the engineering data show very positive results for adherence to both runway and sequence advisories during the operational test. 13It is possible that the controllers felt that the advisories needed to be "perfect," thus, their ratings may reflect their tendency to characterize a less-than-perfect test rush as problematic.A controller's definition of "perfect" advisories was likely to match her/his view of the traffic situation; this does not account for pFAST's knowledge of traffic outside of the controller's perception.In contrast, a "perfect" rush in terms of the flow efficiency (measured by the pFAST researchers) was one in which delay was minimized.Some disagreement between controller preferences and pFAST calculations is inevitable, but is considered in the evaluation of acceptance.
UsabilityUsability was examined through questionnaires, with questions focusing on information presentation and equipment interaction.The controller ratings indicated that additional inputs required did not significantly increase workload.At best, the runway advisories were acceptable enough to require few corrections, or at worst, did not impact controller workload significantly when changes were required.When changes to the runway advisories were required, the greatest concern that the controllers voiced had to do with the associated update delay on their displays.The delay was not rated as excessive, and was reported to contribute minimally to their workload.Coordination and communication were also examined from a usability perspective, to see how pFAST affected the controller communication frequency.The controllers rated the amount of communication that they had with the aircraft under their control.On average, the controllers reported talking to each aircraft between 2 and 5 times.The reported average over all of the controllers was 3.8 times (SD = .80).None of the controllers reported having to talk to any aircraft more frequently due to the pFAST advisories.The controllers also rated the level of coordination required (with other controllers and facilities) when using pFAST.They reported that the required level of coordination did not exceed what they normally experienced.These results indicate that pFAST is not creating additional interactions with either other controllers or with the aircraft.
SuitabilitySuitability was assessed from questionnaires and observations.The workload impact and the effect on controller tasks as well as communication and coordination are described here.
WorkloadWorkload ratings were gathered to examine overall workload, as well as to examine specific elements of the controllers' tasks.The controllers did not report a dramatic increase or decrease in their overall workload when using pFAST.Controllers rated the runway and sequence advisories as contributing no more than "somewhat" to their overall workload.The amount of effort required to use the pFAST advisories was rated as about the same as the controllers were accustomed to working.They also reported that overall, pFAST had no effect on their ability to control traffic in their sectors.Given the demonstrated throughput benefits, this "non-effect" can be seen as a positive result, demonstrating that pFAST did not detract from operations.
Coordination and CommunicationCoordination (between controllers) helps ensure safe aircraft handling. 15The five most frequentlyobserved categories of coordination (regardless of test condition) were: pFAST/ ARTS-related issues, point-outs, handoff issues, runway assignments, and aircraft altitude changes.These categories are described in Table 1.While the controllers did not report any significant increase in controller-to-aircraft or controller-to-controller coordination, changes in coordination were observed between baseline and pFAST conditions.The means and standard deviations of the baseline data compared to the pFAST test data are shown in Fig. 3 with significantly greater frequency under the pFAST conditions.This is somewhat expected, as the new information provided to the controller, as well as the testing environment itself, would likely promote discussion about the advisories.Increased discussion regarding status checking was also found under pFAST conditions relative to baseline, but may be an artifact of the operational assessment itself.It is likely that the testing environment prompted the Assessment Team to increase their monitoring and awareness of operations in order to identify problems.The point-outs category was the only one which demonstrated a significant decrease under pFAST conditions relative to baseline.Point-outs are
Figure 3. Baseline vs. pFAST Coordination Comparisondefined as coordination with another position and utilizing another controllerÕs airspace, but retaining communication and control. 21Twice as many point-outs occurred under baseline conditions as occurred under pFAST conditions.Reducing the number of point-outs could allow controllers to spend more time separating and monitoring aircraft, rather than being concerned with coordination. 21It could also allow controllers to coordinate regarding other aspects of the traffic control process; perhaps more advance planning could be accomplished given more time to evaluate the traffic situation, therefore resulting in controllers using each others' airspace less than they would have to otherwise.Alternatively, point-outs could be reduced out of necessity as there was increased discussion regarding the advisories.However, if this were the case, the controllers should have indicated difficulties with coordination.In contrast, the controllers did not report any difficulties with the amount of coordination that they experienced, and did not report that the amount of coordination required was increased by the use of the pFAST advisories.The controllers reported that point-outs themselves did not contribute, on average, more than minimally to their overall workload.Tables 2 and3 present the mean frequency (and standard deviation) of instances of coordination per rush for the five most frequent categories of coordination in the baseline and test conditions.There were three categories of coordination whose frequencies were common to both baseline and test conditions: altitude changes, runway assignments, and handoffs.All three of these categories were discussed with greater frequency in the test condition than in the baseline condition.As shown in Tables 2 and3, the overall frequency of discussion under pFAST conditions is higher and is more evenly distributed among the top five categories.In the baseline condition, one category, point-outs, occurs twice as frequently as the other top categories of coordination.Categories common to baseline and test conditions.
Table 3. Most common categories of coordination under pFAST test conditionsas procedures exist to accomplish routine coordination.The increase in coordination across the major categories in the pFAST condition may indicate the controllers' need to confirm with one another the changes that they noted in their operations.As pFAST use increases, the general increase in coordination could diminish over time as the controllers become more accustomed to a new way of operating.Regardless, the additional coordination was not reported to negatively impact overall controller workload.
Lessons Learned
Constraints of Field TestingThe human factors results gathered from the pFAST operational assessment need to be interpreted in the light of the constraints inherent in field testing.Some of the constraints were not anticipated prior to the pFAST evaluation and as a result, the human factors data are "noisy."Consequently, care must be exercised in extrapolating the results to other decision-support tools or ATC environments.Some of the problems that were faced from testing in an operational ATC facility would have been eliminated had the testing been confined to more controllable settings.But through the development process of pFAST, it became clear t o both researchers and controllers that an operational evaluation was necessary as laboratory testing was no longer sufficient to evaluate all of pFAST's intended functionality.High-fidelity simulations, both at NASA Ames and the FAA Technical Center, were inadequate to approximate the operational setting.The experience from the pFAST field assessment is instructive for future testing of ATC decision-support tools and for identifying some of the problems faced in testing in an operational environment.During the planning of the operational assessment, the researchers determined that data collection in the field, especially over several months, would be subject to numerous restrictions.Engineering data collection was less problematic than the human factors data collection, as the engineering data were obtained in an automated fashion.In contrast, to collect the human factors data, observations from live operations were required.Controllers, tasked with completing their usual workload, were also asked to provide feedback about pFAST.The collection of the human factors data in the midst of actual operations meant that boundaries were set on data collection activities; all test personnel clearly understood that operational demands took priority over any type of evaluation activity.Weather, training requirements, or other facility demands on space or time could require that the human factors data collection activities be curtailed.In addition, more questionnaire data were originally proposed than what was eventually collected because of concerns that questionnaire results could have become affected by controller fatigue.Other challenges in the operational assessment affected both engineering and human factors data.There was no opportunity to exercise experimental control over air traffic conditions, such as the airport configurations and airport acceptance rates, which would have simplified the data analysis.The test periods themselves were limited by facility concerns about the traffic, or conditions of severe or unpredictable weather.There were also no guarantees about the staffing of the controller positions; while the majority of the rushes were staffed by the Assessment Team controllers, some substitute controllers participated when Assessment Team members were not available.While the Assessment Team carefully chose the substitute controllers, and the substitute controllers were fully briefed on pFAST, variations in controller perception are introduced into the data.The pFAST researchers established the requirement that controllers evaluating pFAST be as familiar as possible with the development process and the philosophy behind pFAST.Consequently, the number of actual participants in the pFAST evaluation was restricted relative t o the overall facility population.This meant that the facility at-large was not well-acquainted with pFAST.Situations arose in which the facility experienced problems with their own systems that were attributed to pFAST even when pFAST was not being used.This is a collateral problem with field testing that was encountered to some degree in other CTAS field evaluations.At a minimum, guarding against events that are unrelated to the test is disconcerting.At worst, however, unsubstantiated rumors about system performance can be damaging to the credibility of the decisionsupport tool.Data collection was also affected by lack of staffing.For example, the human factors team was not able to directly observe or measure the impact of pFAST on the Center, although there were some anecdotal reports that pFAST helped t o reduce holding in one area.Finally, the problems that arise in research in general, such as unexpected loss of data, and the unavailability of data, also occurred.Extending the testing period would have enabled the researchers to collect more data samples per configuration and rush period, but time and resource constraints prevented this from occurring.There are obviously tremendous benefits to field testing.In addition to the definite "engineering" advantages, including exercising pFAST under realtime, real-traffic loads, operational testing also provided valuable insight into human factors issues, such as training, procedural changes, job satisfaction, and coordination and communication effects.The effects on these areas are not easily captured or anticipated through simulations.Field exposure also provides significant benefits t o the researchers who are able to see how the software integrates with existing operations.Direct interaction with the end-users also makes researchers more aware of the users' specific requirements in the target environment.The dialogue between the researchers and the end-users is enhanced through this interaction, thus facilitating coordination required for ongoing development.By being exposed to the development process, the users are able to educate the rest of the facility on the functionality of the decision-support tools.Thus, misconceptions that inevitably arise when such decision-support tools are introduced can be better minimized.
RecommendationsThe pFAST assessment highlighted a number of procedures that should be required in the human factors data collection and analysis of ATC decision-support tools.Ways to improve the collection and analysis of human factors data in future operational assessments are also identified.(1) Extensive involvement by the target facility (or target user-group) is essential.The feedback from the Assessment Team helped to define the boundaries of the data collection, as well as t o clarify the goals of the evaluation.The Assessment Team input was also important for making the data collection materials (surveys and debriefing questions) understandable and usable.The facility involvement in this process should be expanded in order to inform a larger group of target users about the development process, and t o help mitigate misconceptions about the new decision-support tool.(2) Flexibility in data collection is needed; the researchers need to be able to accept some "noise" in the data, and plan for the eventuality of missing and lost data.In doing so, it may be necessary t o reduce a planned test matrix to try and capture more samples but with a limited scope.(3) The number of observers could be increased or rotated to different observation points to obtain a more complete picture of coordination, as long as this is balanced against disrupting operations.(4) The data should be evaluated part-way through the test to streamline the data collection.This would help to weed out questions in questionnaires that reveal few meaningful responses, and highlight other topics that need to be investigated more closely.Such an approach might help focus the data analysis, as well as reduce the fatigue involved with responding to long questionnaires.The human factors evaluation of pFAST emphasizes the need for a testing framework for human factors assessments, describing how data should be gathered and interpreted for multiple, interdependent users of ATC decision-support tools.Some of the findings from this operational assessment can contribute to such a testing framework.
Concluding RemarksThe human factors results are as important as the engineering results to the overall evaluation of pFAST.The engineering data show benefits of runway balancing and throughput.The human factors data describe the outcome of these benefits on the controllers themselves.Because of the heightened throughput and more efficient runway balancing during the pFAST operational evaluation, it would not have been surprising if controllers reported increased workload.Also, the nature of the information being provided (runway assignments and sequences to the runways) might have led to increased controller workload through increased coordination and communication.The human factors data instead bear out a different conclusion: despite the increased number of aircraft controlled during the field test, the controllers did not report any significant increase in overall workload.Furthermore, through direct ratings, the controllers indicated that using pFAST was acceptable in an operational setting.The controllers did not report having to increase their coordination activities between themselves, or between the TRACON and the Center due to the use of pFAST.They also did not report having t o talk to aircraft more frequently to achieve the pFAST-assigned runways or sequences.The operational assessment at DFW TRACON gave the pFAST researchers a unique opportunity to see their ATC decision-support tool tested under the rigorous traffic conditions of one of the busiest ATC facilities in the U.S. The lessons learned from the human factors assessment contribute to defining human factors testing guidelines for field evaluations.The success of pFAST, as demonstrated by the operational assessment, is due to the long history of controller involvement and input in the design and testing of pFAST.Controllers need t o understand new systems in order to effectively utilize and integrate them into their existing knowledge and experience. 15The development of pFAST, from concept definition through operational testing, employed a strategy of closely coupling the researchers and the controllers working within the boundaries of an existing, and highly specialized, setting.The human factors involvement in the development process contributed to identifying controller needs and determining if those needs were being met.This design approach resulted in the trust of the air traffic controllers and their willingness to test pFAST operationally.Without controller understanding and support of the system, benefits might never have been able to be identified.The attention that has been paid to the human factors issues has helped to define CTAS and ensure that it will meet controller needs.The human factors findings from the pFAST operational evaluation help to validate the processes which guided pFAST (and CTAS) development and demonstrate how benefits are achieved not only in terms of overall airport throughput and efficiency, but in terms what impact maybe experienced by the controller.The positive human factors findings increases the confidence in the operational deployment of pFAST by ensuring that key issues from the controllers' perspective have been examined.Figure 1 .1Figure 1.Current and pFAST flight datablocks.
Figure 2 .2Figure 2. Arrival Controller and Observer Positions.
Table 1 .1. Runway assignments, sequences, and spacing were discussed Five Most Frequent Coordination Categories.CategoryDescriptionpFAST/ARTS-• Keyboard entry proceduresrelated issuesrequired for pFAST-relatedinputs, as well as displayissues related to pFAST.• pFAST being turned on or off,or problems with the displayof pFAST information (due tothe ARTS interface).Point-outs• Aircraft requiring:-Special handling-Crossing through airspacethat was not normallyassigned to such aircraft-APREQ's (approval requests,especially from airportsinternal to the TRACON)• Utilizing another controllerÕsairspace, but retainingcommunication/control of theaircraft.• Often non-verbalHandoff Issues• Asking for handoffs• Frequency changes• OwnershipRunway• What the runway assignmentsAssignmentswere• Changes to runwayassignmentsAircraft Altitude• Expedited descentsChanges• Coordination based onaltitude• Inquiring about aircraftaltitudes
It is likely that under baseline conditions, the controllers coordinate only about what is unusual,25.00 25.00 CategoryMean (SD)Point Out10.90 (10.52)BaselineBaselineAltitude Changes4.90 (3.45)Handoffs4.20 (3.52)pFASTpFAST20.00 20.00 Heading Changes4.10 (2.85)Runway Assignment2.50 (2.59)Weather*2.50 (4.12)*The weather conditions were more uniform underpFAST testing than during baseline. 15.00 15.00 Categories common to baseline and test conditions.10.00 10.005.00 5.000.00 0.00Runway Assignment Runway Assignment1 Spacing Sequence SequencePoint-Out 1 SpacingStatus Point-Out CheckStatus Check
Table 2 .2Most common categories of coordination under baseline conditions.CategoryMean (SD)Runway Assignment8.58 (4.65)ARTS Problems7.50 (5.41)Handoffs7.21 (4.19)Sequence6.75 (4.09)Altitude Changes5.74 (4.45)
		
		
			

				


	
		Design of Center-TRACON Automation System
		
			HErzberger
		
		
			TJDavis
		
		
			SMGreen
		
	
	
		Proceedings of the 56 th Symposium on Machine Intelligence in Air Traffic Management
		the 56 th Symposium on Machine Intelligence in Air Traffic ManagementBerlin, Germany
		
			1993
			
		
	
	Erzberger, H., Davis, T.J., & Green, S.M. (1993). Design of Center-TRACON Automation System. In: Proceedings of the 56 th Symposium on Machine Intelligence in Air Traffic Management, Berlin, Germany, pp. 52-1-52-14.



	
		The Center-Tracon Automation System: Simulation and Field Testing
		
			DallasGDenery
		
		
			HeinzErzberger
		
		10.1007/978-3-642-60836-0_6
	
	
		Modelling and Simulation in Air Traffic Management
		Ames Research Center, Moffett Field, CA
		
			Springer Berlin Heidelberg
			1995
			
		
	
	Denery, D. G. & Erzberger, H. (1995). The Center-TRACON Automation System: Simulation and Field Testing. NASA Technical Memorandum 110366, Ames Research Center, Moffett Field, CA.



	
		Design of Automated System for Management of Arrival Traffic. NASA Technical Memorandum 102201
		
			HErzberger
		
		
			WNedell
		
		
			1989
			Ames Research Center, Moffett Field, CA
		
	
	Erzberger, H. & Nedell, W. (1989). Design of Automated System for Management of Arrival Traffic. NASA Technical Memorandum 102201, Ames Research Center, Moffett Field, CA.



	
		Corrosion fatigue crack propagation in metals. (Report)Gangloff, R.P. University of Virginia, NASA Contractor Report No NASA CR-4301 1990 204 pp
		
			KHarwood
		
		
			BSanford
		
		10.1016/0142-1123(91)90371-5
	
	
		International Journal of Fatigue
		International Journal of Fatigue
		0142-1123
		
			13
			4
			
			1993
			Elsevier BV
		
	
	Harwood, K. & Sanford, B. (1993). Denver TMA Assessment. NASA Contractor Report 4554.



	
		
		
			AmesResearch
		
		
			Center
		
		
			Moffett Field, CA
		
	
	Ames Research Center, Moffett Field, CA.



	
		The challenges of field testing the Traffic Management Advisor in an operational air traffic control facility
		
			TyHoang
		
		
			HarrySwenson
		
		
			TyHoang
		
		
			HarrySwenson
		
		10.2514/6.1997-3734
		AIAA 97-3734
	
	
		Guidance, Navigation, and Control Conference
		
			American Institute of Aeronautics and Astronautics
			1997
		
	
	Hoang, T. & Swenson, H.N. 1997. The Challenges of Field Testing the Traffic Management Advisor in an Operational Air Traffic Control Facility. AIAA 97-3734.



	
		Design and Operational Evaluation of the Traffic Management Advisor at the Fort Worth Air Route Traffic Control Center. Presented at the 1st USA
		
			HNSwenson
		
		
			THoang
		
		
			SEngelland
		
		
			DVincent
		
		
			TSanders
		
		
			BSanford
		
		
			KHeere
		
	
	
		Europe Air Traffic Management Research & Development Seminar
		
			1997. June 17-19, 1997
			Saclay, France
		
	
	Swenson, H.N., Hoang, T., Engelland, S., Vincent, D., Sanders, T., Sanford, B., Heere, K. (1997). Design and Operational Evaluation of the Traffic Management Advisor at the Fort Worth Air Route Traffic Control Center. Presented at the 1st USA/Europe Air Traffic Management Research & Development Seminar, Saclay, France, June 17-19, 1997.



	
		Field evaluation of Descent Advisor trajectory prediction accuracy
		
			StevenGreen
		
		
			RobertVivona
		
		10.2514/6.1996-3764
		AIAA 96-3764
	
	
		Guidance, Navigation, and Control Conference
		
			American Institute of Aeronautics and Astronautics
			1996
		
	
	Green, S.M. & Vivona, R.A. (1996). Field Evaluation of Descent Advisor Trajectory Prediction Accuracy. AIAA 96-3764.



	
		Descent Advisor preliminary field test
		
			StevenGreen
		
		
			RobertVivona
		
		
			BeverlySanford
		
		10.2514/6.1995-3368
		AIAA- 95-3368
	
	
		Guidance, Navigation, and Control Conference
		
			American Institute of Aeronautics and Astronautics
			1995
		
	
	Green, S.M., Vivona, R.A. & Sanford, B. (1995). Descent Advisor Preliminary Field Test. AIAA- 95-3368.



	
		
			BDSanford
		
		
			KKLee
		
		
			SEGreen
		
		Decision-aiding Automation for the Enroute Controller: A Human Factors Field Evaluation. Submitted to the Tenth International Symposium in Aviation Psychology
		Columbus, OH
		
			April 1999
		
	
	in progress
	Sanford, B.D., Lee, K.K., & Green, S.E. (in progress). Decision-aiding Automation for the En- route Controller: A Human Factors Field Evaluation. Submitted to the Tenth International Symposium in Aviation Psychology, April 1999, Columbus, OH.



	
		The development of the Final Approach Spacing Tool (FAST): a cooperative controller-engineer design approach
		
			KKLee
		
		
			TJDavis
		
		10.1016/0967-0661(96)00116-5
	
	
		Control Engineering Practice
		Control Engineering Practice
		0967-0661
		
			4
			8
			
			1996. August 1996
			Elsevier BV
		
	
	Lee, K.K. & Davis, T.J. (1996). The Development of the Final Approach Spacing Tool (FAST): A Cooperative Controller-Engineer Design Approach. Control Engineering Practice 4(8), August 1996.



	
		Design and evaluation of an air traffic control Final Approach Spacing Tool
		
			ThomasJDavis
		
		
			HeinzErzberger
		
		
			StevenMGreen
		
		
			WilliamNedell
		
		10.2514/3.20721
	
	
		Journal of Guidance, Control, and Dynamics
		Journal of Guidance, Control, and Dynamics
		0731-5090
		1533-3884
		
			14
			4
			
			1991
			American Institute of Aeronautics and Astronautics (AIAA)
		
	
	Davis, T.J., Erzberger, H., Green, S.M., & Nedell, W. (1991). Design and evaluation of an air traffic control final approach spacing tool. J. of Guidance, Control and Dynamics, 14, 848-854.



	
		Operational Test Results of the Passive Final Approach Spacing Tool
		
			TJDavis
		
		
			DRIsaacson
		
		
			JERobinson
		
		
			WDen Braven
		
		
			KKLee
		
		
			BSanford
		
		10.1016/s1474-6670(17)43820-1
	
	
		IFAC Proceedings Volumes
		IFAC Proceedings Volumes
		1474-6670
		
			30
			8
			
			1997. June 16-18, 1997
			Elsevier BV
			Chania, Greece
		
	
	Davis, T.J., Isaacson, D.R., Robinson, J.E. III, den Braven, W., Lee, K.K., & Sanford, B. (1997). Operational Test Results of the Final Approach Spacing Tool. In: Proceedings of the IFAC 8th Symposium on Transportation Systems '97, Chania, Greece, June 16-18, 1997.



	
		Knowledge-based runway assignment for arrival aircraft in the terminal area
		
			DouglasIsaacson
		
		
			ThomasDavis
		
		
			JohnRobinson, Iii
		
		
			DouglasIsaacson
		
		
			ThomasDavis
		
		
			JohnRobinson, Iii
		
		10.2514/6.1997-3543
		AIAA 97- 3543
	
	
		Guidance, Navigation, and Control Conference
		
			American Institute of Aeronautics and Astronautics
			1997
		
	
	Isaacson, D.R., Davis, T.J., & Robinson, J.E. III (1997). Knowledge-Based Runway Assignment for Arrival Aircraft in the Terminal Area. AIAA 97- 3543.



	
		Fuzzy Reasoning-Based Sequencing of Arrival Aircraft in the Terminal Area
		
			JERobinson
		
		
			Iii
		
		
			TJDavis
		
		
			DRIsaacson
		
		AIAA 97- 3542
		
			1997
		
	
	Robinson, J.E. III, Davis, T.J., & Isaacson, D.R. (1997). Fuzzy Reasoning-Based Sequencing of Arrival Aircraft in the Terminal Area. AIAA 97- 3542.



	
		Air Traffic Control
		
			VDHopkin
		
		10.1016/b978-0-08-057090-7.50025-4
	
	
		Human Factors in Aviation
		
			ELWiener
		
		
			DCNagel
		
		San Diego
		
			Elsevier
			1988
			
		
	
	Hopkin, V.D. (1988). Air Traffic Control. In: E.L. Wiener & D.C. Nagel (Eds.) Human Factors in Aviation. San Diego: Academic Press.



	
		Passive Final Approach Spacing Tool Human Factors Operational Assessment
		
			KKLee
		
		
			BDSanford
		
		10.2514/5.9781600866630.0585.0598
	
	
		Air Transportation Systems Engineering
		Ames Research Center, Moffett Field, CA
		
			American Institute of Aeronautics and Astronautics
			1998
			
		
	
	Lee, K.K. & Sanford, B.D. (1998). Human Factors Assessment: The Passive Final Approach Spacing Tool (pFAST) Operational Evaluation. NASA Technical Memorandum 208750, Ames Research Center, Moffett Field, CA.



	
		Developing ATC Automation in the Field: It Pays to Get Your Hands Dirty
		
			KellyHarwood
		
		
			BeverlyDSanford
		
		
			KatharineKLee
		
		10.2514/atcq.6.1.45
	
	
		Air Traffic Control Quarterly
		Air Traffic Control Quarterly
		1064-3818
		2472-5757
		
			6
			1
			
			1998
			American Institute of Aeronautics and Astronautics (AIAA)
		
	
	Harwood, K., Sanford, B.D., & Lee, K.K. (1998).



	
		Developing ATC Automation in the Field: It Pays to Get Your Hands Dirty
		
			KellyHarwood
		
		
			BeverlyDSanford
		
		
			KatharineKLee
		
		10.2514/atcq.6.1.45
	
	
		Air Traffic Control Quarterly
		Air Traffic Control Quarterly
		1064-3818
		2472-5757
		
			6
			1
			
			
			American Institute of Aeronautics and Astronautics (AIAA)
		
	
	Developing ATC Automation in the Field: It Pays to Get Your Hands Dirty. ATC Quarterly Journal, v. 6, no. 1, 45-70.



	
		Human Factors Certification of Advanced Aviation Technologies
		
			KHarwood
		
		
			BSanford
		
		J. Wise, V.D. Hopkin, & P. Stager
		
			1994
			Embry-Riddle Aeronautical University Press
			Daytona Beach, FL
		
	
	Evaluation in Context: ATC Automation in the Field
	Harwood, K. & Sanford, B. (1994). Evaluation in Context: ATC Automation in the Field. In: J. Wise, V.D. Hopkin, & P. Stager (Eds.), Human Factors Certification of Advanced Aviation Technologies. Daytona Beach, FL: Embry-Riddle Aeronautical University Press.



	
		Defining Human-Centered System Issues for Verifying and Validating Air Traffic Control Systems
		
			KellyHarwood
		
		10.1007/978-3-662-02933-6_6
	
	
		Verification and Validation of Complex Systems: Human Factors Issues
		
			JWise
		
		
			VDHopkin
		
		
			PStager
		
		Berlin
		
			Springer Berlin Heidelberg
			1993
			
		
	
	Verification and Validation of Complex and Integrated Human Machine Systems
	Harwood, K. (1993). Defining Human-Centered System Issues for Verifying and Validating Air Traffic Control Systems. In: J. Wise, V.D. Hopkin, & P. Stager (Eds.), Verification and Validation of Complex and Integrated Human Machine Systems. Berlin: Springer-Verlag.



	
		Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research
		
			SandraGHart
		
		
			LowellEStaveland
		
		10.1016/s0166-4115(08)62386-9
	
	
		Advances in Psychology
		
			PAHancock
		
		
			NMeshkati
		
		Amsterdam
		
			Elsevier
			1988
			
		
	
	Hart, S. G., & Staveland, L.E. (1988). Development of a NASA-TLX (Task Load Index): Results of empirical and theoretical research. In P.A. Hancock and N. Meshkati (Eds.), Human Mental Workload (pp. 139-183). Amsterdam: North-Holland.



	
		A Personal Word...
		
			MPrichard
		
		10.1080/10948009709389905
	
	
		Communication Booknotes
		Communication Booknotes
		0748-657X
		
			28
			6
			
			1997
			Informa UK Limited
		
	
	Personal communication
	Prichard, M. (1997). Personal communication.


				
			
		
	
