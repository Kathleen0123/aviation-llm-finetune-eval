
	
	
		
I. IntroductionHE National Aeronautics and Space Administration (NASA) is conducting a series of data collection flight tests designed to reduce barriers for integrating unmanned aircraft systems (UAS) into the National Airspace System (NAS). 1 The objectives of the flights include encounters between an unmanned and manned aircraft at different geometric vectors to test the performance of prototype Detect and Avoid (DAA) and collision avoidance algorithms, as well as their respective guidance to the pilot in the ground control station (GCS). 2 As with many major simulation and flight test activities, it is expected that a demonstration will be provided to stakeholders to observe a demonstration of the technologies.However, during a flight test, observation can be a challenge due to the need to ensure a sterile cockpit environment in the GCS for the flight crew in control of the aircraft, the logistics of bringing observers to the test range, and encounters of aircraft flying at 10,000 feet being not easily viewable.In order to foster the integration of the DAA technologies into the simulation and flight test environments, NASA has developed a Live, Virtual, Constructive (LVC) test environment that enables distribution of test data across participating facilities across NASA aeronautic research centers. 2,3NASA Ames Research Center has a 360-degree, "out-the-window" tower facility called FutureFlight Central (FFC), which enables researchers to develop and test advanced surface and terminal area concepts in a three-dimensional (3-D) virtual airport emulation. 4,5,6FutureFlight Central provides an immersive simulated visual environment with twelve projectors and extensive video streaming and display capabilities, ideal for technology demonstrations and visualization.By coupling the LVC data distribution infrastructure with the existing FutureFlight Central visualization technologies, NASA has built a real-time live aircraft flight visualization capability.This paper describes the modifications made to FutureFlight Central to leverage its visualization capabilities by enabling the display of live aircraft representations in real-time.It also describes the techniques employed to support the streaming of live video, as well as the inaugural demonstration of the capability during a live flight test of an unmanned aircraft flying encounters to simultaneously test and observe the results of advanced collision avoidance and detect and avoid algorithms.
II. BackgroundUnder the UAS Integration in the NAS (UAS-NAS) project, NASA is conducting research intended to reduce technical barriers related to the safety and operational challenges associated with enabling routine UAS access to the NAS. 7NASA engineers are using the existing FutureFlight Central capabilities to support the demonstration of encounters of live aircraft during flight tests designed to collect data to further Detect and Avoid research.
A. Detect and AvoidNASA is conducting research to provide the remote pilot of an unmanned aircraft with the tools and technologies to replace the existing requirement to see and avoid aircraft and other airspace hazards. 8The see and avoid replacement (referred to as Detect and Avoid) has two primary functions:• Ensure that the unmanned aircraft remains "well-clear" of other aircraft and hazards and,• Interoperate with existing collision avoidance technologies that provide alerting and guidance to all pilots in the event of a near mid-air collision. 9searchers in the UAS-NAS project are conducting research to support the development of the DAA Minimum Operating Performance Standards (MOPS) for RTCA ‡ ‡ Special Committee 228.Phase 1 of the DAA MOPS was completed in July 2017 and covered the transition of unmanned aircraft through Class E into Class A airspace.As Special Committee 228 moves into Phase 2, the DAA MOPS development is expected to include the impact of Terminal airspace and use of low size, weight and power sensors have on DAA alerting parameters. 10n order to address the interoperability of the collision avoidance technologies, the UAS-NAS project is also supporting RTCA Special Committee 147 with the integration of well-clear algorithms into Aircraft Collision Avoidance System, called ACAS X. 11 ACAS X is the NextGen collision avoidance solution, scheduled to replace the existing Traffic Alert and Collison Avoidance System (TCAS). 12The unmanned variant of ACAS X, known as ACAS Xu, is anticipated to provide an integrated well-clear functionality with the collision avoidance technologies providing a complete DAA solution.‡ ‡ RTCA was founded as the Radio Technical Commission for Aeronautics, but is now simply known as RTCA.
T B. UAS Flight TestsFor Phase 1 DAA MOPS, the UAS-NAS project conducted flights out of NASA Armstrong Flight Research Center during the spring of 2015 and 2016. 2 NASA's Predator-B unmanned flight asset (known as the "Ikhana") was equipped with Automatic Dependent Surveillance -Broadcast (ADS-B) and a prototype dual panel air-to-air radar detection system. 13During the flight, manned intruder aircraft were flown with the Ikhana at encounter geometries designed to trigger DAA alerting and advisories.An example of the flight test encounters can be seen in Figure 1.Position data from the intruder aircraft was captured by the sensors on-board the Ikhana, combined into a single representative report by an internal tracking module, and then sent to the ground for analysis by the DAA algorithm.The pilot display in the GCS received alerts or advisories generated by the DAA algorithm and showed those data to the pilot.Phase 2 DAA MOPS flight testing is being planned for execution in the Summer of 2018.These flights are planned to focus on the performance requirements for lower size, weight, and power sensors.
C. FutureFlight CentralFutureFlight Central is an air traffic control simulation facility located at NASA Ames Research Center.FFC provides researchers with a fully immersive 360-degree out-the-window airport tower environment used to conduct research and collect data pertaining to airport capacity. 4 The facility contains a virtual pilot room, test engineer room, and the operations room that emulates an air traffic control tower environment (See Figure 2a).The operations room is approximately 25ft in diameter, with 12 projection screens along the outer wall using Sony laser projectors providing a virtual out-thewindow scene (See Figure 2b).Typically, FFC is used by NASA researchers investigating airport surface and Terminal operations. 5,6As a 360-degree visual simulator, FFC can be configured to replicate any control or ramp tower.The components of the tower simulator include:• a 360-degree out-the-window display • simulated radio communications systems • the underlying LVC/ High Level Architecture for connecting distributed simulation clients • target generators, which supply airborne and surface traffic • an image generator that maps the target positions and draws the point of view against an airport and/or terrain databaseWith its video streaming and high-fidelity capabilities, FFC is also ideal for monitoring simulations, visualization of research requirements, and demonstrations.
III. Demonstration ConceptOne of the biggest challenges for a flight test demonstration, particularly for unmanned aircraft testing, is that the primary operations necessarily take place on the ground, where observing the actual aircraft is problematic.During flight testing, the traffic picture is presented to the pilot on in the GCS via a two-dimensional overhead view (see Figure 3).While this is sufficient for a pilot display, it does not provide adequate spatial context for observers of a flight demonstration.Ideally, observers should be provided with the vantage point of a pilot in the cockpit, or the perspective from a chase aircraft during an encounter.Placing observers on-board an aircraft under test is fraught with safety and logistical challenges, and even more-so when that aircraft is a UAS with no cockpit.To provide a safe and convenient demonstration without impacting the safe and sterile test environment, the question became whether or not the existing data streams and the high-resolution displays available in FFC could be used to visualize a flight test from a cockpit or chase aircraft point of view.This initial proof of concept test sought to determine whether the existing live data feeds had sufficient and consistent update rate to look realistic, whether the fidelity of the display would allow adequate visualization of the intruding aircraft, and whether all data could be distributed from its data source to the FFC.
IV. Data Feeds and TechnologiesIn order to convert FutureFlight Central from tower emulation to live flight encounter visualization, several underlying technologies had to be developed or enhanced to deliver the required data.
A. Data FeedsThree types of data were used for the visualization demonstration: 1.) Live aircraft position data from sensors 2.) Live voice communication audio 3.) Video data from the Ikhana nose/ball cameras and DAA displaysThe aircraft position data is available from two sources.The first is ADS-B data obtained directly from a ground receiver at NASA Armstrong.Data from ADS-B equipped aircraft is updated every second from an on-board global positioning system (GPS) unit and broadcast.Data from the GPS unit are received by a ground receiver and sent to a process that filters the aircraft based on distance from the center of the test range and sends out the nine closest aircraft.A second source of data for the visualization come from the sensors on board the Ikhana, which is sent to the ground via a SatCom data link.Aircraft (ownship) state reports come from the embedded inertial navigation system/GPS, while the intruder aircraft data come from the ADS-B unit on the Ikhana and the on-board air-to-air radar.Audio from the test conductor's frequency (which includes all participating test aircraft) is forwarded to FFC.This allows the observers to follow along with the test encounters as they progress.The streaming video from the Ikhana is supplied by a mounted nose camera and sensor ball that is typically installed on the aircraft.The sensor ball camera can be rotated 360 degrees and provides a medium to low-resolution view with additional flight specific information overlaid onto the display as shown in Figure 4.The video feed is sent to NASA Ames where the Video Ingest Distribution System (discussed in the next section) routes the video stream to any connected facility, including FFC.The two other video streams sent from NASA Armstrong provide views of the Conflict Prediction and Display System (CPDS) traffic and alert display, developed by General Atomics -Aeronautical System, Inc.Two variations of the CPDS display were available during the ACAS flight test, allowing researchers and observers to compare the original DAA alerting to the integrated DAA/ACAS Xu alerting.Figure 5 shows an example of the General Atomics DAA display.
B. TechnologiesSeveral technologies were either developed or modified to send and receive the data and produce the visualization.These technologies are described below.
Live Virtual Constructive InfrastructureIn support of collecting data to enable the DAA research, the UAS-NAS project developed a Live, Virtual, Constructive (LVC) infrastructure.§ § LVC infrastructures are used by the Department of Defense and aerospace communities on a daily basis to integrate live assets or high-fidelity flight and mission simulators with virtual constructs to provide very realistic training for their personnel.This is a cost-effective and safe approach to integrating multiple systems in a complex or otherwise dangerous operating environment. 14,15he LVC infrastructure facilitates the distribution of the data collected and processed by sensors and § § The term LVC is a broadly used name for classifying modeling and simulation.Generally live modeling and simulation involve real actors operating real systems.Virtual modeling and simulation involve real actors operating simulated systems.Constructive modeling and simulation involve simulated people operating simulated systems.algorithms on-board the aircraft to participating systems and facilities on the ground.Collectively this is known as the LVC distributed environment.16 The LVC-distributed environment, as used by the UAS-NAS project for flight tests, ties live assets and facilitates at NASA Armstrong with the air traffic control simulators and facilities (including FFC) at NASA Ames (see Figure 6).3 During simulations and flight activites, the LVC distributed environment has also been used to connect facilities and test assets from NASA Langely and NASA Glenn Research Centers.The LVC system evolved over several years and leveraged many technologies originally developed under the Virtual Airspace Simulation Technologies Project.17
Video Ingest Distribution SystemThe Video Ingest Distribution System (VIDS) was developed at NASA Ames as a web based real-time video streaming and capture solution based on open web standards running on commercial off the shelf (COTS) hardware.Similar to commercial video streaming capabilities used by the entertainment industry, VIDS is designed to provide a framework for distribution and archiving of multiple video sources in research settings, but without the use of proprietary and licensed software.Advantages VIDS has over commercial software solutions are that it can be hosted onsite, configured for a facility's specific screen resolution capability and system layout, and set for dynamic video distribution to those systems without having the need for direct keyboard or mouse access.Video stream bandwidth requirements are also easily managed by adjusting resolution and frame rates.In the past, this type of configuration would typically have been implemented using expensive hardware video solutions, and would have been limited to a single facility.VIDS consists of three main elements:1.) Video Source and Capture technologies 2.) Server and network infrastructures 3.) A cross platform web-based viewing applicationThe video source capture technology used for the ACAS Xu flight test was the Epiphan VGA/DVI Broadcaster, which provides a wide array of different capabilities and streaming options, which fit nicely into the VIDS solution concept. 18This Epiphan hardware solution has been highly successful at NASA Ames for other projects and was used by both Ames and Armstrong during previous UAS-NAS project flights.The ACAS Xu flight test was the first use of the VIDS servers in both labs, and proved to be a very flexible and reliable solution.
Voice CommunicationsDistribution of radio/voice communication between test sites is accomplished using Distributed Interactive Simulation (DIS) IEEE 1278.1A-1998v6 Standard Protocol to send multicast voice packets over the network.Packets are routed to specific receivers based on a simulated frequency that each station monitors (for example, a particular air traffic control airspace sector).NASA Armstrong connects a DIS compliant ASTi ACE-RIU Bridge to a channel bank that interfaces with its DICES III voice over IP (VOIP) system, which provides translation between the incompatible DIS and VOIP protocols.At Ames, the voice data packets are received by the local LVC system and routed to the FFC network.Figure 8 in Section V provides a diagram of the underlying network connectivity.
Reconfigurable Image GeneratorThe Reconfigurable Image Genrator, or RiG, is software developed and patented by NASA developers. 19The RiG provides real-time 3-D renderings of aircraft and surrounding terrain to support visualization in a virtual environement using COTS computers (i.e., Windows rack mounted computers each with a single NVIDIA Quadro M6000 graphics processor unit).The RiG software drives the simulated FFC out-the-window scene and is setup to support viewpoints from anywhere in the defined airspace.Dynamic entities defined in the system have an associated 3-D rendering that are used to display each visible object with the correct orientation in the field of view using incoming vehicle state data, typically received by the RiG at 250Hz (post extrapolation).Each airborne object is automatically scaled based on the perspective distance from the defined "eye-point", which is typically configured to be the center of the FFC operations room.Observers in the tower have a full 360 degree field-of-view that is produced by twelve (12) rearscreen projectors, driven by six COTS computers.Each graphics processor unit in the PC drives two rear-screen projectors.The RiG technologies have been used for several years for surface and Terminal air traffic simulations and have been shown to visualize the intended airspace with little additional latencies. 5,6he use of FFC and the RiG for these flight visualization purposes required a few enhancements.First, the eyepoint had to be changed from a normally static location inside a simulated air traffic control tower to a dynamic one where the observer can be tethered to any aircraft, and hence its location changes dynamically along with the motion of other 3-D entities.Second, the position of the aircraft had to be extrapolated from the incoming 1Hz data to correspond to 60 Hz dynamics within the RiG.The incoming 1-Hz data from Armstrong flight telemetry data is provided by the live inputs and must then be extrapolated to create smooth 60-Hz visual motion for each dynamic 3-D entity.Extrapolation is required to minimize perceived "jumps" in aircraft position.The extrapolation employs a dead-reckoning algorithm, in addition to a second-order low-pass filter.The second-order low-pass filter was attenuated to account for the update rate frequency to be used on the input data from the LVC Gateway and the variability of the live data, which was then extrapolated on by the dead-reckoning algorithm.Next, dynamic data "tags" were added for airborne objects so they could be easily identified (see Figure 7); the textual data tags follow the 3-D objects and show the entity call sign as well as relative height and distance from the current eye-point.And lastly, for improved realism, the specific aircraft types that were involved with the flight test (Ownship: Predator-B and Intruder: King-Air), were rendered as realistic 3-D models for the demonstration.A 3-D database of the Edwards Flight Test range was developed to represent the terrain area.The terrain database process employs Google Earth imagery for the underlying scene.Aircraft scale models of 1x, 2x, and 3x actual size were tested.To facilitate a better observation experience, models were scaled to 3x actual size due to the distances between aircraft at the closest point of approach (anticipated to be between 1 and 0.3 nautical miles).Figure 7 shows a graphic of the visualization of the Ikhana and King-Air with the terrain in the background.Notice that at 1.1 nautical miles, the King-Air is extremely difficult to observe (even at 3x scale).The tag with the leader-line provides a necessary visual cue.
V. Demonstration SystemThe 3-D visualization and live video streaming in FFC was tested during the ACAS Xu Flight Test 2 flights on several days beginning on 13 June through 1 August 2017.Figure 8 shows a high-level a diagram of the system used for the visualization of the ACAS Xu flight test encounters.The embedded inertial navigation system/GPS equipment reported the position of the Ikhana, while other aircraft in the immediate vicinity were captured by the on-board airto-air radar and ADS-B sensors.These data streams were sent to the ground control station and sent to the LVC network from the Ikhana on a one second update rate.Once on the LVC network, the position data were distributed to NASA Ames for monitoring.At NASA Ames, the position data were sent to both the research versions of the DAA algorithm and pilot traffic display running in the DSRL as well as the RiG in FFC via the existing High Level Architecture infrastructure.This allowed researchers to test advanced algorithm and display changes without impacting the system under test.At the same time, the position reports were made available to the RiG for 3-D rendering.The NASA Armstrong airspace and terrain were modeled at a medium fidelity sufficient for the 10,000ft flight level anticipated for the flight testing.In addition, the 3-D models of the participating aircraft were mapped to the known live aircraft callsigns to ensure proper aircraft visualization.Streaming video was sent from the Ikhana cameras to the LVC lab at NASA Armstrong.This video stream, as well as the video from the two CPDS displays, was forwarded to NASA Ames through a secure VPN tunnel using Epiphan video streaming hardware.The custom configured Epiphan hardware was placed inline between the computers originating the video and the monitor output for the desktop computers used at NASA Armstrong.This allowed for minimal operational system impact and low latency for the captured video streams sent out over the network.In addition, Epiphans were connected to the research pilot display running in the DSRL providing an additional video stream to FFC.All four video streams were sent to the primary VIDS Server in the DSRL at NASA Ames.These video streams were then automatically relayed to the secondary VIDS Server found in FFC.This allowed for dynamic routing of the video streams to any of the 12 screens found in FFC with no impact on the upstream systems.All of the video streams were recorded during the live flight-testing to allow for playback at later times.Figure 9 shows the streams of the Ikhana ball camera and pilot display as shown at FFC during the flight demonstration.The voice communication systems that were already in place at NASA Armstrong and NASA Ames enabled FFC observers to monitor the test conductor's mission frequency while visually observing the flight test from the chase position behind Ikhana.The screens in FFC were configured to show the Ikhana distance-separated "observer" viewpoint rendered in 3-D by the RiG on the eight forward projectors.The Ikhana camera video feed, CPDS displays, and research pilot display were displayed on the remaining screens.
VI. Conclusions and Future UseOriginally envisioned to provide a demonstration capability for the UAS-NAS project, the live flight 3-D visualization capability at FutureFlight Central has been shown to be a strong alternative to observing flight-testing activities at the project test range at NASA Armstrong.Not only does this remove the burden of the team conducting the flight test to also support visitors, it also provides a visualization capability for the researchers to support the development of test cards and scenarios prior to flight.In addition, it offers a convenient centralized location where all pertinent data can be observed.Because all data are now centrally located, the 3-D visualization along with the streaming video feeds can be routed via the LVC network to connected NASA Centers and test partners.The 3-D visualization was made possible by the work done by the RiG engineers to filter and smooth the live aircraft position updates.Prior to this enhancement, the jittery quality of the visualization as aircraft were updated at 1 Hz diminished the utility of the system.Based on the positive feedback from researchers and management during the ACAS Xu flight tests demonstrations, other projects are investigating how the 3-D visualization and aircraft point-of-view capabilities can be leveraged.Future uses include modeling the airspace in and around potential urban air mobility takeoff/landing locations, visualizing lead/following aircraft in the final approach of a landing stream, and using the virtual capabilities to plan and script live testing prior flight-testing.The joint FAA/NASA ACAS Xu flight testing occurred in the Fall of 2014 and Summer of 2017, at NASA Armstrong.As with the DAA flights, NASA's Ikhana was the UAS test aircraft and equipped with the TCAS, ADS-B and an air-to-air radar.However, the radar for the Fall 2014 test was a prototype single panel unit.During the first flight test, ACAS Xu software was run on board the aircraft, providing collision avoidance advisories, while the DAA algorithm was run on the ground.During the second ACAS Xu flight test in the Summer of 2017, ACAS Xu software incorporated the horizontal maneuver logic of the draft Phase 1 DAA MOPS, so both the collision avoidance and wellclear algorithms were run on-board the Ikhana on an integrated system.In both flight tests, the position reports for the Ikhana as well as the intruder aircraft were sent from the Ikhana to the GCS.While executing the second ACAS Xu flight test, the position of the Ikhana and intruder aircraft were sent from the test range at NASA Armstrong to the Distributed Simulation Research Laboratory (DSRL) at NASA Ames for realtime data visualization.
Figure 1 .1Figure 1.UAS Flight Test Encounter.Example flight test encounter flown during the DAA and ACAS flight tests with the Ikhana UAS aircraft.
Figure 3 .3Figure 3. UAS Pilot Traffic Display.Pilot traffic display with candidate DAA.alerting.
Figure 5 .5Figure 5. CPDS Traffic Display.Pilot Traffic display showing DAA alerting.
Figure 4 .4Figure 4. Ikhana Video Feed.Video from the Ikhana nose camera with embedded flight information.
Figure 6 .6Figure 6.LVC-Distributed Environment High-Level Concept.The LVC-DE promotes distribution of flight test activities.
Figure 7 . 3 -73Figure 7. 3-D Visualization.Fully rendered Ikhana and King-Air (at 3x scaling) with the NASA Armstrong terrain shown in the background.
Figure 8 .8Figure 8. LVC Distribution used for ACAS Xu Flight Test.High-level depiction of the distribution of data feeding the 3-D visualization capability at FutureFlight Central.
Figure 9 .9Figure 9. Streaming Video.The side-by-side streaming video of the Ikhana ball camera and the DAA traffic display as shown in FFC during the live ACAS flight.
		
		

			
AcknowledgmentsThe authors would like to thank the Bill Chung, who developed the second-order low-pass filter that made the 3-D visualization realizable.
			

			

				


	
		NextGen UAS Research, Development and Demonstration Roadmap: Appendix A
		
			March 2012
		
	
	Joint Planning and Development Office. Version 1.0
	Joint Planning and Development Office, "NextGen UAS Research, Development and Demonstration Roadmap: Appendix A", Version 1.0, March 2012.



	
		Flight Test Overview for the UAS Integration in the NAS Project
		
			JRMurphy
		
		
			PSHayes
		
		
			SKKim
		
		
			WBridges
		
		
			MMarston
		
	
	
		AIAA Paper 2016-1756
		
			January 2016
		
	
	Murphy, J. R., Hayes, P. S., Kim, S. K., Bridges, W., and Marston, M., "Flight Test Overview for the UAS Integration in the NAS Project," AIAA Paper 2016-1756, January 2016.



	
		Evolution of a Distributed Live, Virtual, Constructive Environment for Human in the Loop Unmanned Aircraft Testing
		
			JRMurphy
		
		
			NDOtto
		
	
	
		Royal Aeronautical Society Modelling & Simulation in Air Traffic Management Conference
		
			November 2017
		
	
	Murphy, J. R., Otto, N. D., "Evolution of a Distributed Live, Virtual, Constructive Environment for Human in the Loop Unmanned Aircraft Testing," Royal Aeronautical Society Modelling & Simulation in Air Traffic Management Conference, November 2017.



	
		FutureFlight Central: A Revolutionary Air Traffic Control Tower Simulation Facility
		
			NDorighi
		
		
			BSullivan
		
		AIAA Paper 2003-5598
		
			August 2003
		
	
	Dorighi, N., Sullivan, B., "FutureFlight Central: A Revolutionary Air Traffic Control Tower Simulation Facility", AIAA Paper 2003-5598, August 2003.



	
		Surface Management Systems Simulations in NASA's Future Flight Central
		
			SLockwood
		
		
			SAtkins
		
		
			NDorighi
		
		AIAA Paper 2002-4680
		
			August 2002
		
	
	Lockwood, S., Atkins, S., Dorighi, N., "Surface Management Systems Simulations in NASA's Future Flight Central", AIAA Paper 2002-4680, August 2002.



	
		Runway Incursion Studies in NASA's Future Flight Central
		
			SLockwood
		
		
			NDorighi
		
		
			BRabin
		
		
			MMadson
		
		AIAA Paper 2002-5811
		
			October 2002
		
	
	Lockwood, S., Dorighi, N., Rabin, B., and Madson, M., "Runway Incursion Studies in NASA's Future Flight Central", AIAA Paper 2002-5811, October 2002.



	
		NASA Technology Development Project Plan, Unmanned Aircraft Systems (UAS) Integration in the National Airspace System (NAS)
		
			January 2013
			31
		
		
			National Aeronautics and Space Administration
		
	
	National Aeronautics and Space Administration, NASA Technology Development Project Plan, Unmanned Aircraft Systems (UAS) Integration in the National Airspace System (NAS), 31 January 2013.



	
		
		§91.113
	
	
		Title
		
			14
			United States Code of Federal Regulations
		
	
	United States Code of Federal Regulations, Title 14, §91.113.



	
		Concept of Integration for UAS Operations in the NAS
		
			MConsiglio
		
		
			JChamberlain
		
		
			CMunoz
		
		
			KHoffler
		
		
			September 2012
		
	
	28 th International Congress of the Aeronautical Sciences
	Consiglio, M., Chamberlain, J., Munoz, C., Hoffler, K., "Concept of Integration for UAS Operations in the NAS", 28 th International Congress of the Aeronautical Sciences, September 2012.



	
		Terms of Reference RTCA Special Committee (SC) 228 Minimum Operating Performance Standards for Unmanned Aircraft Systems (Rev 3)
		RTCA Paper No. 239-16/PMC-1527
		
			September 2016
		
	
	"Terms of Reference RTCA Special Committee (SC) 228 Minimum Operating Performance Standards for Unmanned Aircraft Systems (Rev 3)", RTCA Paper No. 239-16/PMC-1527 September 2016.



	
		Terms of Reference RTCA Special Committee (SC) 147 Minimum Operating Performance Standards for Aircraft Collision Avoidance Systems Revision 14
		RTCA Paper No. 079-16/PMC-1478
		
			March 2016
		
	
	"Terms of Reference RTCA Special Committee (SC) 147 Minimum Operating Performance Standards for Aircraft Collision Avoidance Systems Revision 14", RTCA Paper No. 079-16/PMC-1478 March 2016.



	
		Robust Airborne Collision Avoidance through Dynamic Programming
		
			MJKochenderfer
		
		
			JPChryssanthacopoulos
		
		ATC-371
		
			2011
		
		
			Massachusetts Institute of Technology, Lincoln Laboratory
		
	
	Project Report
	Kochenderfer, M. J. and Chryssanthacopoulos, J. P., "Robust Airborne Collision Avoidance through Dynamic Programming", Massachusetts Institute of Technology, Lincoln Laboratory, 2011, Project Report ATC-371.



	
		
		Document No. FT3-FTR-01
	
	
		National Aeronautics and Space Administration, Integrated Test and Evaluation (IT&E) Flight Test
		
			3
			December 2015
		
	
	FT3) Flight Test Report Plan
	13 National Aeronautics and Space Administration, Integrated Test and Evaluation (IT&E) Flight Test 3 (FT3) Flight Test Report Plan, Document No. FT3-FTR-01, December 2015.



	
		
			AmyEHenninger
		
		
			DannieCutts
		
		
			MargaretLoper
		
		Live Virtual Constructive Architecture Roadmap (LVCAR) Final Report
		
			Sept, 2008
		
	
	Institute for Defense Analysis
	Henninger, Amy E., Cutts, Dannie, Loper, Margaret, et al, "Live Virtual Constructive Architecture Roadmap (LVCAR) Final Report", Institute for Defense Analysis, Sept, 2008.



	
		Live, Virtual & Constructive Simulation for Real Time Rapid Prototyping, Experimentation and Testing using Network Centric Operations
		
			WJBezdek
		
		
			JMaleport
		
		
			ROlshon
		
	
	
		AIAA 2008-7090, AIAA Modeling and Simulation Technologies Conference and Exhibit
		
			August 2008
		
	
	Bezdek, W. J., Maleport, J., Olshon, R., "Live, Virtual & Constructive Simulation for Real Time Rapid Prototyping, Experimentation and Testing using Network Centric Operations," AIAA 2008-7090, AIAA Modeling and Simulation Technologies Conference and Exhibit, August 2008.



	
		Message Latency Characterization of a Distributed Live, Virtual, Constructive Simulation Environment
		
			JMurphy
		
		
			SJovic
		
		
			NOtto
		
		AIAA Paper 2015-1647
		
			January 2015
		
	
	Murphy, J. Jovic, S., Otto, N., "Message Latency Characterization of a Distributed Live, Virtual, Constructive Simulation Environment," AIAA Paper 2015-1647, January 2015.



	
		Distributed System Architecture in the VAST-RT for Real-Time Airspace Simulation
		
			RDLehmer
		
		
			SJMalsom
		
		
			August 2004
		
	
	AIAA Paper 2004-5436
	Lehmer, R. D., Malsom, S. J., "Distributed System Architecture in the VAST-RT for Real-Time Airspace Simulation", AIAA Paper 2004-5436, August 2004.



	
		Patent for
		
			JLArchdeacon
		
		
			NHIwai
		
		
			KHKato
		
		
			BTSweet
		
	
	
		National Aeronautics and Space Administration
		Washington D. C., U. S.
		
			20 February 2017
			583
		
	
	Patent No: US 9
	Archdeacon, J. L., Iwai, N. H., Kato, K. H., Sweet, B. T., National Aeronautics and Space Administration, Washington D. C., U. S. Patent for "Reconfigurable Image Generator", Patent No: US 9,583,018 B1, 20 February 2017.


				
			
		
	
