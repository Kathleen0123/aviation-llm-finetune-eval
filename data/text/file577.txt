
	
	
		
INTRODUCTIONAir traffic control Towers suffer visibility impairments during inclement meteorological conditions that negatively impact airport arrival rates, airport departure rates, terminalarea acceptance rates at en-route coordination-fixes, traffic flow management constraints, and surface vehicle accidents, including runway incursions [1] [3].There is no validated method for fully mitigating the operational impact of impaired Tower controller visibility.This technology gap is challenging for both current traffic management operations and the Next Generation Air Traffic System (NextGen) requirements for equivalent operational capabilities in all meteorological and visibility conditions [14].Several independent research activities have proposed designs that use head-worn, head-tracked, see-through display sub-systems to enhance Tower controllers' situation awareness [15][16] [17] [18] [19].Most of these proposals were influenced by previous research in three-dimensional displays developed for display of traffic information to the pilot [7], air traffic simulation [3] [8], and flight deck avion-ics [1][13].This paper reports results of flight test evaluations of the registration performance of an engineering model for real world airfield static objects (e.g., hangars), and dynamic objects represented by the test aircraft.The two principal sources of registration error for aircraft operating in proximity to the airfield are identified, and a simple algorithm is proposed for reducing this error to ≤ 1° when tracked by Automatic Dependent Surveillance-Broadcast (ADS-B), and ≤ 2° for aircraft tracked by current-technology Airport Surveillance Radar.
BACKGROUNDPrevious studies [11] have identified large fuel savings and ancillary economic benefits if a system could be implemented that would insure stable rates of airport capacity in all visibility conditions.A system capable of maintaining the safety and efficiency of surface operations during adverse visibility would produce a variety of benefits.These studies also concluded that higher average airport arrival rates would enable more uniform and productive en route and traffic flow management (TFM) operations.The increased surface capacity reliability would improve metrics for taxitimes, departure queues, ground-delays, ground-holds, and cancellations.Their theoretical low-visibility Tower display was required to provide equivalent situation awareness to a clear 360° field of regard, encompassing the airport surface and all approaches and departure areas in the Class-D airspace.Although various analysts may have estimated the benefits of an effective low-visibility tower tool, they rarely specified how such tools would be designed and operated [12].One approach proposes using augmented reality technology to achieve the desired functionality [22].Augmented reality (AR) refers to the generation of 3D graphics or other media such that they are overlaid on and registered with surrounding objects in our environment.AR differs from virtual reality insofar as AR allows users to view the 'real' world along with superimposed or composited computer-generated displays [10].NASA, with FAA support, developed several AR Tower Technology (ARTT) engineering prototypes which employed a variety of head-tracking see-through head-worn display systems, for the purpose of exploring the potential of AR technology to enhance Air Traffic Control (ATC) Tower controllers' situation awareness and performance [19] [20].Field evaluations by Moffett Field ATC Tower controllers of the initial ARTT prototypes were reported in a NASA Technical Memorandum [21].The controllers opined that AR technology could be 'Very Useful' or 'Useful' in many ATC Tower (ATCT) tasks and duties, acquiring information regarding aircraft location, heading, data-block details, and surface vehicle location and air traffic and Tower situation awareness (Figure 1.).
Figure 2. Controller's responses to 'only if' questionstions indicative of their criticism of AR performance issues.AR technology maturity issues were addressed by questions with the form: "AR technology would be useful in performing the ATCT tasks only if:" "system-fusion is improved," "computer 'augmented' display is improved," "see-through visibility is improved," "certain features are developed," or "ATCT procedures are changed."(Figure 2.).The controllers generally criticized the specific performance, comfort, and vision-impairment issues exhibited by the rudimentary ARTT prototypes.They particularly disapproved of the observed registration errors between the ARTT graphical symbol representing the calculated apparent position of the aircraft, and the actual observed position of the aircraft (which were often ≥ 4°), and expressed a preference for reduction of these errors to ≤ 1° or 2° before they could have confidence in using such systems operationally.The controller responses were employed in subsequent refinement of the ARTT prototypes.In 2008, a flight test was conducted to measure the registration errors associated with aircraft (the most important dynamic objects to ATC) and airfield structures such as hangars and runways (the most relevant static objects).
EXPERIMENTAL DESIGNThe focus of this flight test was to use ARTT engineering prototypes to measure registration errors of real-world airfield static objects and dynamic objects [2][5][6][9] [22] represented by a test aircraft tracked by surveillance systems that include both current conventional Terminal radar and the prospective NextGen system, ADS-B.One test hypothesis proposed that most registration errors of dynamic aircraft objects were caused by two factors: 1) surveillance sensor (e.g., ADS-B) transport latency, and 2) registration error inherited from misregistration between the non-moving (static) environment and its graphical representation, such that compensation for these two error terms may reduce measurable unexplained dynamic error to ≤ 1°.
MATERIALS & METHODSThe NASA ARTT engineering models were composed of five (theoretically independent) sub-systems: a 3D visual database of the surrounding physical environment, traffic management information showing aircraft and surface vehicles, a head motion and orientation tracker to determine the controller's point of view, a real-time computer graphics system to render the synthesized scene combining physical environment with vehicular traffic, and an optical-seethrough display.These five subsystems are required to present a virtual world of digital information that is optically combined with the controller's view of the real world.The registration errors of static airport objects and dynamic aircraft were determined from direct comparison of the computer graphic indication in ARTT and the actual line-ofsight position of the vehicle.A high-performance digital camera collected test data by recording the optical view through the ARTT see-through display.Sufficient graphical and alphanumeric information was included in each captured digital photographic (JPEG) image to measure the offset between the predicted graphical representations and the actual optical images of vehicle and environment.The ARTT experimental apparatus was designed to facilitate the measurement of inaccuracies and errors.Machinevision error-correction techniques were eschewed during the data collection phase.We also chose to compare the projected CGI position of the aircraft and the actual optical image of the aircraft, as opposed to using video-see-through AR technologies in which the real-world objects are rendered by real-time digital video.The ARTT engineering models were designed, assembled, and tested at NASA Ames Research Center.The tests described below were conducted from the Moffett Field Fire Station observation tower, a site with a commanding view of the airfield.The ARTT systems used in this flight test consisted of three principal functional subsystems: Information Infrastructure, Real-time ARTT, and the Test and Evaluation environment.
Information InfrastructureThe ARTT information infrastructure is illustrated under the blue top rectangle in Figure 3. Real-time air traffic data.Real-time ADS-B data was acquired from a Kinetic Avionic Products (hobbyist) model SBS-1 receiver.The SBS-1 received ADS-B data directly from the transmitting aircraft, which was processed by Kinetics proprietary software, and then served to client applications.This ADS-B server infrastructure often exhibited transport latencies of > 2 seconds between ADS-B transmission time and ARTT application reception time.
Real-time FAA Northern California Terminal Radar Approach Control (TRACON) (NCT) air traffic information.This information provided data on aircraft positions, altitudes, velocity, vertical speed, heading; and flight plan information on aircraft ID, aircraft type, equipage, destination, origination, and waypoints.
Additional FAA air traffic real-time data sources, including Oakland (ZOA), Air Route Traffic Control Center (ARTCC), and Enhanced Traffic Management System (ETMS). LocalFAA operational real-time data were routed from California to the FAA Technical facilities in New Jersey, which then securely served the data to NASA Ames Research Center via Center TRACON Automation System (CTAS) infrastructure.Selected CTAS subsystems (e.g., Communication Manager) were used to ingest, record, and serve this data to ARTT applications.
Real-time ARTT SystemsThe Real-time ARTT subsytems are illustrated under the tan right-center rectangle in Figure 3.Monocular optical see-through display.We used a Liteye LE-750a 800×600 resolution optical see-through color OLED display with 70% transmissivity and a maximum luminance of 300 cd/m 2 (later replaced with a Liteye LE-500 with nominally identical specification when the LE-750a failed).A neutral density filter was attached to the display to mitigate the displayed image from washing out in the bright sunlit conditions of the observation tower.Tracker.An InterSense InertiaCube3 hybrid inertialmagnetometer 3DOF orientation tracker was rigidly mounted on the camera assembly to track it, offset from the camera to minimize electromagnetic interference.Because of budgetary limitations, this 3DOF orientation tracker was used instead of a full 6DOF position and orientation tracker, and 3DOF position was determined from site surveys, as described below.A theodolite was used to determine angular measurements of observable features.ARTT software.NASA ARTT prototype software, which includes a custom 3D model of Moffett Field airport structures, was run on an Alienware Area-51 m17x laptop computer.The ARTT software used a static 3DOF position computed from survey maps and received the dynamic 3DOF orientation input from the InterSense InertiaCube3, run through AuSim AST software.The video output from this laptop was used to drive the Liteye display.
Test and Evaluation EnvironmentThe ARTT information infrastructure is illustrated under the green bottom-left rectangle in Figure 3.ARTT Data Collection.The ARTT software generated data frames (nominally at 30Hz) that indicated the calculated apparent position of the aircraft (i.e., the virtual aircraft) and the projected graphical representation of the airfield terrain (e.g., runways and hangars).These data frames also contained alphanumeric information, including compass rose (indicating degrees of arc), a vertical scale (also in degrees of arc), a display indicating precise horizontal and vertical angular position of a computer-graphic crosshair, userinterface settings, and state information (e.g., point-of-view position, both latitude/longitude and FAA Center coordinates; and orientation-sensor Heading, Roll, and Pitch), and other experimentally relevant information.Each data frame also recorded the settings of the compensatory slew, match, and lock software that agnostically corrected static registration errors by enabling rapid x-y-z and zoom adjustments of the projected virtual terrain to visually match the observed actual terrain.When static registration errors were reduced by slew, match, and lock adjustments, the reduction in static registration errors proportionately reduced the apparent registration errors between the actual aircraft and the virtual position symbol.Each data image captured by the digital camera recorded data on each of these three terms of error: ADS-B latency is expressed as the difference between the ADS-B timestamp and the timestamp of the computergenerated ARTT graphical and alphanumeric data.Camera assembly.Point Grey GRAS-20S4C-C 2.0MP Color Grasshopper Firewire 1394b camera, rigidly mounted relative to the Liteye see-through display with a Lumicon LA3040 Universal Digicam adapter, supported by a tripod.Video recorder.The Firewire video output from the Point-Grey GRAS-20S4C-C camera was connected to and recorded on a Sony Vaio laptop computer.JPEG images were encoded at ~90% compression under the Radius codec and recorded at a user-selected rate of between 8-30 frames per second.Test Aircraft: The primary test vehicle was an OH-58C owned and operated by the US Army.This vehicle was equipped with three different position and state tracking systems.The first system was a standard Mode-C transponder.The second system was an Ashtech differential GPS (DGPS) receiver, with a mean positional error of <1 m.This receiver recorded positional information at 5 Hz via an onboard laptop computer.The DGPS data was used to establish positional 'truth' in comparison to the ADS-B and TRACON data.The on-board ADS-B transmitter was a custom pallet containing a Garmin GDL 90, a remote-mounted product that contains a GPS/WAAS receiver and a Universal Access Transceiver (UAT).The GDL 90 transmits ownship data via the UAT data link.It receives data from other UATequipped aircraft, as well as FIS-B weather.Army test pilots repeatedly executed a series of defined test segments to create the test conditions required to satisfy the test matrix.The test points and number of runs performed in the baseline flight test conformed to typical maneuvers performed by helicopter in the terminal area, specifically: Hovers at three altitudes (<100 ft, ~1500 ft, and >1500 ft & < 2500 ft), Approach, Departure, Taxi, Maintained speed runs (10 kts, 40 kts, and 80 kts), Minimum Power Acceleration, Maximum Power Acceleration, and Gradual Deceleration.
RESULTSAnecdotally, one of the most common experiences during these tests was often losing visual track of the helicopter (a small aircraft with an effectively stealthy visual aspect), and yet locating it via the ARTT apparatus.There were often times when the visual image of the aircraft was not readily detectable, and the ARTT CGI virtual aircraft provided useful indication of the low-visibility aircraft's location.Dynamic registration (e.g., panning camera) of both static objects (e.g., runways) and dynamic objects (aircraft) often exhibited > 2° error.The dynamic registration (i.e., changing point of view) performance issues were largely (though not entirely) attributable to the orientation sensor and compass tracking performance issues.In most cases, it was necessary to use the ARTT user interface to slew and scale the graphical virtual environment to achieve accurate static registration (fixed, non-moving POV) of static objects (e.g., hangars) before each set of trials, and often it was necessary to make such adjustments during the session.The preliminary results indicate that static (fixed POV) registration of the virtual aircraft symbol and the image of the actual aircraft may achieve an angular error of ≤ 1° if: (1) the static registration of static objects is first set to ≤ 1°, and(2) ADS-B processing latency is < 0.3 second.Such events, however, were rare.In most cases, ADS-B latency was > 1 second, and horizontal registration (both static and dynamic) of static objects was ≥ 2°.It was found that in all three Hover conditions (<100 ft, ~1500 ft, and >1500 ft & <2500 ft), the apparatus (after the gross adjustments described above) could demonstrate a maintained (>10 second) tracking error of < 2° between the CGI indicator and the apparent optical image of the helicopter.In these hover tests, the dynamic object effectively imitates a static object.It was demonstrated in Approach and Departure trials, that static registration (e.g., non-moving camera) of dynamic objects (e.g., aircraft) with tracking errors of < 2° could be achieved using both ADS-B and TRACON ASR-9 radars for surveillance.The noticeable difference between the two surveillance technologies in these cases is that ASR-9 tracking deteriorated when the aircraft was < 2 nmi from the observation point, and the farther the aircraft from the observer, the greater the agreement between the ADS-B and ASR-9 indicators.This discrepancy between the two surveillance systems is largely attributable to the higher update rate of ADS-B (1 second) in comparison to the slower update rate (4.8 second) of ASR-9.The other tests (Taxi, Maintained Speed, Minimum Power Acceleration Maximum Power Acceleration, and Gradual Deceleration) demonstrated that although certain frames in each test sequence could achieve static (non-moving POV) registration of aircraft (dynamic objects) with ≤ 2° error across multiple frames, there were also more incidents of frames in each sequence where the optical and CGI aircraft position error were > 2°.These errors tended to become more pronounced when the aircraft was < 0.5 nmi from the observer position.In addition to the Army's OH-58C, other aircraft equipped with ADS-B were also observed for less formalized tests, including a B-767-200.The behavior of the ARTT prototype with these fixed-wing aircraft was similar to that with the test helicopter.ADS-B transport latency is defined (for the purposes of this study) as the time difference between the transmission of the ADS-B signal by the aircraft and the rendering of the ADS-B data by ARTT into a CGI error.This particular source of error might be mitigated by more sophisticated ADS-B receivers (which were beyond the budget for this experiment); however, one beneficial upshot of the occasionally excessive ADS-B processing latencies was the ability to simulate similar issues that may be encountered when real-world aircraft surveillance systems perform anomalously.Latency was measured as the difference between the JPEG data image ADS-B timestamp and the frame generation timestamp, as illustrated in Figure 5.The other major source of dynamic object registration error was inherited static object misregistration (from all causes).This type of error may be attributable to congruence inaccuracies in the ARTT prototype's visual database of the airfield, difficulty of accurately determining the exact position of the viewer, and other potential error sources.The inertial tracking and compass sensor often contributed the greatest amount of registration error, due to its intermittent latencies, environmental sensitivities, and inherent inaccuracies.These sensor-involved errors (unsurprisingly) were often more pronounced when the apparatus was dynamic (e.g., camera panning) than when it remained fixed on the tripod with a static orientation.In some cases, the orientation tracker errors were counterintuitive, overcompensating for apparent motion in the opposite direction from the panning motion.Data collected from a trial where the aircraft flew at low altitude above the runway at a sustained speed of 80 kts, followed by a deceleration, is illustrated in Figure 5.The edges of the ARTT-rendered CGI virtual graphic hangars extend 2.8° to the left of the actual real-world hangers, documenting -2.8° horizontal misregistration of static (nonmoving) objects in that particular data image.Since the entire virtual environment is shifted left by 2.8°, the ARTT real-time CGI virtual aircraft symbol (concentric squares surrounding a white dot) is also shifted by -2.8°.Adding 2.8° (in this case) to the virtual aircraft's horizontal position effectively compensates for misregistration artifacts inherited from the registration error associated with static objects.The best method of eliminating misregistration of static objects would require a system for tracking orientation without artifact or error.One method (among many) for improving registration of static objects is to measure the misregistration in each combined image and use that information to calculate an improved solution [4].The effect of a simple corrective algorithm that compensates for these two sources of error is illustrated in Figure 6, which depicts data from a trial where the test aircraft flew at low altitude above the runway at a sustained speed of 80 kts, followed by a deceleration.This data portrays a sequence of events during which the ARTT ADS-B subsystem exhibited excessive transport latency (> 6 seconds in the worst case).The horizontal registration error of static objects varied from highly accurate registration to > 6° mismatch.This test data illustrates how observed misregistration of the dynamic object (i.e., virtual vs. actual aircraft position) is largely attributable to the sum of surveillance latency and static object registration error.One Moffett Field Tower controller "rule of thumb" is that high-speed taxiing aircraft on the more distant runway (~ 0.4 nmi distant) will subtend ~ 2° horizontal translation (approximately the width of a thumb held at arm's length) per second.This heuristic provides a simple (though obviously improvable) method of estimating the horizontal angular error engendered by ADS-B latency for the purposes of the following discussion.The efficacy of subtracting these ADS-B latency and static registration horizontal excursions from the unfiltered virtual aircraft position is illustrated in Figure 5.In this case (as noted above), the edge of the misregistered virtual hangar extends 2.8° from the corresponding edge of the actual hangar, and the ADS-B latency contributes a -3.7° error (in the opposite direction).Since the sum of these two errors is -0.9°, the post-processed virtual aircraft symbol is reset 0.9° to the right of the ARTT-generated virtual aircraft.The resultant position shift reduces the observed dynamic registration error from 2.6° to 1.7°.This filter algorithm may be more formally stated as:   Error is reduced to ≤ 1° in cases where surveillance transport latency is < 2 seconds (subtending a ~ 4° error).Since 2 seconds latency is within the expected NextGen System Wide Information Management ADS-B performance, sub-degree ARTT aircraft registration under these conditions accuracy should be achievable.In cases where surveillance transport latency is < 5 seconds (subtending 10° horizontal error), the horizontal registration error is reduced to ≤ 2°.Since the ASR-9 update rate is 4.8 seconds, horizontal accuracies of ≤ 2° should therefore be achievable with conventional Terminal radar systems.The simple filter proposed in this paper has two principal flaws.The term for the apparent horizontal angular speed rate of the aircraft was heuristically set to a constant 2° per second, upon recommendation of a Tower controller who knew from experience that this constant works well with aircraft accelerating or decelerating down a runway with an average speed of 80 kts.More sophisticated aircraft speed and routing models should yield better results, though that rich discussion is beyond the scope of the current paper.
CONCLUSIONSThe flight tests described in this paper explore the potential of using AR technology as an ATCT tool that could enable controllers in diminished-visibility conditions to effectively visualize airport static object models and dynamic aircraft objects represented by air traffic surveillance data.The principal motivation for these tests was to separate speculation from actual observation of the sources and potential mitigation of registration of dynamic objects representing aircraft operating in proximity of an airfield.The results demonstrate that the most significant error of this kind may be attributable to either static object registration error and/or surveillance transport latency.For the purposes of illustration, a simple filter demonstrates how filtering these two terms may reduce aircraft registration error to < 2° with currently deployed Terminal radar systems and to < 1° with the expected NextGen deployment of ADS-B.Figure 1 .1Figure 1.Information Acquisition Question: 'How useful would ARTT technology be in acquiring the following information?'
Figure 3 .3Figure 3. Block diagram of the principal functional components of the ARTT systems described in this paper.
Figure 4 .•4Figure 4. Instrumentation.(left) See-through display, camera assembly, and orientation tracker, compared with (right) previous ARTT engineering model worn by a Moffett Field Tower controller.ARTT Screen Calibration: Computations made using imagery of Moffett Field obtained from Google Earth (dated June 30, 2007), and consistent with theodolite measurements made inside the Moffett Field Fire Station observation tower, indicate that the distance from the observation tower to the NW corner of Hangars 2 and 3 is approximately 781 m and that the interior of the observation tower measures approximately 5.5 m along its N-S axis.Thus, if tracking an aircraft at the NW corner of Hangars 2 and 3, and assuming the position error of the ARTT infrastructure itself is limited to 5.5 m perpendicular to a target at 781 m, this corresponds to a worst case angular error of arctan(5.5/781)= ~0.4°yaw.(Note that the RMS accuracy of the InterSense InertiaCube3 is specified by the vendor as 1° yaw and that the 1 m mean positional error of the test vehicle corresponds to ~.07° at that distance.)The Liteye 800×600 resolution LE-750a and LE-500 displays are each claimed to have a nominal ~22.4° horizontal field of view (28° diagonal field of view).However, measurements of the displays indicated a horizontal field of view of between ~26°-27°, corresponding to 0.033°-0.034°subtended horizontally per pixel (~30.8-29.6 pixels per degree).The ARTT screen was calibrated using these measurements.Additional sources of error within the test environment included: • Inaccuracies in the ARTT Moffett Field 3D airport model • Effects of static and dynamic environmental metal and electromagnetic signals in the observation tower on the In-terSense InertiaCube3 • Errors in the ATC system surveillance data • Delays between measuring position on the vehicle • Delays communicating position data to ARTT • Processing data for display
Figure 5 .5Figure 5. ARTT data image, illustrating: (a) apparent registration error of dynamic object (test helicopter); (b) sensor transport latency, defined as the difference between the ADS-B timestamp and the ARTT CGI timestamp; (c) registration of static objects (e.g., hangars); and (d) registration error reduction by subtracting the sum of static and latency horizontal errors from the virtual aircraft horizontal value.
Figure 6 .6Figure 6.Registration error of dynamic object is a variable largely dependent on the summed registration errors generated by (a) effects of ADS-B (aircraft surveillance) latency and (b) static object registration errors (all causes).When the ADS-B broadcasts new updates, latency error is sharply reduced.User adjustments to correct static-object registration error similarly improves dynamic-object registration.
PUnfilterd -((V Terrain -A Terrain ) -(R*(S Timestamp -G Timestamp ))), where P Filtered = Filtered Virtual Aircraft Symbol horizontal position (compass degrees of arc), P Unfiltered = Unfiltered Virtual Aircraft Symbol horizontal position (compass degrees of arc), V Terrain = Virtual Terrain = Edge of Virtual Terrain object X value (compass degrees of arc), A Terrain = Actual Terrain = Edge of Actual Terrain object X value (compass degrees of arc), R = Aircraft apparent horizontal angular speed rate (degrees of arc per second); in this case, heuristically set to a constant 2 (see text), S Timestamp = Surveillance update timestamp (in this case from ADS-B), and G Timestamp = ARTT graphical frame-generation timestamp.
Figure 7 .7Figure 7. illustrates how this simple filter scheme reduces horizontal registration errors for dynamic objects (aircraft) .
Figure 7 .7Figure 7. Observed dynamic registration accuracy may be improved by subtracting static registration and sensor-latency errors from unfiltered virtual aircraft symbol position.
		
		

			
ACKNOWLEDGEMENTSThis work was supported by the FAA ATO Ops Planning Systems Engineering Division (particularly Diana Liang, Richard Jehlen, and Steven Bradford).Columbia University's participation was supported under National Aeronautics and Space Administration Grant Agreement No. NNX08AE26A issued through the Ames Research Center.We thank the test pilots and staff of the US Army Aeroflight Dynamics Directorate, the Moffett Field Air Traffic Control Tower, the NASA Ames Fire Department, the NASA Aeronautics Validation and Verification Laboratory (particularly Matt Ma, Joe Cisek, and Jinn Hwei Chen), the Human Systems Integration Division (particularly Steven Ellis and Bernard Adelstein), and the FAA William J. Hughes Technical Center (particularly Brian Ujvary, and the late, great Phil Zinno) for their exemplary and invaluable contributions to this flight project.
			

			

				


	
		Synthetic Vision Enhanced Surface Operations With Head-Worn Display for Commercial Aircraft
		
			Jarvis(trey) JArthur
		
		
			LawrenceJPrinzel
		
		
			KevinJShelton
		
		
			LyndaJKramer
		
		
			StevenPWilliams
		
		
			RandallEBailey
		
		
			RobertMNorman
		
		10.1080/10508410902766507
		
	
	
		The International Journal of Aviation Psychology
		The International Journal of Aviation Psychology
		1050-8414
		1532-7108
		
			19
			2
			
			2009
			Informa UK Limited
		
	
	Arthur III, J. T. J., Prinzel III, L. J., Shelton, K. J., Kra- mer, L. J., Williams, S. P., Bailey, R. E., & Norman, R. M., Synthetic Vision Enhanced Surface Operations With Head-Worn Display for Commercial Aircraft. The Inter- national Journal Of Aviation Psychology, 19, 2, (2009), 158-181. Available at http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/2009 0017752.pdf



	
		Improving static and dynamic registration in an optical see-through HMD
		
			RonaldAzuma
		
		
			GaryBishop
		
		10.1145/192161.192199
	
	
		Proceedings of the 21st annual conference on Computer graphics and interactive techniques - SIGGRAPH '94
		the 21st annual conference on Computer graphics and interactive techniques - SIGGRAPH '94
		
			ACM Press
			1994
			
		
	
	Azuma, R., and Bishop, G. Improving static and dynam- ic registration in an optical see-through HMD. Proceed- ings of the 21st annual conference on Computer graphics and interactive techniques, ACM, (1994), 197-204.



	
		Advanced human-computer interfaces for air traffic management and simulation
		
			RonaldAzuma
		
		
			MikeDaily
		
		
			JimmyKrozel
		
		10.2514/6.1996-3548
	
	
		Flight Simulation Technologies Conference
		Washington, DC
		
			American Institute of Aeronautics and Astronautics
			1996
			
		
	
	Azuma, R., Daily, M., and Krozel, J., Advanced Human- Computer Interfaces for Air Traffic Management and Simulation, Proceedings of 1996 AIAA Flight Simulation Technologies Conference, AIAA, Washington, DC, (1996), 656-666.



	
		Dynamic registration correction in video-based augmented reality systems
		
			MBajura
		
		
			UNeumann
		
		10.1109/38.403828
	
	
		IEEE Computer Graphics and Applications
		IEEE Comput. Grap. Appl.
		0272-1716
		
			15
			5
			
			1995
			Institute of Electrical and Electronics Engineers (IEEE)
		
	
	Bajura, M., and Neumann, U. Dynamic registration cor- rection in video-based augmented reality systems. IEEE Computer Graphics and Applications, 15, 5, (1995), 52- 60.



	
		Visual Features Used by Airport Tower Controllers: Some Implications for the Design of Remote or Virtual Towers
		
			StephenREllis
		
		
			DorionBListon
		
		10.1007/978-3-319-28719-5_2
	
	
		Virtual and Remote Control Tower
		Ames Research Center, Moffett Field, CA
		
			Springer International Publishing
			2011
			
		
	
	NASA TM-2011-216427
	Ellis, S.R., and Liston, D.B., "Static and motion-based visual features used by airport tower controllers," NASA TM-2011-216427, Ames Research Center, Moffett Field, CA, (2011).



	
		Frame Rate Effects on Visual Discrimination of Landing Aircraft Deceleration: Implications for Virtual Tower Design and Speed Perception
		
			SREllis
		
		
			NFurstenau
		
		
			MMittendorf
		
		10.1177/1071181311551015
	
	
		Proceedings of the Human Factors and Ergonomics Society Annual Meeting
		Proceedings of the Human Factors and Ergonomics Society Annual Meeting
		1541-9312
		
			55
			1
			
			September, 2011
			SAGE Publications
			Las Vegas, Nevada
		
	
	Ellis, S.R., Furstenau, N., and Mittendorf, M., Frame Rate Effects on Visual Discrimination of Landing Air- craft Deceleration: Implications for Virtual Tower De- sign and Speed Perception, Proceedings of Human Fac- tors and Ergonomics Society Annual Meeting, Las Ve- gas, Nevada, (September, 2011), 71-75.



	
		Perspective Traffic Display Format and Airline Pilot Traffic Avoidance
		
			StephenREllis
		
		
			MichaelWMcgreevy
		
		
			RobertJHitchcock
		
		10.1177/001872088702900401
	
	
		Human Factors: The Journal of the Human Factors and Ergonomics Society
		Hum Factors
		0018-7208
		1547-8181
		
			29
			4
			
			1987
			SAGE Publications
			Santa Monica, CA
		
	
	HFES
	Ellis, S.R., McGreevy, M.W., and Hitchcock, R.J., Per- spective traffic display format and airline pilot traffic avoidance, Human Factors, 29, HFES, Santa Monica, CA, (1987), 371-382.



	
		New NASA rocket programme
		
			SREllis
		
		
			JRSchmidt-Ott
		
		
			JKrozel
		
		
			RJReisman
		
		
			JGips
		
		10.1108/aeat.2002.12774aab.002
		NASA TM 2002-211853
		
	
	
		Aircraft Engineering and Aerospace Technology
		0002-2667
		
			74
			1
			2002
			Emerald
		
	
	Ellis, S.R., Schmidt-Ott, J. R., Krozel, J., Reisman, R.J., and Gips, J., Augmented reality in a simulated tower en- vironment: effect of field of view on aircraft detection, NASA TM 2002-211853, (2002). Available at http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/2003 0005687.pdf



	
		Towards determination of visual requirements for augmented reality displays and virtual environments for the airport tower
		
			SREllis
		
		HFM-121/RTG 042 HFM-136
		
	
	
		Proceedings of the NATO workshop on Virtual Media for the Military
		the NATO workshop on Virtual Media for the MilitaryWest Point, N.Y.
		
			2006
			
		
	
	Ellis, S.R., Towards determination of visual require- ments for augmented reality displays and virtual envi- ronments for the airport tower. Proceedings of the NATO workshop on Virtual Media for the Military, West Point, N.Y. HFM-121/RTG 042 HFM-136, (2006), 24-1 -24-9. Available at http://www.dtic.mil/dtic/tr/fulltext/u2/a473306.pdf



	
		Augmented Reality: A New Way of Seeing
		
			StevenKFeiner
		
		10.1038/scientificamerican0402-48
		
	
	
		Scientific American
		Sci Am
		0036-8733
		
			286
			4
			
			April 2002
			Springer Science and Business Media LLC
		
	
	Feiner, S. Augmented reality: A new way of seeing. Scientific American, 286, 4, (April 2002), 34-41. Avail- able at http://203.68.243.199/saweb/pdf.file/en/e004/e004p036. pdf



	
		Taxiway Navigation and Situation Awareness (T-NASA) System: Problem, Design Philosophy, and Description of an Integrated Display Suite for Low-Visibility Airport Surface Operations
		
			DavidCFoyle
		
		
			AnthonyDAndre
		
		
			RobertSMccann
		
		
			ElizabethMWenzel
		
		
			DurandRBegault
		
		
			VernolBattiste
		
		10.4271/965551
	
	
		SAE Technical Paper Series
		
			SAE International
			1996
			
		
	
	SAE
	Foyle, D.C., Andre, A.D., McCann, R.S., Wenzel, E., Begault, D., and Battiste, V., Taxiway Navigation and Situation Awareness (T-NASA) System: Problem, de- sign philosophy and description of an integrated display suite for low-visibility airport surface operations, SAE Transactions: Journal of Aerospace, No. 105, SAE, (1996), 1411-1418.



	
		Remote Tower Experimental System with Augmented Vision Videopanorama
		
			NorbertFürstenau
		
		
			MarkusSchmidt
		
		10.1007/978-3-319-28719-5_8
		
	
	
		Virtual and Remote Control Tower
		
			Springer International Publishing
			September, 2008
			
		
	
	Fürstenau, N., Schmidt, M., Rudolph, M., Möhlenbrink, C., & Halle, W., Augmented vision videopanorama sys- tem for remote airport tower operation, Proc. ICAS. (September, 2008). Available at http://www.icas- proceedings.net/ICAS2008/PAPERS/093.PDF



	
		<title>Flight testing an integrated synthetic vision system</title>
		
			LyndaJKramer
		
		
			JarvisJArthur Iii
		
		
			RandallEBailey
		
		
			LawrenceJPrinzel Iii
		
		10.1117/12.601757
		
	
	
		SPIE Proceedings
		
			SPIE
			May, 2005
			
		
	
	Kramer, L. J., Arthur III, J. J., Bailey, R. E., & Prinzel III, L. J., Flight testing an integrated synthetic vision sys- tem. In Defense and Security, International Society for Optics and Photonics, (May, 2005), 1-12. Available at http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/2005 0160185.pdf



	
		Enhanced and synthetic vision for terminal maneuvering area NextGen operations
		
			LyndaJKramer
		
		
			RandallEBailey
		
		
			KyleK EEllis
		
		
			RMichaelNorman
		
		
			StevenPWilliams
		
		
			JarvisJArthur Iii
		
		
			KevinJShelton
		
		
			LawrenceJPrinzel Iii
		
		10.1117/12.885902
		
	
	
		SPIE Proceedings
		
			SPIE
			June, 2011
			
		
	
	Kramer, L. J., Bailey, R. E., Ellis, K. K., Norman, R. M., Williams, S. P., Arthur III, J. J., Shelton, K.J., Prinzel III, L. J., Enhanced and synthetic vision for terminal ma- neuvering area nextgen operations. SPIE Defense, Secu- rity, and Sensing, International Society for Optics and Photonics, (June, 2011), 80420T-80420T. Available at http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/2011 0011181.pdf



	
		Augmented reality system for the ATC Tower
		
			JKrozel
		
		
			LBirtcil
		
		
			KTMueller
		
		
			RAzuma
		
		NASA TR 98183-01
		
			1999
		
	
	Krozel. J., Birtcil, L., Mueller, K.T., and Azuma, R., Augmented reality system for the ATC Tower, NASA TR 98183-01, 1999.



	
		Reinventing the familiar
		
			WendyEMackay
		
		
			Anne-LaureFayard
		
		
			LaurentFrobert
		
		
			LionelMédini
		
		10.1145/274644.274719
		
	
	
		Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '98
		the SIGCHI conference on Human factors in computing systems - CHI '98
		
			ACM Press
			1998
			
		
	
	Mackay, W. E., Fayard, A. L., Frobert, L., & Médini, L., Reinventing the familiar: exploring an augmented reality design space for air traffic control. Proceedings of the SIGCHI conference on Human factors in computing sys- tems, ACM Press/Addison-Wesley Publishing Co., (1998), 558-565. Available at http://stl.cs.queensu.ca/~graham/cisc836/lectures/reading s/chi98-mackay.pdf



	
		An augmented reality pilot display for airport operations under low and zero visibility conditions
		
			HermanRediess
		
		
			HermanRediess
		
		10.2514/6.1997-3680
	
	
		Guidance, Navigation, and Control Conference
		Washington, DC
		
			American Institute of Aeronautics and Astronautics
			1997
			2
			
		
	
	Redeiss, H.A., "An augmented reality pilot display for airport operations under low and zero visibility condi- tions," AIAA Guidance, Navigation, and Control Con- ference, A97-37001 10-63, Vol. 2, AIAA, Washington, DC, (1997), pp. 912-929.



	
		Augmented reality for air traffic control towers
		
			RonaldReisman
		
		
			StephenEllis
		
		10.1145/965400.965426
	
	
		ACM SIGGRAPH 2003 Sketches & Applications
		
			ACM
			2003
		
	
	Reisman, R J., and Ellis, S.R., Augmented reality for air traffic control, SIGGRAPH 2003 Sketches & Applica- tions: In conjunction with the 30th Annual Conference on Computer Graphics and Interactive Techniques, (2003).



	
		Design of Augmented Reality Tools for Air Traffic Control Towers
		
			RonaldReisman
		
		
			DavidBrown
		
		10.2514/6.2006-7713
		
	
	
		6th AIAA Aviation Technology, Integration and Operations Conference (ATIO)
		Wichita, KS
		
			American Institute of Aeronautics and Astronautics
			2006
		
	
	Reisman, R. J., & Brown, D. M., Design of augmented reality tools for air traffic control towers. In 6th AIAA Aviation Technology, Integration and Operations (ATIO) Conference, Wichita, KS, (2006). Available at http://www.aviationsystemsdivision.arc.nasa.gov/publica tions/more/other/reisman_09_06.pdf



	
		Air traffic control tower augmented reality field study
		
			RonaldJReisman
		
		
			StephenREllis
		
		10.1145/1186954.1187014
	
	
		ACM SIGGRAPH 2005 Posters on - SIGGRAPH '05
		
			ACM Press
			2005
			52
		
	
	Reisman, R. J., & Ellis, S. R., Air traffic control tower augmented reality field study. In ACM SIGGRAPH 2005 Posters, ACM, (2005), 52.



	
		Augmented reality tower technology flight test
		
			RonaldJReisman
		
		
			StevenKFeiner
		
		
			DavidMBrown
		
		10.1145/2669592.2669651
		
	
	
		Proceedings of the International Conference on Human-Computer Interaction in Aerospace
		the International Conference on Human-Computer Interaction in Aerospace
		
			ACM
			2010
		
	
	Reisman, R.J., and Brown, D.M., Augmented Reality Tower Technology Assessment, NASA TM-2010- 216011, (2010). Available at http://www.aviationsystemsdivision.arc.nasa.gov/publica tions/2010/Reisman_TM-2010-216011_FINAL.pdf



	
		Usability Considerations for a Tower Controller Near-Eye Augmented Reality Display
		
			JohnWRuffner
		
		
			JimEFulbrook
		
		10.1177/154193120705100214
	
	
		Proceedings of the Human Factors and Ergonomics Society Annual Meeting
		Proceedings of the Human Factors and Ergonomics Society Annual Meeting
		2169-5067
		1071-1813
		
			51
			2
			
			2007
			SAGE Publications
		
	
	Ruffner, J.W., and Fulbrook, J.E., Usability Considera- tions for a Tower Controller Near-Eye Augmented Re- ality Display. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 51, 2, (2007), 117- 121.


				
			
		
	
