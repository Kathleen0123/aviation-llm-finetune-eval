
	
	
		
INTRODUCTIONTechnological advances are changing the way that advanced air traffic control automation should be developed and assessed.Current standards and practices of system development place field testing at the end of the development process, during system production and deployment.This delays our understanding of the true characteristics of the system; that is, its emergent properties as a consequence of implementation in the operational environment, how the tool is used and the myriad interactions and interdependencies between system components.While such practices may have been suitable for manual control systems, where requirements for hardware and personnel can be neatly separated and defined, they are becoming quickly outdated for systems that are harnessing advanced information technology.Current understanding of such systems is limited and criteria for safe and effective system performance are largely undefined.If field testing is delayed until the late stages of development, solutions to design problems run the risk of being technology driven with development relying on decontextualized guidelines of human-computer interaction.Field testing conducted early in system development affords investigation of the users' experience with the system in the context of their work domain.It provides the opportunity to understand the implications for system design of the interdependencies between the physical environment (lighting; workplace layout), task domain (goals/functions of the domain) and work activities (social aspects of team coordination; sources of motivation and job satisfaction).The richness and complexity of these context-based factors and the relationships between them are not accessible through design guidelines or standards.Guidelines and standards cannot provide insight into effective design solutions when system performance is highly contingent on context [1,2].System validation through early field testing promotes the development and validation of a tool as a problem solving instrument [3], thereby increasing the likelihood of a match between the system's capabilities and its context of use [4,5].The FAA TATCA Program recognizes the importance of context through early field testing for the development of advanced ATC automation.It is using rapid prototyping and early field exposure as part of the Center-TRACON Automation System (CTAS) development process, using on-site system evaluations with active controllers and representative traffic flows and conditions.Field testing is regarded as integral to the development process and iterates until a match is achieved between the system and context for its use.A primary objective of the TATCA Program is the evolutionary refinement of CTAS for the purpose of bringing system functionality to a level of usefulness and stability.This approach deviates from traditional approaches to ATC system development and will expedite a possible national deployment of CTAS.Embracing the context of the ATC domain is particularly important because of our limited knowledge of controller/team job performance and the stringent requirements for maintaining ATC system continuity and safety during system transition [6].This paper describes the development and assessment process that has been applied for CTAS at Denver Air Route Traffic Control Center (ARTCC) and Terminal Radar Approach Control (TRACON).CTAS is described first, followed by an overview of the development and assessment process.Descriptions are also provided of progressive assessments, how TMA is used in the field, and how scenarios, shadow exercises and structured interviews are used during assessments.The methods used for interpreting and analyzing data, as well as drawing inferences and identifying implications, are also discussed.Finally, the training process is described.
CTASCTAS is an integrated set of three automation tools: the Traffic Management Advisor (TMA), the Descent Advisor (DA) and the Final Approach Spacing Tool (FAST).Together, they provide decision-making assistance to center and terminal controllers by presenting planning functions and clearance advisories.TMA is the first CTAS automation tool to go through the field development and testing process, and will be the focus of discussion in this paper.TMA sequences and schedules arrival traffic to minimize delays and optimize the use of the available runways.The traffic manager can override TMA's automatically generated schedule at any time by resequencing aircraft, inserting slots for additional aircraft or changing the required separation at the runway for arrival aircraft.TMA represents the traffic flow on configurable moving timelines.Aircraft data tags move down the timelines.Color coding is used to convey information about scheduling status.A traffic load display provides a graphical representation of various traffic load characteristics.Additionally, several configuration panels are available for modifying timeline displays and setting scheduling parameters.TMA operates on a SUN4 SPARc workstation.The interface is presented on a color monitor, and keyboard and mouse input devices are used for data entry.TMA has been developed for use by the traffic manager at Traffic Management Units (TMUs) within ARTCCs and TRACON facilities.Unlike controllers, traffic managers do not control traffic directly.Instead they monitor the demand of arrival traffic into the center and terminal areas, coordinating with Terminal, Center and Tower personnel to make decisions about balancing the flow of traffic so that demand does not exceed capacity in the Center and Terminal areas.Traffic managers use information about the arrival flow when making the various traffic management decisions: 1) metering, that is, spacing of the arrival flow to avoid exceeding the airport capacity; 2) distributing the load from one area to another; 3) assigning departure times for aircraft departing airports within the Center's airspace.Currently, in the field, information about the traffic situation is accessed from several sources, such as a plan view display, aircraft situation displays, weather displays, operational personnel and flight strips.Often, there is no steady state in the traffic flow; the location of one "heavy" aircraft can substantially modify the scheduled flow of traffic, as can poor weather, equipment outages and emergencies.Given the extent of coordination required, the variety of sources of information accessed, and the dynamic and often variable state of the traffic flow, context through early field testing is crucial to ensure the robustness of TMA and its effective integration into the TMU.For this reason, progressive assessments have been integrated into the CTAS development process.
OVERVIEW OF THE DEVELOPMENT AND ASSESSMENT PROCESSThe CTAS system is currently being developed in partnership with the FAA at NASA Ames Research Center.Two laboratories are involved: the Advanced Automation Laboratory and the Verification and Validation Laboratory.Research and development occur in the Advanced Automation Laboratory, and integration testing is performed in the Verification and Validation Laboratory.Proposed modifications to the software are prioritized by a Change Control Board (CCB), a group composed of software developers, human factors engineers, software testing personnel and program management.Modifications are incorporated into the software in the Verification and Validation Laboratory, and the software is released to the field sites.At the field site, traffic managers evaluate TMA on the operational floor.Traffic managers submit their comments and suggestions as field action requests (FARs) through an electronic mail system or by fax to the ATC Field Systems Office at NASA Ames Research Center.These FARs are also reviewed by the CCB.
Progressive AssessmentsTMA assessments occur in a progressive fashion, beginning with computer human interface (CHI) assessments, followed by assessments of usability and, finally, suitability.CHI assessments are conducted in the Verification and Validation Laboratory at NASA-Ames Research Center prior to releasing the software to the field.The purpose of these assessments is to verify that the interface conforms to established human factors guidelines and principles.Any discrepancies between the guidelines and the interface are documented by a human factors engineer, and proposed resolutions are reviewed and prioritized by the CCB.The software modifications are made in the Verification & Validation Laboratory, and the software is released to the field sites.Usability exercises verify that traffic managers are not impeded by the technology from accessing the data they need for making traffic management decisions.This phase is conducted at the field site and builds upon the CHI assessments by focusing on issues that are revealed as inconsistent with human factors guidelines but require user verification.In certain cases, where an inconsistency exists, it is essential to verify its implications from the user's perspective.Examples of usability issues are color discrimination, screen layout, data extraction, character size and label and abbreviation meaningfulness.It is important to verify system usability prior to the assessment of system suitability.If traffic managers find the tool difficult to use for performing various traffic management activities, then it is important to know, up front, whether the display and interactive features may be contributing to the difficulty.Usability exercises are conducted for each release of the TMA software where modifications have been made to the user interface or new features have been added.Suitability assessments focus on the match between the design and the user's task.A system is suitable to the extent that design features and functions support users at their job.A recent assessment addressed the effectiveness of TMA display representations of data from the current metering system, the Arrival Sequencing Program (ASP).TMA with its color, timelines, and graphs represents a significant change from the traffic managers' current metering system user interface.For the TRACON, TMA is their first exposure to metering information.Given these changes, it is important to verify the effectiveness of the TMA display representations at supporting traffic management decisions and activities.Such display representations may modify the way the traffic manager performs traffic management, offering new and different opportunities for making traffic management decisions.Some usability issues are also addressed during this phase of the assessment.Display clutter, color coding, and symbology, for example, may be assessed differently when users are actively engaged in using the TMA functions to solve traffic management problems versus when they are passively evaluating features in an off-line mode.It is necessary for the users to have access to TMA in their operational environment in order to investigate suitability issues.The next phase of suitability assessments will investigate TMA use with CTAS schedules.
TMA in the FieldAssessments are conducted in the traffic management areas at the Denver Center and TRACON.This location serves both technical and pragmatic interests.Traffic management involves extensive coordination with other traffic managers and area supervisors, communications with other facilities, and accessing and integrating information from a variety of different sources, such as weather displays, aircraft situation displays, and flight strips.Understanding TMA use in the context of these operational activities is essential for addressing TMA suitability and user acceptance.In addition, access to operational lighting conditions is thus desirable for validating such technical usability issues as color discrimination and readability.Lighting in the operational area is complex, with overhead lighting located in high ceilings and local lighting on work surfaces.The location of the test area also accommodates resource constraints and works well with the culture of the Unit.It was not possible to schedule participants prior to the assessment, so the supervisors on duty released traffic managers when the traffic demand allowed.Having the supervisors control our access to the traffic managers minimized our impact on the Unit, thereby increasing "buy-in" to the assessment process.Supervisors could release and summon traffic managers as the conditions permitted.A variety of different approaches are used to assess TMA.Scenario driven surveys using prerecorded traffic data are used to assess TMA usability.Shadowing exercises, where TMA shadows traffic management operations, are used to assess technical usability, domain suitability, and user acceptance.These approaches and methods are described next.
ScenariosScenarios are used to assess TMA usability issues, and systematically guide the traffic managers through the TMA display and interactive features, instructing them to view or manipulate different features.Pre-recorded traffic data is used to ensure that everyone views the same traffic conditions during the exercise.Associated with each scenario are validation statements that focus on specific technical usability issues, such as color discriminability, symbol detectability, and ease of interacting with the input devices.Traffic managers indicate whether they agree or disagree with the statement and space is provided for comments and suggestions.A human factors engineer observes the traffic managers as they complete the survey, answering any questions and observing TMA use.
Shadow Mode Exercises & Interviews"Shadowing" involves a traffic manager using TMA to make traffic management decisions, mirroring the activities at the operational traffic management position.The shadowing traffic manager has access to all other sources of information in the Unit except for the operational traffic management system.One observer observes and queries the shadowing traffic manager, and the traffic manager's ongoing commentary is tape recorded for later analysis.Another observer watches the operational traffic manager.Here, traffic management activities and decisions are observed in a more passive mode to avoid disrupting operations.Understanding and interpreting TMA use, at both the Center and TRACON, depend upon an understanding of the operational context.The second observer is critical in this regard.Shadow-mode operations are effective for discovering unexpected tool-uses and for assessing issues of technical usability, operational suitability, and user acceptance.Methods for data collection are similar at the TRACON and Center but are tailored for the unique constraints of each facility.Efforts are focused on capturing the traffic managers' ongoing experience with the system using contextualized interviews [7].This technique involves observing and questioning the users about the tool as they are using it for various planning and problem-solving activities.A critical aspect of contextualized interviews is involving the users in the interpretation of their experience with the system.This aspect is discussed further in the next section on data interpretation.An important aspect of data collection in the field is the period of acclimatization that precedes actual data collection; in our case, surveys and contextual interviews.Prior to conducting structured assessments, we spent several weeks in the traffic management units at the Center and TRACON, simply observing operations and answering questions on the purpose of our presence and the TMA assessment process.This acclimatization period allowed the traffic managers to become comfortable with us, making our observations less intrusive.It also allowed us to work out methodology issues, for example, optimum observation positions and an effective observation checklist, and allowed us to gain a deeper understanding of traffic management operations.
Data InterpretationData interpretation occurs on and off the field site.Observation alone is not sufficient for exploring and assessing system use.The observer's interpretations of the observations must be shared with the user to verify their truthfulness [7].Mutual understanding of the traffic managers' experience with TMA is achieved during the traffic rush and immediately following the rush.The extent of questioning during the traffic rush must be monitored carefully so as not to interfere with planning and problem-solving activities.Following the traffic rush period, the traffic managers are questioned on their experiences with TMA and their impressions of the traffic rush.In turn, we verify our interpretations of TMA-use and their responses to questions.Questions and observations, during and immediately following the traffic rush, are guided by a set of general questions:¥ What was the traffic situation?¥ What decisions and planning activities occurred?¥ What information was accessed from TMA and non-TMA sources?¥ How was TMA used to support various traffic management decisions?¥ What information was lacking or hindered decisions?¥ What improvements are necessary?These questions provide a framework for systematically exploring and understanding TMA use in the context of traffic management operations and provide a basis for deeper probing of usability, suitability, and user acceptance issues.All phases of the interview are tape recorded and conducted at the TMA, in the operational area, to provide a reference for discussing and interpreting the system.The merits of video, for this purpose, have been broadly extolled.Unfortunately, we were precluded from videotaping activities in the control room.
Analysis, Inferences and ImplicationsSurveys, observations, contextual interviews, and subjective ratings provide multiple windows on the traffic manager's experience with TMA.These methods and data provide a qualitative assessment of the match between TMA features and functions and the context for their use.The challenge lies in analyzing this large amount of data in order to make tractable inferences.To date, the focus of the TMA development and assessment process has been on identifying design deficiencies, discovering unexpected feature uses, understanding how the tool is used for various activities, and defining operational requirements.Analyses have been geared accordingly.Frequency counts of negative responses on surveys provide insight into deficiencies and discrepancies.Content analyses of observations and interviews, coupled with subjective ratings, also provide insight into design deficiencies and discrepancies and enhance our understanding of tool use.System requirements evolve from these insights.Effective use of TMA relies on the traffic managers having sufficient understanding of the features and functions.Such understanding is essential for identifying system deficiencies, discovering new uses, defining operational procedures, and meeting program milestones.Training is thus an integral part of the field development and assessment process and is described next.
TrainingTraining on the prototype system is critical for enabling the users to provide meaningful feedback and to have meaningful experiences with the system.Yet a delicate balance must be struck between informing them of the system capabilities and influencing them on how, when, and where these capabilities are actually to be used in problem solving decisions.How the tool is used and how it shapes the task, are a consequence of the constraints and contingencies of the flow of work.These are things to be discovered and are potential emergent properties and implications of the technology in the context of the domain.Allowing the users to develop their experiences with the system is important to determine the match between the system's capabilities and the users' needs.The conduct of training during the field development process is a critical aspect of system validation.Training for the field development and validation of CTAS uses a "cadre" approach [8].This approach involves training selected personnel from the facility as instructors who then train the balance of the facility personnel.The cadre consists of individuals selected for their interest and knowledge of the technology involved, previous training experience and rapport with their peers and facility management personnel.The size of the cadre depends on the size of the facility and personnel availability; so far, CTAS cadres have consisted of three or four individuals.They are trained by a team of technical and operational experts who have a high level of system experience with CTAS and ATC as well as training.Following classroom and hands-on exercises with the system, the cadre tailors the training material to suit their styles and incorporate facility traffic practices as examples to help convey the information.This gives them a sense of ownership for the training program and increases their own buy-in to the system development and evaluation process.Prior to delivering the training, the cadre members perform a dry-run for the CTAS training personnel to ensure that they have sufficient understanding of the system and that their delivery is effective.A well informed cadre can introduce the system's features and functions in the "language" of their peers and dispel any mystery and misconceptions about the technology.The cadre approach also facilitates user buy-in by reinforcing the message that the users are partners in the development and evaluation of advanced automation, encouraging them to explore, manipulate, experiment and observe the system.Such interactions are crucial for validating the system and verifying its appropriateness for the context of its use.Training provided to the cadre and controllers consists of three modes: classroom group instruction, hands-on individual instruction, and exercises using predefined scenarios and off-line traffic.The progression of training follows a modular, function by function approach and is embodied in a sequential, simple to complex presentation of the material.The focus is on providing sufficient understanding of the system's capabilities so that the user can experiment with the system.Classroom instruction is guided by a lesson plan based on FAA Order 3000.6c.Each feature and function is explained with the aid of the workstation to show the dynamic representation of the various TMA features.Class sizes are small (2-4 students) to minimize the impact on the facility scheduling.The second mode, hands on individual instruction, reinforces the classroom instruction by systematically exercising all interactive features and viewing TMA representations.Students operate the system to understand its "surface" characteristics with the assistance of an instructor or cadre member.The last mode, exercising the system off-line with representative traffic, allows the user to use the features to make various traffic management decisions off-line and to perform what-if experiments.The system runs in a "shadow mode" on the operational floor as well as in a separate training room.This last mode is a seamless transition to the type of activities that occur while the system is under evaluation, allowing the user's experience with the system to grow with exposure to a variety of situations.
CONCLUSIONWhile CTAS is viewed as an innovative system from a technical perspective, the development process is equally innovative.Key elements of the process are field assessment and training conducted early in the system development cycle.In order to provide input to the definition of operational requirements and to develop an automation tool that is an effective problem-solving instrument, it is important both to capture the user's experiences with the system, and to understand tool use in an operational context.The more traditional approach calls for field evaluation at the end of the development cycle, but with the integration of advanced information technologies into ATC systems, it is becoming increasingly risky to evaluate design problems out of the complex context of their use.By iteratively assessing the usability and suitability of the system while it is still under development, we are able to provide input into the definition of system requirements, and facilitate development of a system that integrates well into the current operational environment.		
		
			

				


	
		
			DMeister
		
		Behavioral Analysis and Measurement Methods
		New York
		
			John Wiley and Sons
			1985
		
	
	Meister, D., Behavioral Analysis and Measurement Methods ., John Wiley and Sons, New York, 1985.



	
		How to Design Usable Systems
		
			JohnDGould
		
		10.1016/b978-0-444-70536-5.50040-3
	
	
		Handbook of Human-Computer Interaction
		
			MHelander
		
		North Holland), New York
		
			Elsevier
			1988
			
		
	
	How to design usable systems
	Gould, J.D., How to design usable systems. In M. Helander, (Ed.), Handbook of Human- Computer Interaction. Elsevier Science Publishers BV (North Holland), New York, 1988, 757-789.



	
		Explorations in joint human-machine cognitive systems
		
			DDWoods
		
		
			EMRoth
		
		
			KBBennett
		
	
	
		Cognition, Computing, and Cooperation
		
			SPRobertson
		
		
			WZachary
		
		
			JBBlack
		
		Norwood, NJ
		
			Ablex Publishing Company
			1990
			
		
	
	Woods, D.D., Roth, E. M., and Bennett, K.B., Explorations in joint human-machine cognitive systems. In S.P. Robertson, W. Zachary, and J.B. Black, (Eds.), Cognition, Computing, and Cooperation , Ablex Publishing Company, Norwood, NJ, 1990, 123-158.



	
		Information Technology and Work
		
			JensRasmussen
		
		
			LPGoodstein
		
		10.1016/b978-0-444-70536-5.50014-2
	
	
		Handbook of Human-Computer Interaction
		
			MHelander
		
		North Holland), New York
		
			Elsevier
			1988
			
		
	
	Information technology and work
	Rasmussen, J. and Goodstein, L.P., Information technology and work. In M. Helander (Ed.), Handbook of Human-Computer Interaction . Elsevier Science Publishers BV (North Holland), New York, 1988, 175-201.



	
		Ethnographically-informed systems design for air traffic control
		
			RBentley
		
		
			JAHughes
		
		
			DRandall
		
		
			TRodden
		
		
			PSawyer
		
		
			DShapiro
		
		
			ISommerville
		
		10.1145/143457.143470
	
	
		Proceedings of the 1992 ACM conference on Computer-supported cooperative work - CSCW '92
		the 1992 ACM conference on Computer-supported cooperative work - CSCW '92
		
			ACM Press
			1992
		
	
	Bentley, R., Hughes, J.A., Randall, D., Rodden, T., Sawyer, P., Shapiro, D., and Sommerville, I., Ethnographically-informed systems design for air traffic control. CSCW Proceedings , 1992.



	
		Defining Human-Centered System Issues for Verifying and Validating Air Traffic Control Systems
		
			KellyHarwood
		
		10.1007/978-3-662-02933-6_6
	
	
		Verification and Validation of Complex Systems: Human Factors Issues
		
			JWise
		
		
			VDHopkin
		
		
			PStager
		
		Berlin
		
			Springer Berlin Heidelberg
			1993
			
		
	
	Verification and Validation of Complex and Integrated Human Machine Systems
	Harwood, K., Defining human-centered system issues for verifying and validating air traffic control systems. In J. Wise, V.D. Hopkin, and P. Stager (Eds.), Verification and Validation of Complex and Integrated Human Machine Systems , Springer-Verlag, Berlin, 1993.



	
		Usability Engineering: Our Experience and Evolution
		
			JohnWhiteside
		
		
			JohnBennett
		
		
			KarenHoltzblatt
		
		10.1016/b978-0-444-70536-5.50041-5
	
	
		Handbook of Human-Computer Interaction
		
			MHelander
		
		North Holland), New York
		
			Elsevier
			1988
			
		
	
	Handbook of Human-Computer Interaction
	Whiteside, J., Bennet, J., and Holtzblatt, K., Usability engineering: Our experience and evolution. In M. Helander (Ed.), Handbook of Human-Computer Interaction . Elsevier Science Publishers BV (North Holland), New York, 1988, 791-817.



	
		A training approach for highly automated ATC systems
		
			HBergeron
		
		
			HHeinrichs
		
	
	
		Proceedings of the Seventh International Symposium on Aviation Psychology
		the Seventh International Symposium on Aviation Psychology
		
			April, 1993
		
	
	Bergeron, H, and Heinrichs, H., A training approach for highly automated ATC systems, Proceedings of the Seventh International Symposium on Aviation Psychology , April, 1993.


				
			
		
	
