
	
	
		
Their participation and enthusiasm helped see this project through and greatly eased our transition into ZFW and the air traffic control environment.
Air Traffic Coordination and TerminologyAs a coordinator of air traffic management, a TMC estimates and predicts the demand of air traffic and a facility's capacity to absorb it.Demand describes the number of aircraft destined for a common airspace, be it a sector or an Air Traffic Control (ATC) facility, within a specified block of time.Capacity defines the maximum number of aircraft that can be safely accommodated and controlled within an airspace and during a given period.The capacity level of a facility is very dynamic and is heavily influenced by weather conditions, the availability of runways and meter gates, capacity fluctuations of adjacent ATC facilities, and the staffing level at the facility.A "rush" period refers to a period of time when the demand of traffic exceeds the handling
TMA in the Traffic Management Unit (TMU)The TMU consists of a group of TMCs working in concert to coordinate all traffic within the facility.The TMCs are presented with two flow visualization tools, the PGUI and TGUI.These tools provide the TMC with different visual representations of the arrival traffic.The PGUI shows a planview of the Center airspace (fig.2).Aircraft radar tracks within the Center boundary and as far as 400 nmi from the meter fix can be displayed on the PGUI.The PGUI allows the TMC to quickly and easily query aircraft information via aircraft datablock tags.Alternatively, the PGUI can display aircraft in the sequence-list format similar to that displayed on the PVD.TMA was used in place of an existing tool, ASP, which provided a sequence list format on the PVD.One of the conditions imposed by the TMCs was that this format be available in TMA as well.Aircraft sequences are frozen as each aircraft's ETA passes the freeze horizon.The freeze horizon is a userdefined temporal parameter that determines when (in minutes, before an aircraft reaches the reference point, generally a meter fix) aircraft schedule, sequence and STA time should be fixed.Frozen sequences and delay values are displayed in a list format on the controller's PVD scope.The frozen schedules give the controller a stable list to control aircraft.From the TGUI timelines, the TMC can predict when and where a rush will arrive.*.l u ZFW Air.-l_Â¢e _ Aircraft S_l_nee Li$1 _'_ _ _'._ -,_.q, n,. o,.The predictive capability permits the TMC to produce an effective traffic plan.In addition to traffic planning, ZFW and DFW-TRACON TMCs used TMA as a collaborative decision making tool.Because Center can provide TRACON with an accurate characteristic of the impending rush traffic, the TMCs can negotiate an AAR that is acceptable to both facilities.
TMA at the Radar PositionsUse of TMA had minimal impact on the controllers.Under ASP, the STA metering times and delay values were displayed on the controller's PVD in a table-like format called a sequence list.TMA emulated the format of the ASP sequence list to display advisory times.From the controller's perspective, the TMA-generated sequence list varied little from the ASP list.The variation included the outer arc reference for high altitude controller and tactical sequence control.For all intents, TMA usage was made transparent to the sector controllers.The only indication given to the controller is a header message indicating TMA was operational instead of ASP.The header message, located at the top of the sequence list, indicates the AAR along with a text message displaying "CTAS."
Scope of the Field Evaluation Engineering EvaluationsThe field evaluation was conducted in two stages, one week of engineering evaluation and four weeks of operational evaluation in which quantitative and qualitative data was gathered.The primary objective of the engineering evaluation was to establish a secured two-way (bi-directional) communication link between the ZFW Host computer system and the TMA system.The two-way communication allowed for the broadcasting of TMA advisory times onto the controllers' PVD.Operational data collection depended on the successful completion of the engineering evaluation.Other testing criteria were assessed during the engineering evaluation period.Accuracy of STA times and delay values at the outer fix and meter fix were scrutinized.The sequencing of arrival aircraft to each meter fix and runway was checked for efficiency.Another goal of the evaluation was to check for assurances that all available landing slots were occupied and loading of the runways was balanced.
Operational EvaluationsThe For each of the 39 recorded rushes, an engineering and a human factors data set was recorded.The TMA evaluation team held a debriefing after each rush period to clarify any system and traffic anomalies encountered during that rush.Together, the two data sets formed a complimentary view of how traffic was controlled, the workload involved in controlling it, and the factors that  where demand dissipated."Front-Load" allows TRACON to land more aircraft than the AAR, at the beginning of the rush when a higher capacity is allowed.The amount of traffic that exists before front-loading is comparable to the drop-off period.Figure 7 represents a typical rush period with front loading.
Challenges and LimitationsMany challenges were encountered during the preevaluation and formal evaluation period at ZFW.The Without exception, TMA had to gain non-interference approval from the FAATC before operational evaluation at Z FW.
Shadow Data CollectionA series of live traffic recordings or "shadow" data files were collected before and during the field evaluation.These shadow files formed the baseline model against which the data collected during field evaluation was to be compared.The shadow files captured STA time, delay value, and aircraft sequence data that were generated by the ASP metering program.These files show the traffic handling capability at ZFW under ASP.A method of assuring a statistical match between the baseline and field test data set was to record a large sample of baseline data.It was anticipated that matching data sets could be found between similar rush periods.Although not a part of the evaluation team, the TMCs at the TRACON facility also assisted with the field evaluation.They were instrumental in providing feedback to the evaluation team.The feedback included comments on the quality and quantity of aircraft delivered into the TRACON airspace.This allowed the ZFW TMCs to update their traffic plans, and to adjust the flow characteristics of subsequent rush periods.
Challenges at the
Operational ProceduresAn operational procedure for the field evaluation was developed with inputs from ZFW management and controllers.The procedure described when, who, and under what conditions the TMA system should be engaged to meter traffic.These procedural limitations were necessary to ensure air safety.A fundamental constraint stated that under no circumstances shall the TMA system interfere with the Host computer system.In a worst case scenario, where the TMA system malfunctioned while metering traffic, a safety procedure was developed that allowed the ZFW facility to revert back to the ASP program for metering advisories.Several simulations of this procedure showed that it took less than one minute to accomplish the transition.The ZFW facility found the transition time acceptable.Although this was an unlikely scenario, the test team had to demonstrate this capability to gain the confidence of the personnel involved and ZFW management.The operational procedure required that the engagement of the TMA system be conducted under the supervision of a TMC who was trained on the TMA system.The TMC would determine if metering was necessary, when to meter, and the duration of the metering period.Ultimately, the TMC is accountable for flow control decisions that affect all aircraft within the facility's airspace.If necessary, the TMC had the ability to override any and all TMA generated advisories.In addition, a TMA engineer was required to be present during each metering session to monitor the health of the system.The evaluation also stipulated that the testing exact minimal impact on normal facility operations.As an example, prior to engaging the TMA system, the air traffic automation staff (who maintains the Host computer) was asked to enable two software "switches" that allowed TMA to communicate with the Host
Training IssuesTo operate TMA efficiently and effectively, TMCs  The collection of data was very much an opportunistic affair.Although the test matrix provided a data gathering schedule, the behavior of the rush traffic was uncertain.An example of this opportunistic activity can be seen in the data collection made on July 23rd (see Table 1).Three data recording periods were planned for that evening but the rush traffic did not appear.Earlier in the day, a thunder storm encroached on the Dallas/Ft.Worth area and forced traffic to divert to adjacent ATC facilities.The diversion dispersed enough aircraft that the anticipated 6:00 and 8:00 PM rushes never materialized.Whereas on July 24th, a rush period formed an hour earlier than the scheduled 3:00 PM rush.By the time the 3:00 PM hour arrived, there was not enough traffic to require metering.The test team was alert to traffic conditions and data collection was initiated when warranted.The diverse traffic collected in the data files was classified using attributes common to each rush.The data file contained information such as airport flow configuration, AAR, weather conditions, and delay classification.From the amount of delay imposed upon the aircraft in the system, the rush traffic could be classified as light (delay less than 5 minutes), medium (less than 15 min.)or heavy (greater than 15 rain.).These attributes were used to catalog and classify the type of rush traffic, as well as database search parameters.The same cataloging method was applied to the shadow files.Although each recorded rush was unique, the cataloging scheme provided a means of comparing similarly configured operational and baseline data files.At the sector stations, the objective of the data collectors was to collect accurate log of traffic while keeping controller-interrupts to a minimum.To insure accuracy, it is preferable to document traffic situations as they unfold rather than from memory.Clarification of traffic anomalies was more accessible at the CTAS-TMU than at the sector stations, due to high controller workload.Human factors data were collected at randomly chosen sector stations and at the CTAS-TMU, where engineering data was also collected.
Concluding RemarksMany challenges were encountered in preparation for the TMA field evaluation at ZFW, from the construction of the baseline database to the non-interference testing at the FAATC.These challenges included: installationIntroductionThis paper describes the challenges of conducting a field evaluation of a modern air traffic management tool at an operational Air Route Traffic Control Center (ARTCC) facility.The tool is the TMA.TMA is being developed by the NASA Ames Research Center and the Federal Aviation Administration (FAA) under the Center-Terminal Radar Approach CONtrol (TRACON) Automation System program (ref.1).The CTAS program was created to develop advanced air traffic control decision support tools.The test facility was the Fort Worth ARTCC or ZFW.The field test evaluated the functionality and usability of the human-computer interface, and the acceptance of the TMA tool by the ZFW facility.The test also validated the usefulness and acceptability of the TMA system by ZFW controllers and traffic management coordinators (TMCs).The paper details the operational impact TMA had on the ZFW facility.An in-depth analysis of the data collected during the test is presented in follow-up papers by Swenson (ref.2), and Sanford and Lee (ref.3).Participation by the ZFW and Dallas/Ft.Worth (DFW)-TRACON facilities, FAA William J. Hughes Technical Center and the FAA CTAS Program Office, as well as various FAA contractors made this field evaluation a successful venture.Special thanks go out to the Fort Worth Center TMCs, Danny Vincent, Tommy Sanders, and Dutch Daugherty, our National Air Traffic Controllers Association (NATCA) representative, Jim Karlovich, and DFW-TRACON TMC-Jerry Saunders.
capacity of the ATC facility.During a rush period, flow management methods are implemented to insure a safe and expeditious flow of air traffic.The flow control methods consist of any combination of the following techniques: aircraft route modifications, redistribution of time and distance separation between aircraft, speed
Figure 2 .2Figure 2. Aircraft tracks and sequence list, as displayed on the PGUI.
Figure 3 .3Figure 3. Threshold timeline with aircraft tags and STA times.
Figure 4 .Figure 5 Figure 5 .455Figure 4. Aircraft tracks in center airspace,
Figure 6 Figure 6 .66Figure 6 plots the predicted aircraft throughput (flow) as a function of time.The throughput peaked at 30 aircraft about 16:30 UTC (or 17,000 seconds after TMA started).The TMA aircraft-count parameter was set to count aircraft within a 10-minute sliding window.With 30 aircraft per 10-minute interval and at an AAR of 108, which is equivalent to 18 aircraft per 10 minute interval, demand easily exceeded airport capacity.The figure plots arrival aircraft for both DFW--solid line and Dallas Love Field (DAL)-dotted line, airports.Currently, only TMA generated scheduling times and sequences for aircraft landing at the primary airport (DFW) are used and not at any satellite airports (i.e., DAL).
Figure 7 .7Figure 7. Threshold aircraft count.
challenges and limitations varied in scope, ranging from system checkout to training of facility personnel, and installation ofthesystem atthetest site.Inthefield, procedures ofconduct were developed foroperational useanddata collection.Challenges Prior to the Field EvaluationHuman-in-the-Loop Computer Simulation Extensive preparatory work was done to support the field evaluation.The human-in-the-loop computer simulations employed pilots and controllers to evaluate the TMA software.Controllers used TMA-generated sector airspace maps to give advisories to the pilots, who flew simulated aircraft that had six degrees of freedom dynamics (ref.9).The simulations duplicated most controller and pilot interactions and exercised most TMA functions.However, the human-in-the-loop simulations conducted at NASA Ames could not simulate the two-way, bi-directional communication protocol that takes place between the TMA and Host Computer System (Host) at the Center.The testing of the two-way communication was performed at the FAATC in New Jersey.The FAATC facility has the capability to duplicate the operational environment at ZFW.The simulation permitted interaction between the Host computer, TMA, and controllers.FAATC simulations examined controller issues, verified non-interference, and allowed for an overall system checkout.Issues that controllers looked for included the quality and usefulness of the humancomputer interface, and testing of new controllerspecific functions.Checks for non-interference compliance were extensively conducted to insure communication integrity with the Host.In two-way mode, the Host transmits TMA-generated times to be displayed on the PVD scope of all metering sectors.
CTAS-TMU used TMA to construct a plan to meter traffic.When the CTAS-TMU is operational, the existing ZFW TMU acts as a secondary unit.In the event of a CTAS-TMU emergency, the ZFW TMU would supersede as the primary TMU.The CTAS-TMU area housed TMA and the supporting hardware (fig.8).Five Sun SPARCstation20 workstations, video displays, a PVD, and the TMC communication station were located in the CTAS-TMU area.Five additional UNIX workstations were situated at another location, but all were connected on the same network.The compliment of nine workstations is considered a minimal hardware configuration required for operational use and to conduct the field evaluation.The TMC monitors all arrival traffic via the PVD, PGUI, and TGUI displays.The communication station allows the TMC to contact other TMCs, sector controllers, and ATC facilities.
Figure 8 .8Figure 8. CTAS-TMU, TMA engineer (background), and TMC with PVD scope (foreground).
computer.The same switches were used to disconnect TMA from the Host computer.The toggling of the switches could be accomplished by one person and required little supervision by the automation staff, thereby minimizing facility support of TMA.
unfamiliar with TMA were required to attend a training session.The training required one day of classroom instruction and several weeks of on-the-job training (OJT).The new TMCs were trained by a TMC on the evaluation staff.The classroom instruction introduced the trainees to TMA capabilities.The OJT allowed the trainees to exercise TMA functionality and varied in duration for each TMC.The TMCs were not required to get formal certification on the TMA system, but it was critical that they got fully acquainted with the capability of the system.Decisions made by the TMC have a rippling effect throughout the facility, affecting high sector and low sector controllers, and the delivery of traffic into the TRACON facility.As mentioned previously, training of sector controllers on the interpretation of the sequence list was minimal.The similarity of the format of the ASP vs. TMA lists required little controller training and expedited the acceptance of the TMA system.The minimal requirement in training was significant, considering the number of controllers working at the facility.Many controllers work the rush traffic, with up to three controllers per sector and a minimum of eight sectors.Throughout the evaluation period, TMA was used to meter traffic during different hours of the day with at least one change of controller work-shift.Each work shift change has the potential to introduce new variables into the test system, in terms of aircraft handling preference and style.Nevertheless, it was critical that the
of the system at the test site, training of the TMCs and controllers, development of operational procedures including fall-back plans in the event of a TMA failure, matching the display format of a previously used metering aid and establishing data collection protocols.The hurdles were overcome via a high degree of coordination and cooperation between ZFW management, controllers, and the TMA evaluation team.The field evaluation itself is a challenge.Unlike the controlled environment of a simulation test where specific cases can be created and arrival patterns can be duplicated exactly, gathering data in the field is strictly opportunistic.Matching ensembles of data sets collected with TMA in use to "equivalent" rush periods when TMA was not in use was a painstaking process, involving developing a categorization based on rush attributes.Careful collection of human factors data at radar positions involved creation of data collection forms, completion of controller questionnaires, as well as intensive observation of the rush period.Overall, the field evaluation was considered a tremendous success.The field evaluation imposed minimal impact on the hosting facility and provided valuable engineering and human factors data.The success of the project was realized when the ZFW facility requested to have TMA operational during all metering periods.
Table 1 .1objective of the second stage of the evaluation was Fieldevaluation test matrix.geared toward the collection of engineeringand humanfactors data. Operationally,TMA metered multiple rushtraffic throughout the day. An in-depth analysis of theengineering data is detailed by Swenson (ref. 2).Engineering data collected during the operationalperiodHuman factors issues included human-computerinterface and workload factor as perceived by bothcontrollers and TMC during rush periods.ZFW rush traffic hours were known to occur at: 8, 9, and11 AM, 12 Noon, 1,2, 5, and 7 PM local time. Due toresource constraints, but for quantitativecomparisons,TMA was used to meter two occurrences of the samerush hour traffic. Table 1 shows the rush traffic that wasrecorded during the engineering and operationalevaluation periods. When time permitted, additionalshadow files (see "Shadow Data Collection")werecollected to expand the shadow-filedatabase. A total ofwere very similar to those collected during the engineering evaluation period, i.e., aircraft sequencing and accuracy of STAand delay values.Human factors assessments are addressed by Sanford and Lee (ref.3).
The Data Set and Recording
			operational periods were recorded.As shown, TMA metered multiple rush hours throughout the day.
		
		
			
Operational procedures were developed and applied to the evaluation process that would ensure air safety.The five weeks of field evaluation imposed minimal impact on the hosting facility and provided valuable engineering and human factors data.The collection of data was very much an opportunistic affair, due to dynamic traffic conditions.One measure of the success of the TMA evaluation is that, rather than remove TMA after the evaluation until it could be fully implemented, the prototype TMA is in continual use at ZFW as the fully operational version is readied for implementation.
SUBJECT TERMS			
			

				


	
		Design of Center-TRACON Automation System
		
			HErzberger
		
		
			TJDavis
		
		
			SMGreen
		
	
	
		Proceedings of the AGARD Guidance and Control Panel 56th Symposium on Machine Intelligence in Air Traffic Management
		the AGARD Guidance and Control Panel 56th Symposium on Machine Intelligence in Air Traffic ManagementBerlin, Germany
		
			1993
			
		
	
	Erzberger, H., Davis, T.J., and Green, S.M.: "Design of Center-TRACON Automation System." Proceedings of the AGARD Guidance and Control Panel 56th Symposium on Machine Intelligence in Air Traffic Management, Berlin, Germany, 1993, pp. 11-1-11-12.



	
		Design and Operational Evaluation of the Traffic Management Advisor at the Ft. Worth Air Route Traffic Control Center
		
			HNSwenson
		
		
			THoang
		
		
			SEngelland
		
		
			DVincent
		
		
			TSanders
		
		
			BSanford
		
		
			KHeere
		
		
	
	to be published as a NASA Technical Memorandum
	Swenson, H.N., Hoang, T., Engelland, S., Vincent, D., Sanders, T., Sanford, B., and Heere, K.: "Design and Operational Evaluation of the Traffic Management Advisor at the Ft. Worth Air Route Traffic Control Center," to be published as a NASA Technical Memorandum.



	
		APPENDIX C:
		
			BSanford
		
		
			KLee
		
		10.2307/j.ctv25c4z16.37
	
	
		Venus
		
			University of Arizona Press
			
			
		
	
	to be published as a NASA Technical Memorandum
	Sanford, B. and Lee, K.: "Traffic Management Advisor: An Operational Human Factors Evaluation," to be published as a NASA Technical Memorandum.



	
		THE FINAL APPROACH SPACING TOOL
		
			TJDavis
		
		
			KJKrzeczowski
		
		
			CBergh
		
		10.1016/b978-0-08-042238-1.50015-x
	
	
		Automatic Control in Aerospace 1994 (Aerospace Control '94)
		Palo
		
			Elsevier
			
			
		
	
	Davis, T.J., Krzeczowski, K.J., and Bergh, C.: "The Final Approach Spacing Tool." Proceedings of the 13th IFAC Symposium on Automatic Control in Aerospace-Aerospace Control, Palo



	
		Electric Power Research Institute, Environmental Control Technology Center monthly report to the Steering Committee, June 1994
		
			CAAlto
		
		10.2172/37661
		
			1994
			Office of Scientific and Technical Information (OSTI)
			
		
	
	Alto, CA, 1994, pp. 70-76.



	
		APPENDIX C:
		
			GWong
		
		10.2307/j.ctv25c4z16.37
	
	
		Venus
		
			University of Arizona Press
			
			
		
	
	to be published as a NASA Technical Memorandum
	Wong, G.: "The Dynamic Planner," to be published as a NASA Technical Memorandum.



	
		
			AirTraffic
		
		
			ManagementBranch
		
	
	
		TMA Procedure Summaries, Release 4.2.1 t
		Moffett Field, CA
		
			July 1995
		
	
	Air Traffic Management Branch: "TMA Procedure Summaries, Release 4.2.1 t," NASA Ames Research Center, Moffett Field, CA, July 1995.



	
		
			AirTraffic
		
		
			ManagementBranch
		
	
	
		Traffic Management Advisor, PGUI Addendum
		Moffett Field, CA
		
			August 1996
		
	
	Release 4.3.0t
	Air Traffic Management Branch: "Traffic Management Advisor, PGUI Addendum, Release 4.3.0t," NASA Ames Research Center, Moffett Field, CA, August 1996.



	
		Two Way Interface (TWI) Developer's Guide, Preliminary Release
		
			AirTraffic
		
		
			ManagementBranch
		
	
	
		NASA Ames Research Center
		
			January 1997
			Moffett Field, CA
		
	
	Air Traffic Management Branch: "Two Way Interface (TWI) Developer's Guide, Preliminary Release." NASA Ames Research Center, Moffett Field, CA, January 1997.



	
		Pseudo Aircraft Systems - A multi-aircraft simulation system for airtraffic control research
		
			ReidWeske
		
		
			GeorgeDanek
		
		10.2514/6.1993-3585
	
	
		Flight Simulation and Technologies
		Monterey, CA
		
			American Institute of Aeronautics and Astronautics
			August 1993
		
	
	Weske, R.A and Danek, G.L.: "Pseudo Aircraft Systems: A Multi-Aircraft Simulation System for Air Traffic Control Research." Proceedings of the AIAA Flight Simulation Technologies Conference, Monterey, CA, August 1993.


				
			
		
	
