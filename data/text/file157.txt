
	
	
		
I. INTRODUCTIONAs the popularity of using small Unmanned Aircraft Systems (sUAS) for various applications continues to increase in the coming years, so will the number of aircraft operating in low altitude airspace.With this increase in air traffic density, it is imperative to accurately identify and track all vehicles.This becomes increasingly evident when operators, both commercial and hobbyists alike, begin flying their vehicles beyond visual line of sight (BVLOS).In these scenarios, accurate state estimation is vital, regardless of whether a human traffic manager or an automated system is responsible for keeping aircraft separated from one another or from physical structures.Due to the fact that several sensor sources can provide position estimations for a single aircraft, these data must be interpreted and combined into a single position estimate.If all data were provided to an air traffic manager or sUAS operator, the abundant information could be overwhelming and difficult to interpret.Furthermore, it may be difficult for a human operator to identify readings that are inaccurate or contradictory, which may lead to poor decisions.Although many solutions have been presented to take the data from multiple sources and fuse the values into a single estimation, these solutions are typically constrained to specific systems and are not adaptive based on the accuracy and number of sensors being used [1,2].In this study, a novel adaptive sensor fusion technique that can be used in real-time operations is proposed.In particular, three sensor types are used to identify where several sUAS are located: GPS, radar, and an onboard detection suite.For the onboard and radar sources, a preliminary Maximum a Posteriori (MAP) estimator is used to combine the various readings from similar sensor types to a single estimation.After a single reading from each sensor type is found, a Fuzzy Logic based system is used to determine the confidence in each reading to create a weighted average position estimate.
II. PROBLEM DESCRIPTIONGiven position data for several sUAS, obtained from several GPS, radar, and onboard detection sensors, one must be able to provide an accurate estimate of where the vehicle is located using sensor fusion.The amount, and accuracy, of information available to the sensor fusion platform is determined by each sensors' specifications.In this study, the source of each data point is known, as well as, to which vehicle the data belongs.In the following sections, a description of each sensor and its performance is shown.
A. Sensor PlatformsIn this study, three sensor platforms, or types, were used to provide raw position data of vehicles in three-dimensional space: GPS, radar, and onboard sensors.Each of these sensor platforms vary in performance and reliability.Thus, a vehicle may be identified by one or more sensor types.Whereas the GPS platform produces only one position estimate for each vehicle, for both the radar and onboard sensor platforms more than one source may be within range to sense the vehicle.For example, if multiple vehicles are within close proximity, their onboard detection sensors will each measure the position of one another.To model the various sensors for simulation, functions were created to provide noisy position estimates of a vehicle given its true position.To accomplish this, the perceived location of each vehicle was found using each sensors' respective performance (i.e. standard deviation in error).By using the standard deviation in error for each sensor platform and the built-in randn function in MATLAB, a normally distributed pseudorandom number generator, a noisy output could be found.Because the GPS system estimates the position of the vehicle in three-dimensional space, the standard deviations in error for the lateral and vertical planes were used to generate perceived vehicle position estimates in Cartesian space.For both the ground based radar and the onboard sensor packages, the vehicle position is estimated by converting the range, azimuth, and elevation measurements to Cartesian space.The standard deviation in error for each sensor package is shown in Table I.In addition, the maximum sensing range for the radars and onboard sensors are shown.The error values for the radar and GPS platforms were found in [3] and [4], respectively.Because this study focuses on sUAS applications, the radar maximum sensing range was limited to 2000 m.However, since the radar source is designed to detect larger objects at a greater distance, the probability of detection of an sUAS at this range would be low.By increasing the sensing range the error associated with all measurements also increases.Therefore, this large detection range allowed the sensor fusion system to be tested under high uncertainty conditions.The onboard detection sensor standard deviation values were not selected from any particular sensor package.These onboard sensor values were selected such that they provide an optimistic estimation of a nearby vehicle.Thus, the solution presented in this study needs to be tuned to the sensor packages available.With these standard deviations in error, the measured location of each vehicle can be found using the following equations.For the GPS, the measurement values were found using (1) through (5).
𝛼 = 2𝜋 𝑟𝑎𝑛𝑑   𝑟 = 𝜎 𝑟 𝑟𝑎𝑛𝑑𝑛 Where  is the built-in MATLAB function to find a random number between zero and one,   is the standard deviation in the lateral plane,  is the random standard deviation function, and  and  are the noisy returned measurements of the angle and range, respectively.The noisy measurement in the lateral plane, found using (1) and ( 2), can be converted to Cartesian space (  ,   ,   ) using (3), (4), and (5).Here, , , and  represent the true location of the vehicle in Cartesian space,   is the standard deviation in altitude, and  is the random standard deviation function.𝑥 𝑚 = 𝑥 + 𝑟 cos 𝛼(3)  =  +  sin  (4)𝑧 𝑚 = 𝑧 + 𝜎 𝑧 𝑟𝑎𝑛𝑑𝑛 (5)Similarly, the measurements for both the radar and onboard detection sensor types can be found using (6) through (11).Here, ,  , and  are the true range, azimuth angle, and elevation angle, respectively, used to describe the vehicle location with respect to the sensor source,   ,   , and   are the standard deviations in range, azimuth, and elevation, respectively, and   ,   , and   are the noisy measured range and angles with respect to the sensor location.𝑅 𝑚 = 𝑅 + 𝜎 𝑅 𝑟𝑎𝑛𝑑𝑛(6)  =  +    (7)𝜀 𝑚 = 𝜀 + 𝜎 𝜀 𝑟𝑎𝑛𝑑𝑛(8)To convert the measured values (  ,   , and   ) from their respective spherical reference frame to the global Cartesian frame, (9) through (11) are used.Here,   ,   , and   represent the position of the sensor source in Cartesian space and   ,   , and   represent the measured position of the vehicle in Cartesian space.𝑥 𝑚 = 𝑥 𝑠 + 𝑅 𝑚 cos 𝜀 𝑚 cos 𝜃 𝑚    𝑦 𝑚 = 𝑦 𝑠 + 𝑅 𝑚 cos 𝜀 𝑚 sin 𝜃 𝑚    𝑧 𝑚 = 𝑧 𝑠 + 𝑅 𝑚 sin 𝜀 𝑚   B.
Simulation EnvironmentTo test the effectiveness of the Fuzzy Logic based sensor fusion system, a simulation environment was used to compare the raw sensor measurements to the fused position estimates.In this study, one or more vehicles were randomly placed within a 2 km by 2 km area.On each corner of the area boundary, a radar source was placed, each with a maximum sensing radius of 2 km.After the vehicle location(s) were set, each sensor package would run independently to measure the location of each vehicle.If a vehicle was located outside of a particular sensor's range, it would not be recognized by that sensor.Thus, some vehicles may be identified by all three sensor sources and others by a combination of radar and only one other source.Due to the locations and ranges of each radar, each vehicle is always identified by at least two radar sources.A depiction of the simulation space is shown in Fig. 1.The various arcs depict the sensor ranges of the radar sources.All simulations conducted constrain the vehicles to be within the areas shaded in yellow due to symmetry.In this figure, the numbers two through four represent the number of radars that can reach that particular region.For this study, 135 cases, each varying in vehicle position and sensor availability, were tested.For each combination of available sensor platforms, a total of 45 cases were tested.For each set of 45 cases, 15 belonged to each designated region shown in Fig. 1.A breakdown of the cases can be seen in Table II.In practice, not all sUAS will be self-reporting its GPS information to a ground based station, or broadcasting its location via a transponder to all surrounding vehicles.Thus, some vehicles that are placed within the airspace will not be identified via GPS.For scenarios involving only radar and GPS, all cases involved only one sUAS.For these scenarios, 15 total cases were evaluated in each radar configuration (two, three, and four radars).These 15 cases were a result of testing five different vehicle locations, each tested at three different altitudes.For scenarios involving only radar and onboard sensors, the available radars again varied, however, for each radar configuration there were between two and four vehicles (whose onboard sensors could sense one another), yielding three different scenarios per radar configuration (nine in total).These nine scenarios were each evaluated for five different vehicle location sets.Lastly, for the scenarios involving all thee sensor types, the same cases as previously described for the onboard and radar case were used, with GPS enabled.For each of the cases shown in Table II, 1,000 independent measurements from each sensor source were obtained, each with randomized noise.The error between the true vehicle location and the measured vehicle location was recorded.These raw measurements were then passed through the sensor fusion package and the results of the final position estimations were compared against the raw measurement values.
III. PROPOSED SOLUTIONTo accurately estimate the state of an sUAS given measurements from several sensor sources, a sensor fusion package based on fuzzy logic was developed.This sensor fusion package considers the number of sensor types and determines how much confidence one should place in each measurement.If, for example, a GPS measurement is obtained, and is known to have low uncertainty, whereas, a radar measurement is expected to have high uncertainty, one would place more confidence in the GPS measurement accuracy.Therefore, the level of confidence for each measurement is taken into consideration to output a single position estimation.In practice, each sensor's accuracy should be found through sensor testing/calibration and/or obtained from manufacturer hardware specifications.Prior to using the fuzzy solution to calculate measurement confidence, two additional steps are taken.The first is to reduce the number of measurements being examined by the fuzzy system.To do this, the algorithm first takes all the radar and onboard data, separately for each sensor type, and combines the measurements into a single estimate for each.Thus, if for example all four radar sources identify an sUAS, the four measurements are combined into a single value.To do this, a Maximum a Posteriori (MAP) estimator was used.Once the measurements for each vehicle were reduced to a single value for each sensor type, the types of sources available for each vehicle were identified.Since a radar measurement was guaranteed for all vehicles in all scenarios, there were three possible sensor type combinations: GPS and radar, onboard and radar, and all three types.In this study, for each measurement recorded, the sensor type and sensor performance is known.In addition, after all measurements are recorded they are processed simultaneously.Therefore, if sensors sample data at different rates, measurements would be stored until the fusion system can process all data simultaneously (i.e.fuse the data at slowest available sensor's sample rate).
A. Maximum a Posteriori EstimatorIf multiple measurements for the same vehicle were obtained from a similar source (e.g.multiple radars or multiple onboard sensors) a Maximum a Posteriori (MAP) estimator was used to combine the multiple measurements into a single position estimate for that sensor type.To accomplish this, the posterior probability distributions of the measurements, as given by the normal distribution parameters in Table I, were maximized to yield the best overall estimation as perceived by that sensor type.Because the standard deviations for each measurement are measured in the local spherical frame, to calculate the MAP estimate one must first convert each individual measurement to the global spherical frame, as shown in (12) through (15).Θ 𝑖 = tan -1 ( 𝑦 𝑖 𝑥 𝑖 )(12)Ε 𝑖 = tan -1 ( 𝑧 𝑖 𝑟 𝑖 ) (13)𝑟 𝑖 = 𝑦 𝑖 sin 𝜃 𝑖(14)Where Θ  , and Ε  are the measurements for the  ℎ radar (or onboard sensor) source in the spherical global frame,   ,   , and   are the raw measurements in the Cartesian global frame ((9) through ( 11)), and   is the two-dimensional range of the measured value on the x-y plane, found using (14).Then using (15), the range in the spherical global frame,   , can be found.𝑅 𝑖 = 𝑧 𝑖 sin Ε 𝑖(15)Using the above global representation for each measurement, the MAP estimate for  radars (or  onboard sensors) can be found using (16).Here,   represents the MAP estimation found using the measurements of interest (  ) and their respective standard deviations (  ).So, this equation can be used to calculate the MAP estimate of the range (  ), azimuth (Θ  ), and elevation (Ε  ) in the global frame by using the respective individual measurements and standard deviations for range (  ,   ), azimuth (  ,   ), and elevation measurements (Ε  ,   ).𝑄 𝑀𝐴𝑃 = ∑ ( ∏ (𝜎 𝑞 𝑗 2 ) 𝑛 𝑗=1 ∑ [∏ (𝜎 𝑞 𝑟 2 ) 𝑛 𝑟=1 ] 𝑛 𝑝=1 𝑄 𝑖 ) 𝑛 𝑖=1,  ≠  and  ≠  (16)Once calculated, the measurement can be converted to the Cartesian frame using (17) through ( 19), where   ,   , and   are the final , , and  MAP values, respectively.𝑥 𝑀𝐴𝑃 = 𝑅 𝑀𝐴𝑃 cos Ε 𝑀𝐴𝑃 cos Θ 𝑀𝐴𝑃(17)  =   cos Ε  sin Θ  (18)𝑧 𝑀𝐴𝑃 = 𝑅 𝑀𝐴𝑃 sin Ε 𝑀𝐴𝑃(19)Although the above MAP estimation helps decrease the measurement uncertainty for each sensor type, the following fuzzy fusion technique can be achieved without finding the MAP estimate.If not found, the sensor confidence levels shown in (20) would need to be modified.In particular, the confidence values for each sensor type would need to be divided by the number of raw measurements obtained from that particular sensor type.Doing so will satisfy the constraints in (20).
B. Fuzzy Sensor FusionOnce the measurements from each source have been reduced (if necessary) to only one estimate per sensor type, the fuzzy sensor fusion package is employed.This fuzzy approach is used to determine how much confidence one should place in each sensor type's estimate.Due to each sensor having variation in its performance for both the lateral (x-y plane) and vertical (z) directions, the sensor confidence was calculated separately for each.Thus, if for example two sensor types are available and one is relatively more accurate in its altitude estimation, but the other is more accurate in its lateral position estimation, one could vary the confidence on each estimation accordingly.Overall, the confidence values for each sensor type are used to create a weighted average of the measurements.Therefore, the final estimation of the vehicle position given  sensor types can be described by (20).Where  ⃗  is the final fused position estimation in threedimensional Cartesian space,   ,   , and   , are measurements from the  ℎ sensor type, and    and    are the lateral and vertical confidence, respectively, for the  ℎ sensor type.In this study, three separate fuzzy systems were developed to help simplify the construction of the sensor fusion system.Overall, each fuzzy system is constructed in a similar manner and is governed by the same fuzzy architecture, where each differs is in the inputs, outputs, and rule bases.Each of the fuzzy systems are of Mamdani-type and have the following architecture: triangular membership functions, fuzzy partitioning, normalized inputs and outputs, minimum "and" method, minimum implication method, sum aggregation, and centroid defuzzification.For the scope of this paper, the fuzzy partitioning is such that the membership functions are structured where the end points of one membership function coincide with the center points of the neighboring membership functions.Due to this fuzzy partitioning, and the fact that each Fuzzy Inference System (FIS) input and output contains three membership functions, for all possible inputs exactly two rules will be activated.Thus, the third rule yields an output of zero membership.This can be verified by referencing Fig. 2.Each FIS consists of a single input with four outputs.Depending on the FIS being used, the input to the system is based on the normalized distance a sensor is from the sUAS.If, for example, one or more onboard sensors detect a single sUAS, the average range the detected vehicle is away from each sensor is used.However, if no vehicles are close enough to a particular sUAS to sense it with their onboard sensors, the input will instead be based on the distance from the radar sources.Here, instead of using the average distance, the input will be the range to the closest radar source.The four outputs of each FIS are dependent on the sensor platforms available.If only two sensor types are available, the FIS outputs would be the confidences in the lateral and vertical estimations for each sensor type.Given this common architecture, examples of input and output membership functions are shown in Fig. 2. In the left inset, the input (normalized distance) is described by three membership functions: Close, Medium, and Far.Regardless of the source used to describe the distance, the input domain will always lie between zero and one.Similar to the input, each output (sensor confidence) is also described by three membership functions: Low, Medium, and High.These two outputs represent the confidence level placed on two sensor types along the same direction (either lateral or vertical).Since the sum of the confidence values for each direction must be equal to one, as seen in (20), the domain for each output must satisfy the constraint described by (21).Where  1 and  2 are the domains for outputs one and two, respectively.Therefore, in this example, the first input domain lies between 0.55 and 0.85, and the second output has a domain between 0.15 and 0.45.Thus, 0.55 + 0.45 = 1 and 0.85 + 0.15 = 1 , satisfying (21).This property will hold for all domains that satisfy this constraint due to the structure of each FIS and the input output relationships described by the rule bases, shown in Tables IV andVI.Since a centroid defuzzification technique is used, and two rules are always active for all input values, the FIS output can never reach the bounds of the output domain.Thus, the actual minimum and maximum outputs of the FIS are limited by the relationship shown in (22).Here,   and   are the minimum and maximum values of the  ℎ output domain (  ), respectively.Out ∈   :𝑎 𝑖 +𝑐 𝑖 6 ≤ Out ≤ 𝑎 𝑖 +5𝑐 𝑖 6 (22)The rule bases of each respective FIS are shown in Tables III, V, and VII.In addition, the domains of each output for each respective FIS are shown in Tables IV, VI, and VIII.Here, the minimum and maximum values for each output are also shown.In Tables III through VIII, the following shorthand is used:• Normalized distance: [Dist] • GPS confidence
a) GPS and RadarIf an sUAS is equipped with an onboard GPS system, and is also detected by two or more ground based radars, the FIS rules shown in Table III are used.Here, the input to the system would be the minimum normalized distance the vehicle is sensed from all radar sources.To normalize the distance input, the true range is divided by 1464.2 m.This is 50 m greater than the distance from the center of the simulation area to any of the four radars.b) Onboard and Radar If an sUAS does not have an onboard GPS system, but is recognized by both a ground based radar and at least one other vehicle, the following FIS is used.The input to the system is the normalized average range the vehicle is from all other sUAS that sense that particular vehicle.This distance is normalized by taking the true average value and dividing it by 150 m.This normalization value was selected after testing and tuning the fusion system.This value is near the range where the onboard sensor errors become exceptionally large (i.e. less accurate).c) GPS, Onboard, and Radar If an sUAS has GPS onboard, is detected by surrounding vehicles, and identified by the ground based radars, then this FIS will be employed.Here, the input to the FIS is again the normalized average range separating the vehicles, as sensed by the onboard sensors.This average range was normalized by taking the true average value and dividing it by 180 m (also determined after testing/tuning).Unlike the previously described FISs, a total of six confidence values need to be assigned.To accomplish this, it was decided that two of the six values, one for each direction (lateral and vertical) would be held constant, regardless of the input value.Thus, the FIS still only needs to compute four confidence values.For this study, G(xy) and R(z) were held constant at 0.5.These were selected due to the low uncertainty associated with the GPS estimation in the lateral plane, and the relatively low uncertainty with radar measurements in the vertical plane.Recalling (20), we need to ensure that the sum of all three confidences must be equal to one, for both the lateral and vertical directions.Thus, the domain of each output must consider how much confidence has already been placed in the sensor that is held constant.For this fuzzy system, the domain of the outputs must satisfy (23).Here,  1 and  2 are the domains for outputs one and two (for the same direction), respectively, and  is the constant confidence level as defined by the designer.IV.RESULTS In each of the 135 configuration cases shown in Table II, 1,000 independent measurements were evaluated.For each case, the error for each independent measurement was recorded.To visualize the error distributions, each sensor type's estimate and the final fused estimate was plotted on a histogram.In Fig. 3, an example histogram is shown.This histogram shows the results of a sample trial where all four radar sources are available and three sUAS can sense the vehicle with onboard sensors.Here, the measurement error was broken down into the lateral error, vertical error, and the total error.As seen from this figure, the fuzzy fusion estimation more accurately modeled the true vehicle location than all other sources.In all cases, the fused mean error is lesser than all other estimations, and has a lesser standard deviation.Although not explicitly shown, in all cases the MAP estimations displayed were more accurate than the individual raw measurement values.To evaluate the performance of the proposed fusion technique, the mean and standard deviation of the error for each configuration was computed and compared against the sensor MAP values.The results of all 135 cases have been described in Fig. 4. Here, the mean and standard deviation of the error for each sensor type combination is shown.Whereas the left inset in Fig. 4 segregates the data by the number of radars, the middle and right insets combine the results for all numbers of radars and instead segregates the data by number of sUAS.As it can be seen from these graphs, the fused value had a lower mean and standard deviation than any single sensor MAP estimation in all cases.In addition, as the number of sUAS increased, the mean and standard deviation of the error decreased.This means that as the number of vehicles sensing one particular vehicle increased, so did the accuracy of the position estimation.Lastly, using all three sensor types resulted in the best fused estimation.
V. CONCLUSIONIn this study, we have demonstrated a novel approach to estimate the location of an sUAS using a Fuzzy Logic based sensor fusion technique.The presented fusion system produced position estimates that had lower mean error and standard deviation when compared to using a Maximum a Posterior estimator for each sensor platform.Overall, this approach could be applied to any number of sensor sources with varying reliability and performance, and be used in real-time operations.At this time, the fuzzy system parameters were developed by hand with no additional tuning.To improve its performance, we wish to develop a Genetic Algorithm to train each FIS.In addition, we would like to incorporate this system into a vehicle tracker.Therefore, as vehicles move in space throughout time, the tracker would need to identify the vehicles and use data association to assign new data to tracks.Adding this time component would allow this fuzzy solution to be used for stateestimation of the speed and heading of all sUAS.Lastly, we are interested in using a quaternion approach to estimate vehicle locations.This approach may have a beneficial reduction in the computational complexity of the proposed system.Fig. 1 .1Fig. 1.Simulation Area
Fig. 22Fig. 2 Example of Fuzzy Inference System Structure
in lateral and vertical measurements, respectively: [G(xy)] and [G(z)] • Onboard sensor confidence in lateral and vertical measurements, respectively: [O(xy)] and [O(z)] • Radar confidence in lateral and vertical measurements, respectively: [R(xy)] and [R(z)] • Domain and Output bounds, respectively: [] and [Out] • Inputs: Close [C], Medium [M], and Far [F] • Outputs: Low [L], Medium [M], and High [H]

TABLE II.SENSOR SPECIFICATIONSTypeParameterStd. Dev.Sensor RangeRange (R)4.37 m2000 mRadarAzimuth (𝜃)0.002 radN/AElevation (𝜀)0.002 rad0.5236 radRange (R)1.00 m200 mOnboardAzimuth (𝜃)0.175 radN/AElevation (𝜀).0175 radN/AGPSLateral (𝑟) Altitude (z)3.10 m 3.90 mN/A N/A
TABLE IIII.SIMULATION CASES# Radars# UAS# CasesGPS On?115Yes22 35 5Yes/No Yes/No45Yes/No115Yes32 35 5Yes/No Yes/No45Yes/No115Yes42 35 5Yes/No Yes/No45Yes/No
TABLE IIIIII.GPS AND RADAR FIS RULESInputOutputsRule #DistG(xy)R(xy)G(z)R(z)1CLHLH2MMMMM3FHLHLTABLE IV.GPS AND RADAR FIS OUTPUT DOMAINSG(xy)R(xy)G(z)R(z)𝓓[0.55, 0.85][0.15, 0.45][0.2, 0.5][0.5, 0.8]𝐎𝐮𝐭[0.6, 0.8][0.2, 0.4][0.25, 0.45][0.55, 0.75]
TABLE V .VONBOARD AND RADAR FIS RULESInputOutputsRule #DistO(xy)R(xy)O(z)R(z)1CHLHL2MMMMM3FLHLHTABLE VI.ONBOARD AND RADAR FIS OUTPUT DOMAINSO(xy)R(xy)O(z)R(z)𝓓[0.325, 0.775] [0.225, 0.675] [0.2, 0.425][0.575, 0.8]𝐎𝐮𝐭[0.4, 0.7][0.3, 0.6][0.25, 0.4][0.6, 0.75]
TABLE VIIVII.GPS, ONBOARD, AND RADAR FIS RULESInputOutputsRule #DistO(xy)R(xy)O(z)G(z)1CHLHL2MMMMM3FLHLHTABLE VIII. GPS, ONBOARD, AND RADAR FIS OUTPUT DOMAINSO(xy), R(xy)O(z)G(z)𝓓[0.175, 0.325][0.125, 0.275][0.225, 0.375]𝐎𝐮𝐭[0.2, 0.3][0.15, 0.25][0.25, 0.35]
		
		
			

				


	
		Accurate differential global positioning system via fuzzy logic Kalman filter sensor fusion technique
		
			KKobayashi
		
		
			KCCheok
		
		
			KWatanabe
		
		
			FMunekata
		
		10.1109/41.679010
	
	
		IEEE Transactions on Industrial Electronics
		IEEE Trans. Ind. Electron.
		0278-0046
		
			45
			3
			
			Aug. 2002
			Institute of Electrical and Electronics Engineers (IEEE)
		
	
	K. Kobayashi, K.C. Cheok, K. Watanable, F. Munekata. "Accurate differential global positioning system via fuzzy logic Kalman filter sensor fusion technique," IEEE Transactions on Industrial Electronics, vol. 45, issue 3, pp. 510-518, Aug. 2002



	
		Multisensor data fusion: A review of the state-of-the-art
		
			Khaleghi
		
		
			AKhamis
		
		
			FOKarray
		
		
			SNRazavi
		
	
	
		Information Fusion
		
			2013
			
		
	
	B, Khaleghi, A. Khamis, F.O. Karray, S.N. Razavi. "Multisensor data fusion: A review of the state-of-the-art." Information Fusion, pp. 28-44, 2013.



	
		RADAR Measurement and Tracking
		
			GRCurry
		
	
	
		RADAR System Performance Modeling
		
			MA
			2005
			
		
	
	nd ed.
	G.R. Curry. "RADAR Measurement and Tracking", RADAR System Performance Modeling, 2 nd ed., Artech House, MA, 2005, pp. 169-171.



	
		Fig. 2. Change in HbA1c during treatment (mean ± standard error).
		
			BWParkinson
		
		10.14341/dm9586-3254
	
	
		Progress in astronautics and aeronautics: Global positioning system: Theory and applications
		
			Endocrinology Research Centre
			1996
			2
		
	
	B.W. Parkinson. Progress in astronautics and aeronautics: Global positioning system: Theory and applications. AIAA, Vol. 2., 1996. Fig 3. Histograms of Measurement Error Fig 4. Mean and Standard Deviations for all Sensor Types


				
			
		
	
